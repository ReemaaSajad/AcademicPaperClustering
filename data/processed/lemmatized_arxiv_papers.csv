title,abstract,authors,link,source,cleaned_abstract
Lecture Notes: Neural Network Architectures,"These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",Evelyn Herberg,http://arxiv.org/abs/2304.05133v2,arXiv,these lecture note provide an overview of neural network architecture from a mathematical point of view especially machine learn with neural network be see a an optimization problem cover be an introduction to neural network and the follow architecture feedforward neural network convolutional neural network resnet and recurrent neural network
Self-Organizing Multilayered Neural Networks of Optimal Complexity,The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.,V. Schetinin,http://arxiv.org/abs/cs/0504056v1,arXiv,the principle of self-organizing the neural network of optimal complexity be consider under the unrepresentative learn set the method of self-organizing the multi-layered neural network be offer and use to train the logical neural network which be apply to the medical diagnostics
"Neural Network Processing Neural Networks: An efficient way to learn
  higher order functions","Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called ""Neural Network Processing Neural Networks"" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures.",Firat Tuna,http://arxiv.org/abs/1911.05640v2,arXiv,function be rich in meaning and can be interpret in a variety of way neural network be proven to be capable of approximate a large class of function 1 in this paper we propose a new class of neural network call neural network processing neural network nnpnns which input neural network and numerical value instead of just numerical value thus enable neural network to represent and process rich structure
"Guaranteed Quantization Error Computation for Neural Network Model
  Compression","Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.","Wesley Cooke, Zihao Mo, Weiming Xiang",http://arxiv.org/abs/2304.13812v1,arXiv,neural network model compression technique can address the computation issue of deep neural network on embed device in industrial system the guaranteed output error computation problem for neural network compression with quantization be address in this paper a merge neural network be built from a feedforward neural network and it quantize version to produce the exact output difference between two neural network then optimization-based method and reachability analysis method be apply to the merge neural network to compute the guaranteed quantization error finally a numerical example be propose to validate the applicability and effectiveness of the propose approach
Graph Structure of Neural Networks,"Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a ""sweet spot"" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.","Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",http://arxiv.org/abs/2007.06559v2,arXiv,neural network be often represent a graph of connection between neuron however despite their wide use there be currently little understand of the relationship between the graph structure of the neural network and it predictive performance here we systematically investigate how do the graph structure of neural network affect their predictive performance to this end we develop a novel graph-based representation of neural network call relational graph where layer of neural network computation correspond to round of message exchange along the graph structure use this representation we show that 1 a sweet spot of relational graph lead to neural network with significantly improve predictive performance 2 neural network s performance be approximately a smooth function of the cluster coefficient and average path length of it relational graph 3 our finding be consistent across many different task and datasets 4 the sweet spot can be identify efficiently 5 top-performing neural network have graph structure surprisingly similar to those of real biological neural network our work open new direction for the design of neural architecture and the understand on neural network in general
"Hybrid Quantum-Classical Neural Networks for Downlink Beamforming
  Optimization","This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network. The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method. The second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network. Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters. The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices.","Juping Zhang, Gan Zheng, Toshiaki Koike-Akino, Kai-Kit Wong, Fraser Burton",http://arxiv.org/abs/2408.04747v1,arXiv,this paper investigates quantum machine learn to optimize the beamforming in a multiuser multiple-input single-output downlink system we aim to combine the power of quantum neural network and the success of classical deep neural network to enhance the learn performance specifically we propose two hybrid quantum-classical neural network to maximize the sum rate of a downlink system the first one proposes a quantum neural network employ parameterized quantum circuit that follow a classical convolutional neural network the classical neural network can be jointly train with the quantum neural network or pre-trained lead to a fine-tuning transfer learn method the second one design a quantum convolutional neural network to well extract feature follow by a classical deep neural network our result demonstrate the feasibility of the propose hybrid neural network and reveal that the first method can achieve similar sum rate performance compare to a benchmark classical neural network with significantly less training parameter while the second method can achieve high sum rate especially in presence of many user still with less training parameter the robustness of the propose method be verify use both software simulator and hardware emulator consider noisy intermediate-scale quantum device
"A Novel Neural Filter to Improve Accuracy of Neural Network Models of
  Dynamic Systems","The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state's accuracy. The neural filter's improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems. Numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance, outperforming the neural network predictions. Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network.","Parham Oveissi, Turibius Rozario, Ankit Goel",http://arxiv.org/abs/2409.13654v2,arXiv,the application of neural network in model dynamic system have become prominent due to their ability to estimate complex nonlinear function despite their effectiveness neural network face challenge in long-term prediction where the prediction error diverges over time thus degrade their accuracy this paper present a neural filter to enhance the accuracy of long-term state prediction of neural network-based model of dynamic system motivate by the extend kalman filter the neural filter combine the neural network state prediction with the measurement from the physical system to improve the estimate state s accuracy the neural filter s improvement in prediction accuracy be demonstrate through application to four nonlinear dynamical system numerical experiment show that the neural filter significantly improves prediction accuracy and bound the state estimate covariance outperform the neural network prediction furthermore it be also show that the accuracy of a poorly train neural network model can be improve to the same level a that of an adequately train neural network model potentially decrease the training cost and require data to train a neural network
Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.",Liyao Gao,http://arxiv.org/abs/1804.03313v1,arXiv,neural network have be successfully apply to many real-world problem such a image recognition and machine translation however for the current architecture of neural network it be hard to perform complex cognitive task for example to process the image and audio input together cortex a an important architecture in the brain be important for animal to perform the complex cognitive task we view the architecture of cortex in the brain a a miss part in the design of the current artificial neural network in this paper we purpose cortex neural network crtxnn the cortex neural network be an upper architecture of neural network which motivate from cerebral cortex in the brain to handle different task in the same learn system it be able to identify different task and solve them with different method in our implementation the cortex neural network be able to process different cognitive task and perform reflection to get a high accuracy we provide a series of experiment to examine the capability of the cortex architecture on traditional neural network our experiment prove it ability on the cortex neural network can reach accuracy by 98 32 on mnist and 62 on cifar10 at the same time which can promisingly reduce the loss by 40
Parametrical Neural Networks and Some Other Similar Architectures,"A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",Leonid B. Litinskii,http://arxiv.org/abs/cs/0608073v1,arXiv,a review of work on associative neural network accomplish during last four year in the institute of optical neural technology ra be give the presentation be base on description of parametrical neural network pnn for today pnn have record recognize characteristic storage capacity noise immunity and speed of operation presentation of basic idea and principle be accentuate
Assessing Intelligence in Artificial Neural Networks,"The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.","Nicholas J. Schaub, Nathan Hotaling",http://arxiv.org/abs/2006.02909v1,arXiv,the purpose of this work be to develop of metric to assess network architecture that balance neural network size and task performance to this end the concept of neural efficiency be introduce to measure neural layer utilization and a second metric call artificial intelligence quotient aiq be create to balance neural network performance and neural network efficiency to study aiq and neural efficiency two simple neural network be train on mnist a fully connect network lenet-300-100 and a convolutional neural network lenet-5 the lenet-5 network with the high aiq be 2 32 less accurate but contain 30 912 time few parameter than the high accuracy network both batch normalization and dropout layer be found to increase neural efficiency finally high aiq network be show to be memorization and overtraining resistant capable of learn proper digit classification with an accuracy of 92 51 even when 75 of the class label be randomize these result demonstrate the utility of aiq and neural efficiency a metric for balance network performance and size
Rational Neural Network Controllers,"Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is important to consider alternative architectures. This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops. Rational activation functions are replaced by a general rational neural network structure, which is convex in the neural network's parameters. A method is proposed to recover a stabilising controller from a Sum of Squares feasibility test. This approach is then applied to a refined rational neural network which is more compatible with Sum of Squares programming. Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty.","Matthew Newton, Antonis Papachristodoulou",http://arxiv.org/abs/2307.06287v1,arXiv,neural network have show great success in many machine learn related task due to their ability to act a general function approximators recent work have demonstrate the effectiveness of neural network in control system know a neural feedback loop most notably by use a neural network a a controller however one of the big challenge of this approach be that neural network have be show to be sensitive to adversarial attack this mean that unless they be design properly they be not an ideal candidate for controller due to issue with robustness and uncertainty which be pivotal aspect of control system there have be initial work on robustness to both analyse and design dynamical system with neural network controller however one prominent issue with these method be that they use exist neural network architecture tailor for traditional machine learn task these structure may not be appropriate for neural network controller and it be important to consider alternative architecture this paper considers rational neural network and present novel rational activation function which can be use effectively in robustness problem for neural feedback loop rational activation function be replace by a general rational neural network structure which be convex in the neural network s parameter a method be propose to recover a stabilise controller from a sum of square feasibility test this approach be then apply to a refine rational neural network which be more compatible with sum of square program numerical example show that this method can successfully recover stabilise rational neural network controller for neural feedback loop with non-linear plant with noise and parametric uncertainty
Asymptotic Theory of Expectile Neural Networks,"Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework.","Jinghang Lin, Xiaoxi Shen, Qing Lu",http://arxiv.org/abs/2011.01218v1,arXiv,neural network be become an increasingly important tool in application however neural network be not widely use in statistical genetics in this paper we propose a new neural network method call expectile neural network when the size of parameter be too large the standard maximum likelihood procedure may not work we use sieve method to constrain parameter space and we prove it consistency and normality under nonparametric regression framework
"Combining Recurrent and Convolutional Neural Networks for Relation
  Classification","This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.","Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze",http://arxiv.org/abs/1605.07333v1,arXiv,this paper investigates two different neural architecture for the task of relation classification convolutional neural network and recurrent neural network for both model we demonstrate the effect of different architectural choice we present a new context representation for convolutional neural network for relation classification extend middle context furthermore we propose connectionist bi-directional recurrent neural network and introduce rank loss for their optimization finally we show that combine convolutional and recurrent neural network use a simple voting scheme be accurate enough to improve result our neural model achieve state-of-the-art result on the semeval 2010 relation classification task
"A Comprehensive Review of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices","Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.","Kai Malcolm, Josue Casco-Rodriguez",http://arxiv.org/abs/2303.10780v2,arXiv,biological neural network continue to inspire breakthrough in neural network performance and yet one key area of neural computation that have be under-appreciated and under-investigated be biologically plausible energy-efficient spike neural network whose potential be especially attractive for low-power mobile or otherwise hardware-constrained setting we present a literature review of recent development in the interpretation optimization efficiency and accuracy of spike neural network key contribution include identification discussion and comparison of cutting-edge method in spike neural network optimization energy-efficiency and evaluation start from first principle so a to be accessible to new practitioner
"Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics","Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.",Sanjana Shetty,http://arxiv.org/abs/2401.10289v1,arXiv,neural network have be employ for a wide range of processing application like image processing motor control object detection and many others living neural network offer advantage of low power consumption faster processing and biological realism optogenetics offer high spatial and temporal control over biological neuron and present potential in training live neural network this work proposes a simulated living neural network train indirectly by backpropagating stdp base algorithm use precision activation by optogenetics achieve accuracy comparable to traditional neural network training algorithm
Convex Formulation of Overparameterized Deep Neural Networks,"Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice.","Cong Fang, Yihong Gu, Weizhong Zhang, Tong Zhang",http://arxiv.org/abs/1911.07626v1,arXiv,analysis of over-parameterized neural network have drawn significant attention in recentyears it be show that such system behave like convex system under various restrictedsettings such a for two-level neural network and when learn be only restrict locally inthe so-called neural tangent kernel space around specialized initialization however there areno theoretical technique that can analyze fully train deep neural network encounter inpractice this paper solves this fundamental problem by investigate such overparameterizeddeep neural network when fully train we generalize a new technique call neural feature repopulation originally introduce in fang et al 2019a for two-level neural network to analyze deep neural network it be show that under suitable representation overparameterized deep neural network be inherently convex and when optimize the system can learn effective feature suitable for the underlie learn task under mild condition this new analysis be consistent with empirical observation that deep neural network be capable of learn efficient feature representation therefore the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural network empirical study confirm that prediction of our theory be consistent with result observe in practice
"Approximate Bisimulation Relations for Neural Networks and Application
  to Assured Neural Network Compression","In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.","Weiming Xiang, Zhongzhu Shao",http://arxiv.org/abs/2202.01214v1,arXiv,in this paper we propose a concept of approximate bisimulation relation for feedforward neural network in the framework of approximate bisimulation relation a novel neural network merge method be developed to compute the approximate bisimulation error between two neural network base on reachability analysis of neural network the developed method be able to quantitatively measure the distance between the output of two neural network with the same input then we apply the approximate bisimulation relation result to perform neural network model reduction and compute the compression precision i e assure neural network compression at last use the assure neural network compression we accelerate the verification process of acas xu neural network to illustrate the effectiveness and advantage of our propose approximate bisimulation approach
"Optimal rates of approximation by shallow ReLU$^k$ neural networks and
  applications to nonparametric regression","We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\""older functions, which complements recent results for deep neural networks. It is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.","Yunfei Yang, Ding-Xuan Zhou",http://arxiv.org/abs/2304.01561v3,arXiv,we study the approximation capacity of some variation space correspond to shallow relu neural network it be show that sufficiently smooth function be contain in these space with finite variation norm for function with less smoothness the approximation rate in term of the variation norm be establish use these result we be able to prove the optimal approximation rate in term of the number of neuron for shallow relu neural network it be also show how these result can be use to derive approximation bound for deep neural network and convolutional neural network cnns a application we study convergence rate for nonparametric regression use three relu neural network model shallow neural network over-parameterized neural network and cnn in particular we show that shallow neural network can achieve the minimax optimal rate for learn h old function which complement recent result for deep neural network it be also proven that over-parameterized deep or shallow neural network can achieve nearly optimal rate for nonparametric regression
"Understanding Vector-Valued Neural Networks and Their Relationship with
  Real and Hypercomplex-Valued Neural Networks","Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels. Finally, we show how V-nets, including hypercomplex-valued neural networks, can be implemented in current deep-learning libraries as real-valued networks.",Marcos Eduardo Valle,http://arxiv.org/abs/2309.07716v2,arXiv,despite the many successful application of deep learn model for multidimensional signal and image processing most traditional neural network process data represent by multidimensional array of real number the intercorrelation between feature channel be usually expect to be learn from the training data require numerous parameter and careful training in contrast vector-valued neural network be conceive to process array of vector and naturally consider the intercorrelation between feature channel consequently they usually have few parameter and often undergo more robust training than traditional neural network this paper aim to present a broad framework for vector-valued neural network refer to a v-nets in this context hypercomplex-valued neural network be regard a vector-valued model with additional algebraic property furthermore this paper explains the relationship between vector-valued and traditional neural network precisely a vector-valued neural network can be obtain by place restriction on a real-valued model to consider the intercorrelation between feature channel finally we show how v-nets include hypercomplex-valued neural network can be implement in current deep-learning library a real-valued network
One weird trick for parallelizing convolutional neural networks,I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.,Alex Krizhevsky,http://arxiv.org/abs/1404.5997v2,arXiv,i present a new way to parallelize the training of convolutional neural network across multiple gpus the method scale significantly well than all alternative when apply to modern convolutional neural network
Geometric Decomposition of Feed Forward Neural Networks,"There have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. The field has exploded in the last decade, yet neural networks are still treated much like a black box. In this work we describe a structure that is inherent to a feed forward neural network. This will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. Our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective.",Sven Cattell,http://arxiv.org/abs/1612.02522v1,arXiv,there have be several attempt to mathematically understand neural network and many more from biological and computational perspective the field have explode in the last decade yet neural network be still treat much like a black box in this work we describe a structure that be inherent to a feed forward neural network this will provide a framework for future work on neural network to improve training algorithm compute the homology of the network and other application our approach take a more geometric point of view and be unlike other attempt to mathematically understand neural network that rely on a functional perspective
Neural Networks Architecture Evaluation in a Quantum Computer,"In this work, we propose a quantum algorithm to evaluate neural networks architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks. Unlike conventional algorithms for evaluating neural network architectures, QNNAE does not depend on initialization of weights. The proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network. And its computational cost is equal to the computational cost to train a neural network.","Adenilton José da Silva, Rodolfo Luan F. de Oliveira",http://arxiv.org/abs/1711.04759v1,arXiv,in this work we propose a quantum algorithm to evaluate neural network architecture name quantum neural network architecture evaluation qnnae the propose algorithm be base on a quantum associative memory and the learn algorithm for artificial neural network unlike conventional algorithm for evaluate neural network architecture qnnae do not depend on initialization of weight the propose algorithm have a binary output and result in 0 with probability proportional to the performance of the network and it computational cost be equal to the computational cost to train a neural network
Building Compact and Robust Deep Neural Networks with Toeplitz Matrices,"Deep neural networks are state-of-the-art in a wide variety of tasks, however, they exhibit important limitations which hinder their use and deployment in real-world applications. When developing and training neural networks, the accuracy should not be the only concern, neural networks must also be cost-effective and reliable. Although accurate, large neural networks often lack these properties. This thesis focuses on the problem of training neural networks which are not only accurate but also compact, easy to train, reliable and robust to adversarial examples. To tackle these problems, we leverage the properties of structured matrices from the Toeplitz family to build compact and secure neural networks.",Alexandre Araujo,http://arxiv.org/abs/2109.00959v1,arXiv,deep neural network be state-of-the-art in a wide variety of task however they exhibit important limitation which hinder their use and deployment in real-world application when develop and training neural network the accuracy should not be the only concern neural network must also be cost-effective and reliable although accurate large neural network often lack these property this thesis focus on the problem of training neural network which be not only accurate but also compact easy to train reliable and robust to adversarial example to tackle these problem we leverage the property of structure matrix from the toeplitz family to build compact and secure neural network
Application of Neural Network in Optimization of Chemical Process,"Artificial neural network (ANN) has been widely used due to its strong nonlinear mapping ability, fault tolerance and self-learning ability. This article summarizes the development history of artificial neural networks, introduces three common neural network types, BP neural network, RBF neural network and convolutional neural network, and focuses on the practical application in chemical process optimization, especially the results achieved in multi-objective control optimization and process parameter improvement.","Fei Liang, Taowen Zhang",http://arxiv.org/abs/2110.04942v1,arXiv,artificial neural network ann have be widely use due to it strong nonlinear mapping ability fault tolerance and self-learning ability this article summarizes the development history of artificial neural network introduces three common neural network type bp neural network rbf neural network and convolutional neural network and focus on the practical application in chemical process optimization especially the result achieve in multi-objective control optimization and process parameter improvement
Nonlinear Systems Identification Using Deep Dynamic Neural Networks,"Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data","Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans",http://arxiv.org/abs/1610.01439v1,arXiv,neural network be know to be effective function approximators recently deep neural network have proven to be very effective in pattern recognition classification task and human-level control to model highly nonlinear realworld system this paper investigates the effectiveness of deep neural network in the model of dynamical system with complex behavior three deep neural network structure be train on sequential data and we investigate the effectiveness of these network in model associate characteristic of the underlie dynamical system we carry out similar evaluation on select publicly available system identification datasets we demonstrate that deep neural network be effective model estimator from input-output data
"Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks","The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.","Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira",http://arxiv.org/abs/2401.02277v2,arXiv,the universal approximation theorem state that a neural network with one hidden layer can approximate continuous function on compact set with any desire precision this theorem support use neural network for various application include regression and classification task furthermore it be valid for real-valued neural network and some hypercomplex-valued neural network such a complex- quaternion- tessarine- and clifford-valued neural network however hypercomplex-valued neural network be a type of vector-valued neural network define on an algebra with additional algebraic or geometric property this paper extends the universal approximation theorem for a wide range of vector-valued neural network include hypercomplex-valued model a particular instance precisely we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural network define on such algebra
"Stable Learning Using Spiking Neural Networks Equipped With Affine
  Encoders and Decoders","We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder; we refer to these as affine spiking neural networks. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights enables a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these affine spiking neural networks to standard machine learning benchmarks and reach competitive results. Finally, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.","A. Martina Neuman, Dominik Dold, Philipp Christian Petersen",http://arxiv.org/abs/2404.04549v3,arXiv,we study the learn problem associate with spike neural network specifically we focus on spike neural network compose of simple spike neuron have only positive synaptic weight equip with an affine encoder and decoder we refer to these a affine spike neural network these neural network be show to depend continuously on their parameter which facilitates classical cover number-based generalization statement and support stable gradient-based training we demonstrate that the positivity of the weight enables a wide range of expressivity result include rate-optimal approximation of smooth function and dimension-independent approximation of barron regular function in particular we show in theory and simulation that affine spike neural network be capable of approximate shallow relu neural network furthermore we apply these affine spike neural network to standard machine learn benchmark and reach competitive result finally we observe that from a generalization perspective contrary to feedforward neural network or previous result for general spike neural network the depth have little to no adverse effect on the generalization capability
Detecting Neural Trojans Through Merkle Trees,"Deep neural networks are utilized in a growing number of industries. Much of the current literature focuses on the applications of deep neural networks without discussing the security of the network itself. One security issue facing deep neural networks is neural trojans. Through a neural trojan, a malicious actor may force the deep neural network to act in unintended ways. Several potential defenses have been proposed, but they are computationally expensive, complex, or unusable in commercial applications. We propose Merkle trees as a novel way to detect and isolate neural trojans.",Joshua Strubel,http://arxiv.org/abs/2306.05368v1,arXiv,deep neural network be utilized in a grow number of industry much of the current literature focus on the application of deep neural network without discuss the security of the network itself one security issue face deep neural network be neural trojan through a neural trojan a malicious actor may force the deep neural network to act in unintended way several potential defense have be propose but they be computationally expensive complex or unusable in commercial application we propose merkle tree a a novel way to detect and isolate neural trojan
"Performance Analysis Of Neural Network Models For Oxazolines And
  Oxazoles Derivatives Descriptor Dataset","Neural networks have been used successfully to a broad range of areas such as business, data mining, drug discovery and biology. In medicine, neural networks have been applied widely in medical diagnosis, detection and evaluation of new drugs and treatment cost estimation. In addition, neural networks have begin practice in data mining strategies for the aim of prediction, knowledge discovery. This paper will present the application of neural networks for the prediction and analysis of antitubercular activity of Oxazolines and Oxazoles derivatives. This study presents techniques based on the development of Single hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural network (GDBPNN), Gradient Descent Back propagation with momentum neural network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN) and Quantile regression neural network (QRNN) of artificial neural network (ANN) models Here, we comparatively evaluate the performance of five neural network techniques. The evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application. Cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties. Predictive accuracy was evaluated using the root mean squared error (RMSE), Coefficient determination(???), mean absolute error(MAE), mean percentage error(MPE) and relative square error(RSE). We found that all five neural network models were able to produce feasible models. QRNN model is outperforms with all statistical tests amongst other four models."," Doreswamy, Chanabasayya . M. Vastrad",http://arxiv.org/abs/1312.2853v1,arXiv,neural network have be use successfully to a broad range of area such a business data mining drug discovery and biology in medicine neural network have be apply widely in medical diagnosis detection and evaluation of new drug and treatment cost estimation in addition neural network have begin practice in data mining strategy for the aim of prediction knowledge discovery this paper will present the application of neural network for the prediction and analysis of antitubercular activity of oxazolines and oxazoles derivative this study present technique base on the development of single hidden layer neural network shlffnn gradient descent back propagation neural network gdbpnn gradient descent back propagation with momentum neural network gdbpmnn back propagation with weight decay neural network bpwdnn and quantile regression neural network qrnn of artificial neural network ann model here we comparatively evaluate the performance of five neural network technique the evaluation of the efficiency of each model by way of benchmark experiment be an accepted application cross-validation and resampling technique be commonly use to derive point estimate of the performance which be compare to identify method with good property predictive accuracy be evaluate use the root mean square error rmse coefficient determination mean absolute error mae mean percentage error mpe and relative square error rse we found that all five neural network model be able to produce feasible model qrnn model be outperforms with all statistical test amongst other four model
"Why Quantization Improves Generalization: NTK of Binary Weight Neural
  Networks","Quantized neural networks have drawn a lot of attention as they reduce the space and computational complexity during the inference. Moreover, there has been folklore that quantization acts as an implicit regularizer and thus can improve the generalizability of neural networks, yet no existing work formalizes this interesting folklore. In this paper, we take the binary weights in a neural network as random variables under stochastic rounding, and study the distribution propagation over different layers in the neural network. We propose a quasi neural network to approximate the distribution propagation, which is a neural network with continuous parameters and smooth activation function. We derive the neural tangent kernel (NTK) for this quasi neural network, and show that the eigenvalue of NTK decays at approximately exponential rate, which is comparable to that of Gaussian kernel with randomized scale. This in turn indicates that the Reproducing Kernel Hilbert Space (RKHS) of a binary weight neural network covers a strict subset of functions compared with the one with real value weights. We use experiments to verify that the quasi neural network we proposed can well approximate binary weight neural network. Furthermore, binary weight neural network gives a lower generalization gap compared with real value weight neural network, which is similar to the difference between Gaussian kernel and Laplace kernel.","Kaiqi Zhang, Ming Yin, Yu-Xiang Wang",http://arxiv.org/abs/2206.05916v1,arXiv,quantize neural network have drawn a lot of attention a they reduce the space and computational complexity during the inference moreover there have be folklore that quantization act a an implicit regularizer and thus can improve the generalizability of neural network yet no exist work formalizes this interest folklore in this paper we take the binary weight in a neural network a random variable under stochastic round and study the distribution propagation over different layer in the neural network we propose a quasi neural network to approximate the distribution propagation which be a neural network with continuous parameter and smooth activation function we derive the neural tangent kernel ntk for this quasi neural network and show that the eigenvalue of ntk decay at approximately exponential rate which be comparable to that of gaussian kernel with randomize scale this in turn indicates that the reproduce kernel hilbert space rkhs of a binary weight neural network cover a strict subset of function compare with the one with real value weight we use experiment to verify that the quasi neural network we propose can well approximate binary weight neural network furthermore binary weight neural network give a low generalization gap compare with real value weight neural network which be similar to the difference between gaussian kernel and laplace kernel
Fourier Neural Networks for Function Approximation,"The success of Neural networks in providing miraculous results when applied to a wide variety of tasks is astonishing. Insight in the working can be obtained by studying the universal approximation property of neural networks. It is proved extensively that neural networks are universal approximators. Further it is proved that deep Neural networks are better approximators. It is specifically proved that for a narrow neural network to approximate a function which is otherwise implemented by a deep Neural network, the network take exponentially large number of neurons. In this work, we have implemented existing methodologies for a variety of synthetic functions and identified their deficiencies. Further, we examined that Fourier neural network is able to perform fairly good with only two layers in the neural network. A modified Fourier Neural network which has sinusoidal activation and two hidden layer is proposed and the results are tabulated.","R Subhash Chandra Bose, Kakarla Yaswanth",http://arxiv.org/abs/2111.08438v1,arXiv,the success of neural network in provide miraculous result when apply to a wide variety of task be astonish insight in the work can be obtain by study the universal approximation property of neural network it be prove extensively that neural network be universal approximators far it be prove that deep neural network be well approximators it be specifically prove that for a narrow neural network to approximate a function which be otherwise implement by a deep neural network the network take exponentially large number of neuron in this work we have implement exist methodology for a variety of synthetic function and identify their deficiency far we examine that fourier neural network be able to perform fairly good with only two layer in the neural network a modify fourier neural network which have sinusoidal activation and two hidden layer be propose and the result be tabulate
Bayesian Neural Networks: Essentials,"Bayesian neural networks utilize probabilistic layers that capture uncertainty over weights and activations, and are trained using Bayesian inference. Since these probabilistic layers are designed to be drop-in replacement of their deterministic counter parts, Bayesian neural networks provide a direct and natural way to extend conventional deep neural networks to support probabilistic deep learning. However, it is nontrivial to understand, design and train Bayesian neural networks due to their complexities. We discuss the essentials of Bayesian neural networks including duality (deep neural networks, probabilistic models), approximate Bayesian inference, Bayesian priors, Bayesian posteriors, and deep variational learning. We use TensorFlow Probability APIs and code examples for illustration. The main problem with Bayesian neural networks is that the architecture of deep neural networks makes it quite redundant, and costly, to account for uncertainty for a large number of successive layers. Hybrid Bayesian neural networks, which use few probabilistic layers judicially positioned in the networks, provide a practical solution.",Daniel T. Chang,http://arxiv.org/abs/2106.13594v1,arXiv,bayesian neural network utilize probabilistic layer that capture uncertainty over weight and activation and be train use bayesian inference since these probabilistic layer be design to be drop-in replacement of their deterministic counter part bayesian neural network provide a direct and natural way to extend conventional deep neural network to support probabilistic deep learn however it be nontrivial to understand design and train bayesian neural network due to their complexity we discus the essential of bayesian neural network include duality deep neural network probabilistic model approximate bayesian inference bayesian prior bayesian posterior and deep variational learn we use tensorflow probability apis and code example for illustration the main problem with bayesian neural network be that the architecture of deep neural network make it quite redundant and costly to account for uncertainty for a large number of successive layer hybrid bayesian neural network which use few probabilistic layer judicially position in the network provide a practical solution
"Genetic cellular neural networks for generating three-dimensional
  geometry","There are a number of ways to procedurally generate interesting three-dimensional shapes, and a method where a cellular neural network is combined with a mesh growth algorithm is presented here. The aim is to create a shape from a genetic code in such a way that a crude search can find interesting shapes. Identical neural networks are placed at each vertex of a mesh which can communicate with neural networks on neighboring vertices. The output of the neural networks determine how the mesh grows, allowing interesting shapes to be produced emergently, mimicking some of the complexity of biological organism development. Since the neural networks' parameters can be freely mutated, the approach is amenable for use in a genetic algorithm.",Hugo Martay,http://arxiv.org/abs/1603.08551v1,arXiv,there be a number of way to procedurally generate interest three-dimensional shape and a method where a cellular neural network be combine with a mesh growth algorithm be present here the aim be to create a shape from a genetic code in such a way that a crude search can find interest shape identical neural network be place at each vertex of a mesh which can communicate with neural network on neighbor vertex the output of the neural network determine how the mesh grows allow interest shape to be produce emergently mimic some of the complexity of biological organism development since the neural network parameter can be freely mutate the approach be amenable for use in a genetic algorithm
Survey of Dropout Methods for Deep Neural Networks,"Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.","Alex Labach, Hojjat Salehinejad, Shahrokh Valaee",http://arxiv.org/abs/1904.13310v2,arXiv,dropout method be a family of stochastic technique use in neural network training or inference that have generate significant research interest and be widely use in practice they have be successfully apply in neural network regularization model compression and in measure the uncertainty of neural network output while original formulate for dense neural network layer recent advance have make dropout method also applicable to convolutional and recurrent neural network layer this paper summarizes the history of dropout method their various application and current area of research interest important propose method be described in additional detail
"General Regression Neural Networks, Radial Basis Function Neural
  Networks, Support Vector Machines, and Feedforward Neural Networks","The aim of this project is to develop a code to discover the optimal sigma value that maximum the F1 score and the optimal sigma value that maximizes the accuracy and to find out if they are the same. Four algorithms which can be used to solve this problem are: Genetic Regression Neural Networks (GRNNs), Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines (SVMs) and Feedforward Neural Network (FFNNs).","Alison Jenkins, Vinika Gupta, Mary Lenoir",http://arxiv.org/abs/1911.07115v1,arXiv,the aim of this project be to develop a code to discover the optimal sigma value that maximum the f1 score and the optimal sigma value that maximizes the accuracy and to find out if they be the same four algorithm which can be use to solve this problem be genetic regression neural network grnns radial base function rbf neural network rbfnns support vector machine svms and feedforward neural network ffnns
On neural network kernels and the storage capacity problem,"In this short note, we reify the connection between work on the storage capacity problem in wide two-layer treelike neural networks and the rapidly-growing body of literature on kernel limits of wide neural networks. Concretely, we observe that the ""effective order parameter"" studied in the statistical mechanics literature is exactly equivalent to the infinite-width Neural Network Gaussian Process Kernel. This correspondence connects the expressivity and trainability of wide two-layer neural networks.","Jacob A. Zavatone-Veth, Cengiz Pehlevan",http://arxiv.org/abs/2201.04669v1,arXiv,in this short note we reify the connection between work on the storage capacity problem in wide two-layer treelike neural network and the rapidly-growing body of literature on kernel limit of wide neural network concretely we observe that the effective order parameter study in the statistical mechanic literature be exactly equivalent to the infinite-width neural network gaussian process kernel this correspondence connects the expressivity and trainability of wide two-layer neural network
Unary Coding for Neural Network Learning,This paper presents some properties of unary coding of significance for biological learning and instantaneously trained neural networks.,Subhash Kak,http://arxiv.org/abs/1009.4495v1,arXiv,this paper present some property of unary cod of significance for biological learn and instantaneously train neural network
Deep Neural Networks - A Brief History,Introduction to deep neural networks and their history.,Krzysztof J. Cios,http://arxiv.org/abs/1701.05549v1,arXiv,introduction to deep neural network and their history
GPU Acceleration of Sparse Neural Networks,"In this paper, we use graphics processing units(GPU) to accelerate sparse and arbitrary structured neural networks. Sparse networks have nodes in the network that are not fully connected with nodes in preceding and following layers, and arbitrary structure neural networks have different number of nodes in each layers. Sparse Neural networks with arbitrary structures are generally created in the processes like neural network pruning and evolutionary machine learning strategies. We show that we can gain significant speedup for full activation of such neural networks using graphical processing units. We do a prepossessing step to determine dependency groups for all the nodes in a network, and use that information to guide the progression of activation in the neural network. Then we compute activation for each nodes in its own separate thread in the GPU, which allows for massive parallelization. We use CUDA framework to implement our approach and compare the results of sequential and GPU implementations. Our results show that the activation of sparse neural networks lends very well to GPU acceleration and can help speed up machine learning strategies which generate such networks or other processes that have similar structure.","Aavaas Gajurel, Sushil J. Louis, Frederick C Harris",http://arxiv.org/abs/2005.04347v1,arXiv,in this paper we use graphic processing unit gpu to accelerate sparse and arbitrary structure neural network sparse network have node in the network that be not fully connect with node in precede and follow layer and arbitrary structure neural network have different number of node in each layer sparse neural network with arbitrary structure be generally create in the process like neural network prune and evolutionary machine learn strategy we show that we can gain significant speedup for full activation of such neural network use graphical processing unit we do a prepossess step to determine dependency group for all the node in a network and use that information to guide the progression of activation in the neural network then we compute activation for each node in it own separate thread in the gpu which allows for massive parallelization we use cuda framework to implement our approach and compare the result of sequential and gpu implementation our result show that the activation of sparse neural network lends very well to gpu acceleration and can help speed up machine learn strategy which generate such network or other process that have similar structure
Spiking Neural Networks with Random Network Architecture,"The spiking neural network, known as the third generation neural network, is an important network paradigm. Due to its mode of information propagation that follows biological rationality, the spiking neural network has strong energy efficiency and has advantages in complex high-energy application scenarios. However, unlike the artificial neural network (ANN) which has a mature and unified framework, the SNN models and training methods have not yet been widely unified due to the discontinuous and non-differentiable property of the firing mechanism. Although several algorithms for training spiking neural networks have been proposed in the subsequent development process, some fundamental issues remain unsolved. Inspired by random network design, this work proposes a new architecture for spiking neural networks, RanSNN, where only part of the network weights need training and all the classic training methods can be adopted. Compared with traditional training methods for spiking neural networks, it greatly improves the training efficiency while ensuring the training performance, and also has good versatility and stability as validated by benchmark tests.","Zihan Dai, Huanfei Ma",http://arxiv.org/abs/2505.13622v1,arXiv,the spike neural network know a the third generation neural network be an important network paradigm due to it mode of information propagation that follow biological rationality the spike neural network have strong energy efficiency and have advantage in complex high-energy application scenario however unlike the artificial neural network ann which have a mature and unified framework the snn model and training method have not yet be widely unified due to the discontinuous and non-differentiable property of the fire mechanism although several algorithm for training spike neural network have be propose in the subsequent development process some fundamental issue remain unsolved inspire by random network design this work proposes a new architecture for spike neural network ransnn where only part of the network weight need training and all the classic training method can be adopt compare with traditional training method for spike neural network it greatly improves the training efficiency while ensure the training performance and also have good versatility and stability a validate by benchmark test
Neural Network Pruning as Spectrum Preserving Process,"Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.","Shibo Yao, Dantong Yu, Ioannis Koutis",http://arxiv.org/abs/2307.08982v1,arXiv,neural network have achieve remarkable performance in various application domain nevertheless a large number of weight in pre-trained deep neural network prohibit them from be deployed on smartphones and embed system it be highly desirable to obtain lightweight version of neural network for inference in edge device many cost-effective approach be propose to prune dense and convolutional layer that be common in deep neural network and dominant in the parameter space however a unified theoretical foundation for the problem mostly be miss in this paper we identify the close connection between matrix spectrum learn and neural network training for dense and convolutional layer and argue that weight prune be essentially a matrix sparsification process to preserve the spectrum base on the analysis we also propose a matrix sparsification algorithm tailor for neural network prune that yield well prune result we carefully design and conduct experiment to support our argument hence we provide a consolidated viewpoint for neural network prune and enhance the interpretability of deep neural network by identify and preserve the critical neural weight
On Hiding Neural Networks Inside Neural Networks,Modern neural networks often contain significantly more parameters than the size of their training data. We show that this excess capacity provides an opportunity for embedding secret machine learning models within a trained neural network. Our novel framework hides the existence of a secret neural network with arbitrary desired functionality within a carrier network. We prove theoretically that the secret network's detection is computationally infeasible and demonstrate empirically that the carrier network does not compromise the secret network's disguise. Our paper introduces a previously unknown steganographic technique that can be exploited by adversaries if left unchecked.,"Chuan Guo, Ruihan Wu, Kilian Q. Weinberger",http://arxiv.org/abs/2002.10078v3,arXiv,modern neural network often contain significantly more parameter than the size of their training data we show that this excess capacity provide an opportunity for embed secret machine learn model within a train neural network our novel framework hide the existence of a secret neural network with arbitrary desire functionality within a carrier network we prove theoretically that the secret network s detection be computationally infeasible and demonstrate empirically that the carrier network do not compromise the secret network s disguise our paper introduces a previously unknown steganographic technique that can be exploit by adversary if left unchecked
Consistency of Neural Networks with Regularization,"Neural networks have attracted a lot of attention due to its success in applications such as natural language processing and computer vision. For large scale data, due to the tremendous number of parameters in neural networks, overfitting is an issue in training neural networks. To avoid overfitting, one common approach is to penalize the parameters especially the weights in neural networks. Although neural networks has demonstrated its advantages in many applications, the theoretical foundation of penalized neural networks has not been well-established. Our goal of this paper is to propose the general framework of neural networks with regularization and prove its consistency. Under certain conditions, the estimated neural network will converge to true underlying function as the sample size increases. The method of sieves and the theory on minimal neural networks are used to overcome the issue of unidentifiability for the parameters. Two types of activation functions: hyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been taken into consideration. Simulations have been conducted to verify the validation of theorem of consistency.","Xiaoxi Shen, Jinghang Lin",http://arxiv.org/abs/2207.01538v1,arXiv,neural network have attract a lot of attention due to it success in application such a natural language processing and computer vision for large scale data due to the tremendous number of parameter in neural network overfitting be an issue in training neural network to avoid overfitting one common approach be to penalize the parameter especially the weight in neural network although neural network have demonstrate it advantage in many application the theoretical foundation of penalize neural network have not be well-established our goal of this paper be to propose the general framework of neural network with regularization and prove it consistency under certain condition the estimate neural network will converge to true underlie function a the sample size increase the method of sieve and the theory on minimal neural network be use to overcome the issue of unidentifiability for the parameter two type of activation function hyperbolic tangent function tanh and rectify linear unit relu have be take into consideration simulation have be conduct to verify the validation of theorem of consistency
"Understanding Weight Similarity of Neural Networks via Chain
  Normalization Rule and Hypothesis-Training-Testing","We present a weight similarity measure method that can quantify the weight similarity of non-convex neural networks. To understand the weight similarity of different trained models, we propose to extract the feature representation from the weights of neural networks. We first normalize the weights of neural networks by introducing a chain normalization rule, which is used for weight representation learning and weight similarity measure. We extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural networks. With the chain normalization rule and the new statistical inference, we study the weight similarity measure on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), and find that the weights of an identical neural network optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a similar local solution in a metric space. The weight similarity measure provides more insight into the local solutions of neural networks. Experiments on several datasets consistently validate the hypothesis of weight similarity measure.","Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai",http://arxiv.org/abs/2208.04369v1,arXiv,we present a weight similarity measure method that can quantify the weight similarity of non-convex neural network to understand the weight similarity of different train model we propose to extract the feature representation from the weight of neural network we first normalize the weight of neural network by introduce a chain normalization rule which be use for weight representation learn and weight similarity measure we extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural network with the chain normalization rule and the new statistical inference we study the weight similarity measure on multi-layer perceptron mlp convolutional neural network cnn and recurrent neural network rnn and find that the weight of an identical neural network optimize with the stochastic gradient descent sgd algorithm converge to a similar local solution in a metric space the weight similarity measure provide more insight into the local solution of neural network experiment on several datasets consistently validate the hypothesis of weight similarity measure
"A New Constructive Method to Optimize Neural Network Architecture and
  Generalization","In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular value of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with every error in, and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks in by each error, and a function with jumping discontinuity point can be constructively approximated by a decay RBF neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data.","Hou Muzhou, Moon Ho Lee",http://arxiv.org/abs/1302.0324v1,arXiv,in this paper after analyze the reason of poor generalization and overfitting in neural network we consider some noise data a a singular value of a continuous function - jump discontinuity point the continuous part can be approximate with the simplest neural network which have good generalization performance and optimal network architecture by traditional algorithm such a constructive algorithm for feed-forward neural network with incremental training bp algorithm elm algorithm various constructive algorithm rbf approximation and svm at the same time we will construct rbf neural network to fit the singular value with every error in and we prove that a function with jumping discontinuity point can be approximate by the simplest neural network with a decay rbf neural network in by each error and a function with jumping discontinuity point can be constructively approximate by a decay rbf neural network in by each error and the constructive part have no generalization influence to the whole machine learn system which will optimize neural network architecture and generalization performance reduce the overfitting phenomenon by avoid fitting the noisy data
"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering. However, modern deep neural networks' growing energy requirements now increasingly limit their scaling and broader use. We propose a radical alternative for implementing deep neural network models: Physical Neural Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware Training to efficiently train sequences of controllable physical systems to act as deep neural networks. This method automatically trains the functionality of any sequence of real physical systems, directly, using backpropagation, the same technique used for modern deep neural networks. To illustrate their generality, we demonstrate physical neural networks with three diverse physical systems-optical, mechanical, and electrical. Physical neural networks may facilitate unconventional machine learning hardware that is orders of magnitude faster and more energy efficient than conventional electronic processors.","Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu, Peter L. McMahon",http://arxiv.org/abs/2104.13386v1,arXiv,deep neural network have become a pervasive tool in science and engineering however modern deep neural network grow energy requirement now increasingly limit their scale and broader use we propose a radical alternative for implement deep neural network model physical neural network we introduce a hybrid physical-digital algorithm call physics-aware training to efficiently train sequence of controllable physical system to act a deep neural network this method automatically train the functionality of any sequence of real physical system directly use backpropagation the same technique use for modern deep neural network to illustrate their generality we demonstrate physical neural network with three diverse physical systems-optical mechanical and electrical physical neural network may facilitate unconventional machine learn hardware that be order of magnitude faster and more energy efficient than conventional electronic processor
Graph Metanetworks for Processing Diverse Neural Architectures,"Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.","Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",http://arxiv.org/abs/2312.04501v2,arXiv,neural network efficiently encode learn information within their parameter consequently many task can be unified by treat neural network themselves a input data when do so recent study demonstrate the importance of accounting for the symmetry and geometry of parameter space however those work developed architecture tailor to specific network such a mlps and cnns without normalization layer and generalize such architecture to other type of network can be challenge in this work we overcome these challenge by building new metanetworks - neural network that take weight from other neural network a input put simply we carefully build graph represent the input neural network and process the graph use graph neural network our approach graph metanetworks gmns generalizes to neural architecture where compete method struggle such a multi-head attention layer normalization layer convolutional layer resnet block and group-equivariant linear layer we prove that gmns be expressive and equivariant to parameter permutation symmetry that leave the input neural network function unchanged we validate the effectiveness of our method on several metanetwork task over diverse neural network architecture
"Bayesian Learning of Neural Networks for Signal/Background
  Discrimination in Particle Physics","Neural networks are used extensively in classification problems in particle physics research. Since the training of neural networks can be viewed as a problem of inference, Bayesian learning of neural networks can provide more optimal and robust results than conventional learning methods. We have investigated the use of Bayesian neural networks for signal/background discrimination in the search for second generation leptoquarks at the Tevatron, as an example. We present a comparison of the results obtained from the conventional training of feedforward neural networks and networks trained with Bayesian methods.","Michael Pogwizd, Laura Jane Elgass, Pushpalatha C. Bhat",http://arxiv.org/abs/0707.0930v1,arXiv,neural network be use extensively in classification problem in particle physic research since the training of neural network can be view a a problem of inference bayesian learn of neural network can provide more optimal and robust result than conventional learn method we have investigate the use of bayesian neural network for signal background discrimination in the search for second generation leptoquarks at the tevatron a an example we present a comparison of the result obtain from the conventional training of feedforward neural network and network train with bayesian method
Deep Neural Networks for Pattern Recognition,"In the field of pattern recognition research, the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods. Deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification, object detection, and segmentation. This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Then we identify the operational processes and applications of conditional generative adversarial networks, which are being actively researched based on the bottom-up and top-down mechanisms, the most important functions of the human visual perception process. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.","Kyongsik Yun, Alexander Huyen, Thomas Lu",http://arxiv.org/abs/1809.09645v1,arXiv,in the field of pattern recognition research the method of use deep neural network base on improve compute hardware recently attract attention because of their superior accuracy compare to conventional method deep neural network simulate the human visual system and achieve human equivalent accuracy in image classification object detection and segmentation this chapter introduces the basic structure of deep neural network that simulate human neural network then we identify the operational process and application of conditional generative adversarial network which be be actively research base on the bottom-up and top-down mechanism the most important function of the human visual perception process finally recent development in training strategy for effective learn of complex deep neural network be address
"Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization","Understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning. Despite the close association of notable successes in this domain with the convergence of artificial neural networks, this concept remains predominantly theoretical. In reality, due to the non-convex nature of the optimization problems that artificial neural networks tackle, very few trained networks actually achieve convergence. To expand recent research efforts on artificial-neural-network convergence, this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network.",Thien An L. Nguyen,http://arxiv.org/abs/2403.05610v1,arXiv,understand the convergence process of neural network be one of the most complex and crucial issue in the field of machine learn despite the close association of notable success in this domain with the convergence of artificial neural network this concept remains predominantly theoretical in reality due to the non-convex nature of the optimization problem that artificial neural network tackle very few train network actually achieve convergence to expand recent research effort on artificial-neural-network convergence this paper will discus a different approach base on observation of cohesive-convergence group emerge during the optimization process of an artificial neural network
"Hybrid deep neural network based prediction method for unsteady flows
  with moving boundaries","A novel hybrid deep neural network architecture is designed to capture the spatial-temporal features of unsteady flows around moving boundaries directly from high-dimensional unsteady flow fields data. The hybrid deep neural network is constituted by the convolutional neural network (CNN), improved convolutional Long-Short Term Memory neural network (ConvLSTM) and deconvolutional neural network (DeCNN). Flow fields at future time step can be predicted through flow fields by previous time steps and boundary positions at those steps by the novel hybrid deep neural network. Unsteady wake flows around a forced oscillation cylinder with various amplitudes are calculated to establish the datasets as training samples for training the hybrid deep neural networks. The trained hybrid deep neural networks are then tested by predicting the unsteady flow fields around a forced oscillation cylinder with new amplitude. The effect of neural network structure parameters on prediction accuracy was analyzed. The hybrid deep neural network, constituted by the best parameter combination, is used to predict the flow fields in the future time. The predicted flow fields are in good agreement with those calculated directly by computational fluid dynamic solver, which means that this kind of deep neural network can capture accurate spatial-temporal information from the spatial-temporal series of unsteady flows around moving boundaries. The result shows the potential capability of this kind novel hybrid deep neural network in flow control for vibrating cylinder, where the fast calculation of high-dimensional nonlinear unsteady flow around moving boundaries is needed.","Renkun Han, Zhong Zhang, Yixing Wang, Ziyang Liu, Yang Zhang, Gang Chen",http://arxiv.org/abs/2006.00690v1,arXiv,a novel hybrid deep neural network architecture be design to capture the spatial-temporal feature of unsteady flow around move boundary directly from high-dimensional unsteady flow field data the hybrid deep neural network be constitute by the convolutional neural network cnn improve convolutional long-short term memory neural network convlstm and deconvolutional neural network decnn flow field at future time step can be predict through flow field by previous time step and boundary position at those step by the novel hybrid deep neural network unsteady wake flow around a force oscillation cylinder with various amplitude be calculate to establish the datasets a training sample for training the hybrid deep neural network the train hybrid deep neural network be then test by predict the unsteady flow field around a force oscillation cylinder with new amplitude the effect of neural network structure parameter on prediction accuracy be analyze the hybrid deep neural network constitute by the best parameter combination be use to predict the flow field in the future time the predict flow field be in good agreement with those calculate directly by computational fluid dynamic solver which mean that this kind of deep neural network can capture accurate spatial-temporal information from the spatial-temporal series of unsteady flow around move boundary the result show the potential capability of this kind novel hybrid deep neural network in flow control for vibrate cylinder where the fast calculation of high-dimensional nonlinear unsteady flow around move boundary be need
"Perception-Informed Neural Networks: Beyond Physics-Informed Neural
  Networks","This article introduces Perception-Informed Neural Networks (PrINNs), a framework designed to incorporate perception-based information into neural networks, addressing both systems with known and unknown physics laws or differential equations. Moreover, PrINNs extend the concept of Physics-Informed Neural Networks (PINNs) and their variants, offering a platform for the integration of diverse forms of perception precisiation, including singular, probability distribution, possibility distribution, interval, and fuzzy graph. In fact, PrINNs allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions, enabling the creation of modern data-driven models. Some of the key contributions include Mixture of Experts Informed Neural Networks (MOEINNs), which combine heterogeneous expert knowledge into the network, and Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the incorporation of meta-information for enhanced model performance. Additionally, Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture, allowing online training without pre-training and eliminating the need for defuzzification. PrINNs represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches, enabling neural networks to learn from both structured physics laws and flexible perception-based rules. This approach empowers neural networks to operate in uncertain environments, model complex systems, and discover new forms of differential equations, making PrINNs a powerful tool for advancing computational science and engineering.","Mehran Mazandarani, Marzieh Najariyan",http://arxiv.org/abs/2505.03806v2,arXiv,this article introduces perception-informed neural network prinns a framework design to incorporate perception-based information into neural network address both system with know and unknown physic law or differential equation moreover prinns extend the concept of physics-informed neural network pinns and their variant offering a platform for the integration of diverse form of perception precisiation include singular probability distribution possibility distribution interval and fuzzy graph in fact prinns allow neural network to model dynamical system by integrate expert knowledge and perception-based information through loss function enable the creation of modern data-driven model some of the key contribution include mixture of expert inform neural network moeinns which combine heterogeneous expert knowledge into the network and transformed-knowledge inform neural network tkinns which facilitate the incorporation of meta-information for enhance model performance additionally fuzzy-informed neural network finn a a modern class of fuzzy deep neural network leverage fuzzy logic constraint within a deep learn architecture allow online training without pre-training and eliminate the need for defuzzification prinns represent a significant step forward in bridging the gap between traditional physics-based model and modern data-driven approach enable neural network to learn from both structure physic law and flexible perception-based rule this approach empowers neural network to operate in uncertain environment model complex system and discover new form of differential equation make prinns a powerful tool for advance computational science and engineering
Neural network learning dynamics in a path integral framework,A path-integral formalism is proposed for studying the dynamical evolution in time of patterns in an artificial neural network in the presence of noise. An effective cost function is constructed which determines the unique global minimum of the neural network system. The perturbative method discussed also provides a way for determining the storage capacity of the network.,J. Balakrishnan,http://arxiv.org/abs/cond-mat/0308503v1,arXiv,a path-integral formalism be propose for study the dynamical evolution in time of pattern in an artificial neural network in the presence of noise an effective cost function be construct which determines the unique global minimum of the neural network system the perturbative method discuss also provide a way for determine the storage capacity of the network
Feedforward Neural Networks for Caching: Enough or Too Much?,"We propose a caching policy that uses a feedforward neural network (FNN) to predict content popularity. Our scheme outperforms popular eviction policies like LRU or ARC, but also a new policy relying on the more complex recurrent neural networks. At the same time, replacing the FNN predictor with a naive linear estimator does not degrade caching performance significantly, questioning then the role of neural networks for these applications.","Vladyslav Fedchenko, Giovanni Neglia, Bruno Ribeiro",http://arxiv.org/abs/1810.06930v1,arXiv,we propose a cache policy that us a feedforward neural network fnn to predict content popularity our scheme outperforms popular eviction policy like lru or arc but also a new policy rely on the more complex recurrent neural network at the same time replace the fnn predictor with a naive linear estimator do not degrade cache performance significantly question then the role of neural network for these application
"Deep Kronecker neural networks: A general framework for neural networks
  with adaptive activation functions","We propose a new type of neural networks, Kronecker neural networks (KNNs), that form a general framework for neural networks with adaptive activation functions. KNNs employ the Kronecker product, which provides an efficient way of constructing a very wide network while keeping the number of parameters low. Our theoretical analysis reveals that under suitable conditions, KNNs induce a faster decay of the loss than that by the feed-forward networks. This is also empirically verified through a set of computational examples. Furthermore, under certain technical assumptions, we establish global convergence of gradient descent for KNNs. As a specific case, we propose the Rowdy activation function that is designed to get rid of any saturation region by injecting sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy activation function can be employed in any neural network architecture like feed-forward neural networks, Recurrent neural networks, Convolutional neural networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated through various computational experiments including function approximation using feed-forward neural networks, solution inference of partial differential equations using the physics-informed neural networks, and standard deep learning benchmark problems using convolutional and fully-connected neural networks.","Ameya D. Jagtap, Yeonjong Shin, Kenji Kawaguchi, George Em Karniadakis",http://arxiv.org/abs/2105.09513v2,arXiv,we propose a new type of neural network kronecker neural network knns that form a general framework for neural network with adaptive activation function knns employ the kronecker product which provide an efficient way of construct a very wide network while keep the number of parameter low our theoretical analysis reveals that under suitable condition knns induce a faster decay of the loss than that by the feed-forward network this be also empirically verify through a set of computational example furthermore under certain technical assumption we establish global convergence of gradient descent for knns a a specific case we propose the rowdy activation function that be design to get rid of any saturation region by inject sinusoidal fluctuation which include trainable parameter the propose rowdy activation function can be employ in any neural network architecture like feed-forward neural network recurrent neural network convolutional neural network etc the effectiveness of knns with rowdy activation be demonstrate through various computational experiment include function approximation use feed-forward neural network solution inference of partial differential equation use the physics-informed neural network and standard deep learn benchmark problem use convolutional and fully-connected neural network
The Representation Theory of Neural Networks,"In this work, we show that neural networks can be represented via the mathematical theory of quiver representations. More specifically, we prove that a neural network is a quiver representation with activation functions, a mathematical object that we represent using a network quiver. Also, we show that network quivers gently adapt to common neural network concepts such as fully-connected layers, convolution operations, residual connections, batch normalization, pooling operations and even randomly wired neural networks. We show that this mathematical representation is by no means an approximation of what neural networks are as it exactly matches reality. This interpretation is algebraic and can be studied with algebraic methods. We also provide a quiver representation model to understand how a neural network creates representations from the data. We show that a neural network saves the data as quiver representations, and maps it to a geometrical space called the moduli space, which is given in terms of the underlying oriented graph of the network, i.e., its quiver. This results as a consequence of our defined objects and of understanding how the neural network computes a prediction in a combinatorial and algebraic way. Overall, representing neural networks through the quiver representation theory leads to 9 consequences and 4 inquiries for future research that we believe are of great interest to better understand what neural networks are and how they work.","Marco Antonio Armenta, Pierre-Marc Jodoin",http://arxiv.org/abs/2007.12213v2,arXiv,in this work we show that neural network can be represent via the mathematical theory of quiver representation more specifically we prove that a neural network be a quiver representation with activation function a mathematical object that we represent use a network quiver also we show that network quiver gently adapt to common neural network concept such a fully-connected layer convolution operation residual connection batch normalization pool operation and even randomly wire neural network we show that this mathematical representation be by no mean an approximation of what neural network be a it exactly match reality this interpretation be algebraic and can be study with algebraic method we also provide a quiver representation model to understand how a neural network creates representation from the data we show that a neural network save the data a quiver representation and map it to a geometrical space call the modulus space which be give in term of the underlie orient graph of the network i e it quiver this result a a consequence of our define object and of understand how the neural network computes a prediction in a combinatorial and algebraic way overall represent neural network through the quiver representation theory lead to 9 consequence and 4 inquiry for future research that we believe be of great interest to well understand what neural network be and how they work
Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise,"Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.","Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh",http://arxiv.org/abs/1906.02355v1,arXiv,neural ordinary differential equation neural ode have be propose a a continuous approximation to the resnet architecture some commonly use regularization mechanism in discrete neural network e g dropout gaussian noise be miss in current neural ode network in this paper we propose a new continuous neural network framework call neural stochastic differential equation neural sde network which naturally incorporates various commonly use regularization mechanism base on random noise injection our framework can model various type of noise injection frequently use in discrete network for regularization purpose such a dropout and additive multiplicative noise in each block we provide theoretical analysis explain the improve robustness of neural sde model against input perturbation adversarial attack furthermore we demonstrate that the neural sde network can achieve well generalization than the neural ode and be more resistant to adversarial and non-adversarial input perturbation
Can a powerful neural network be a teacher for a weaker neural network?,"The transfer learning technique is widely used to learning in one context and applying it to another, i.e. the capacity to apply acquired knowledge and skills to new situations. But is it possible to transfer the learning from a deep neural network to a weaker neural network? Is it possible to improve the performance of a weak neural network using the knowledge acquired by a more powerful neural network? In this work, during the training process of a weak network, we add a loss function that minimizes the distance between the features previously learned from a strong neural network with the features that the weak network must try to learn. To demonstrate the effectiveness and robustness of our approach, we conducted a large number of experiments using three known datasets and demonstrated that a weak neural network can increase its performance if its learning process is driven by a more powerful neural network.","Nicola Landro, Ignazio Gallo, Riccardo La Grassa",http://arxiv.org/abs/2005.00393v2,arXiv,the transfer learn technique be widely use to learn in one context and apply it to another i e the capacity to apply acquire knowledge and skill to new situation but be it possible to transfer the learn from a deep neural network to a weaker neural network be it possible to improve the performance of a weak neural network use the knowledge acquire by a more powerful neural network in this work during the training process of a weak network we add a loss function that minimizes the distance between the feature previously learn from a strong neural network with the feature that the weak network must try to learn to demonstrate the effectiveness and robustness of our approach we conduct a large number of experiment use three know datasets and demonstrate that a weak neural network can increase it performance if it learn process be driven by a more powerful neural network
"Message Passing Variational Autoregressive Network for Solving
  Intractable Ising Models","Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks. Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems. Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking. Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables. The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures. The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks. Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems.","Qunlong Ma, Zhi Ma, Jinlong Xu, Hairui Zhang, Ming Gao",http://arxiv.org/abs/2404.06225v1,arXiv,many deep neural network have be use to solve ising model include autoregressive neural network convolutional neural network recurrent neural network and graph neural network learn a probability distribution of energy configuration or find the ground state of a disorder fully connect ising model be essential for statistical mechanic and np-hard problem despite tremendous effort a neural network architecture with the ability to high-accurately solve these fully connect and extremely intractable problem on large system be still lack here we propose a variational autoregressive architecture with a message passing mechanism which can effectively utilize the interaction between spin variable the new network train under an anneal framework outperforms exist method in solve several prototypical ising spin hamiltonians especially for large spin system at low temperature the advantage also come from the great mitigation of mode collapse during the training process of deep neural network consider these extremely difficult problem to be solve our method extends the current computational limit of unsupervised neural network to solve combinatorial optimization problem
Dynamics of Deep Neural Networks and Neural Tangent Hierarchy,"The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel (NTK) as introduced in [20], where it was proven that in the infinite width limit the NTK converges to an explicit limiting kernel and it stays constant during training. The NTK was also implicit in some other recent papers [6,13,14]. In the overparametrization regime, a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting NTK. And the gradient descent achieves zero training loss for a deep overparameterized neural network. However, it was observed in [5] that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. This performance gap is likely to originate from the change of the NTK along training due to the finite width effect. The change of the NTK along the training is central to describe the generalization features of deep neural networks.   In the current paper, we study the dynamic of the NTK for finite width deep fully-connected neural networks. We derive an infinite hierarchy of ordinary differential equations, the neural tangent hierarchy (NTH) which captures the gradient descent dynamic of the deep neural network. Moreover, under certain conditions on the neural network width and the data set dimension, we prove that the truncated hierarchy of NTH approximates the dynamic of the NTK up to arbitrary precision. This description makes it possible to directly study the change of the NTK for deep neural networks, and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting NTK.","Jiaoyang Huang, Horng-Tzer Yau",http://arxiv.org/abs/1909.08156v1,arXiv,the evolution of a deep neural network train by the gradient descent can be described by it neural tangent kernel ntk a introduce in 20 where it be proven that in the infinite width limit the ntk converges to an explicit limit kernel and it stay constant during training the ntk be also implicit in some other recent paper 6 13 14 in the overparametrization regime a fully-trained deep neural network be indeed equivalent to the kernel regression predictor use the limit ntk and the gradient descent achieves zero training loss for a deep overparameterized neural network however it be observe in 5 that there be a performance gap between the kernel regression use the limit ntk and the deep neural network this performance gap be likely to originate from the change of the ntk along training due to the finite width effect the change of the ntk along the training be central to describe the generalization feature of deep neural network in the current paper we study the dynamic of the ntk for finite width deep fully-connected neural network we derive an infinite hierarchy of ordinary differential equation the neural tangent hierarchy nth which capture the gradient descent dynamic of the deep neural network moreover under certain condition on the neural network width and the data set dimension we prove that the truncate hierarchy of nth approximates the dynamic of the ntk up to arbitrary precision this description make it possible to directly study the change of the ntk for deep neural network and shed light on the observation that deep neural network outperform kernel regression use the correspond limit ntk
"Novel Kernel Models and Exact Representor Theory for Neural Networks
  Beyond the Over-Parameterized Regime","This paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width, depth and topology, assuming only finite-energy neural activations; and a novel representor theory for neural networks in terms of a matrix-valued kernel. The first model is exact (un-approximated) and global, casting the neural network as an elements in a reproducing kernel Banach space (RKBS); we use this model to provide tight bounds on Rademacher complexity. The second model is exact and local, casting the change in neural network function resulting from a bounded change in weights and biases (ie. a training step) in reproducing kernel Hilbert space (RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model provides insight into model adaptation through tight bounds on Rademacher complexity of network adaptation. We also prove that the neural tangent kernel (NTK) is a first-order approximation of the LiNK kernel. Finally, and noting that the LiNK does not provide a representor theory for technical reasons, we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel (LeNK). This representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models. Throughout the paper (a) feedforward ReLU networks and (b) residual networks (ResNet) are used as illustrative examples.","Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh",http://arxiv.org/abs/2405.15254v1,arXiv,this paper present two model of neural-networks and their training applicable to neural network of arbitrary width depth and topology assume only finite-energy neural activation and a novel representor theory for neural network in term of a matrix-valued kernel the first model be exact un-approximated and global cast the neural network a an element in a reproduce kernel banach space rkbs we use this model to provide tight bound on rademacher complexity the second model be exact and local cast the change in neural network function result from a bound change in weight and bias ie a training step in reproduce kernel hilbert space rkhs in term of a local-intrinsic neural kernel link this local model provide insight into model adaptation through tight bound on rademacher complexity of network adaptation we also prove that the neural tangent kernel ntk be a first-order approximation of the link kernel finally and note that the link do not provide a representor theory for technical reason we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in term of a local-extrinsic neural kernel lenk this representor theory give insight into the role of higher-order statistic in neural network training and the effect of kernel evolution in neural-network kernel model throughout the paper a feedforward relu network and b residual network resnet be use a illustrative example
Feature Weight Tuning for Recursive Neural Networks,"This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform ""weight tuning"" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.",Jiwei Li,http://arxiv.org/abs/1412.3714v2,arXiv,this paper address how a recursive neural network model can automatically leave out useless information and emphasize important evidence in other word to perform weight tune for higher-level representation acquisition we propose two model weight neural network wnn and binary-expectation neural network benn which automatically control how much one specific unit contributes to the higher-level representation the propose model can be view a incorporate a more powerful compositional function for embed acquisition in recursive neural network experimental result demonstrate the significant improvement over standard neural model
"Neural Nets via Forward State Transformation and Backward Loss
  Transformation","This article studies (multilayer perceptron) neural networks with an emphasis on the transformations involved --- both forward and backward --- in order to develop a semantical/logical perspective that is in line with standard program semantics. The common two-pass neural network training algorithms make this viewpoint particularly fitting. In the forward direction, neural networks act as state transformers. In the reverse direction, however, neural networks change losses of outputs to losses of inputs, thereby acting like a (real-valued) predicate transformer. In this way, backpropagation is functorial by construction, as shown earlier in recent other work. We illustrate this perspective by training a simple instance of a neural network.","Bart Jacobs, David Sprunger",http://arxiv.org/abs/1803.09356v1,arXiv,this article study multilayer perceptron neural network with an emphasis on the transformation involve -- - both forward and backward -- - in order to develop a semantical logical perspective that be in line with standard program semantics the common two-pass neural network training algorithm make this viewpoint particularly fitting in the forward direction neural network act a state transformer in the reverse direction however neural network change loss of output to loss of input thereby act like a real-valued predicate transformer in this way backpropagation be functorial by construction a show earlier in recent other work we illustrate this perspective by training a simple instance of a neural network
Neural Networks Processing Mean Values of Random Variables,"We introduce a class of neural networks derived from probabilistic models in the form of Bayesian belief networks. By imposing additional assumptions about the nature of the probabilistic models represented in the belief networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian belief networks, providing distributed versions of probabilistic models.","M. J. Barber, J. W. Clark, C. H. Anderson",http://arxiv.org/abs/cond-mat/0407436v1,arXiv,we introduce a class of neural network derive from probabilistic model in the form of bayesian belief network by impose additional assumption about the nature of the probabilistic model represent in the belief network we derive neural network with standard dynamic that require no training to determine the synaptic weight that can pool multiple source of evidence and that deal cleanly and consistently with inconsistent or contradictory evidence the present neural network capture many property of bayesian belief network provide distribute version of probabilistic model
Stability of a neural network model with small-world connections,"Small-world networks are highly clustered networks with small distances among the nodes. There are many biological neural networks that present this kind of connections. There are no special weightings in the connections of most existing small-world network models. However, this kind of simply-connected models cannot characterize biological neural networks, in which there are different weights in synaptic connections. In this paper, we present a neural network model with weighted small-world connections, and further investigate the stability of this model.","Chunguang Li, Guanrong Chen",http://arxiv.org/abs/cond-mat/0410492v1,arXiv,small-world network be highly cluster network with small distance among the node there be many biological neural network that present this kind of connection there be no special weighting in the connection of most exist small-world network model however this kind of simply-connected model can not characterize biological neural network in which there be different weight in synaptic connection in this paper we present a neural network model with weight small-world connection and far investigate the stability of this model
Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks,"Mobility prediction allows estimating the stability of paths in a mobile wireless Ad Hoc networks. Identifying stable paths helps to improve routing by reducing the overhead and the number of connection interruptions. In this paper, we introduce a neural network based method for mobility prediction in Ad Hoc networks. This method consists of a multi-layer and recurrent neural network using back propagation through time algorithm for training.","Heni Kaaniche, Farouk Kamoun",http://arxiv.org/abs/1004.4610v1,arXiv,mobility prediction allows estimate the stability of path in a mobile wireless ad hoc network identify stable path help to improve rout by reduce the overhead and the number of connection interruption in this paper we introduce a neural network base method for mobility prediction in ad hoc network this method consists of a multi-layer and recurrent neural network use back propagation through time algorithm for training
Designing neural networks that process mean values of random variables,"We introduce a class of neural networks derived from probabilistic models in the form of Bayesian networks. By imposing additional assumptions about the nature of the probabilistic models represented in the networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that perform accurate calculation of the mean values of the random variables, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian networks, providing distributed versions of probabilistic models.","Michael J. Barber, John W. Clark",http://arxiv.org/abs/1004.5326v1,arXiv,we introduce a class of neural network derive from probabilistic model in the form of bayesian network by impose additional assumption about the nature of the probabilistic model represent in the network we derive neural network with standard dynamic that require no training to determine the synaptic weight that perform accurate calculation of the mean value of the random variable that can pool multiple source of evidence and that deal cleanly and consistently with inconsistent or contradictory evidence the present neural network capture many property of bayesian network provide distribute version of probabilistic model
A Primer on Neural Network Models for Natural Language Processing,"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",Yoav Goldberg,http://arxiv.org/abs/1510.00726v1,arXiv,over the past few year neural network have re-emerged a powerful machine-learning model yield state-of-the-art result in field such a image recognition and speech processing more recently neural network model start to be apply also to textual natural language signal again with very promising result this tutorial survey neural network model from the perspective of natural language processing research in an attempt to bring natural-language researcher up to speed with the neural technique the tutorial cover input encode for natural language task feed-forward network convolutional network recurrent network and recursive network a well a the computation graph abstraction for automatic gradient computation
On the Relative Expressiveness of Bayesian and Neural Networks,"A neural network computes a function. A central property of neural networks is that they are ""universal approximators:"" for a given continuous function, there exists a neural network that can approximate it arbitrarily well, given enough neurons (and some additional assumptions). In contrast, a Bayesian network is a model, but each of its queries can be viewed as computing a function. In this paper, we identify some key distinctions between the functions computed by neural networks and those by marginal Bayesian network queries, showing that the former are more expressive than the latter. Moreover, we propose a simple augmentation to Bayesian networks (a testing operator), which enables their marginal queries to become ""universal approximators.""","Arthur Choi, Ruocheng Wang, Adnan Darwiche",http://arxiv.org/abs/1812.08957v1,arXiv,a neural network computes a function a central property of neural network be that they be universal approximators for a give continuous function there exists a neural network that can approximate it arbitrarily well give enough neuron and some additional assumption in contrast a bayesian network be a model but each of it query can be view a compute a function in this paper we identify some key distinction between the function compute by neural network and those by marginal bayesian network query show that the former be more expressive than the latter moreover we propose a simple augmentation to bayesian network a test operator which enables their marginal query to become universal approximators
Power Law in Sparsified Deep Neural Networks,"The power law has been observed in the degree distributions of many biological neural networks. Sparse deep neural networks, which learn an economical representation from the data, resemble biological neural networks in many ways. In this paper, we study if these artificial networks also exhibit properties of the power law. Experimental results on two popular deep learning models, namely, multilayer perceptrons and convolutional neural networks, are affirmative. The power law is also naturally related to preferential attachment. To study the dynamical properties of deep networks in continual learning, we propose an internal preferential attachment model to explain how the network topology evolves. Experimental results show that with the arrival of a new task, the new connections made follow this preferential attachment process.","Lu Hou, James T. Kwok",http://arxiv.org/abs/1805.01891v1,arXiv,the power law have be observe in the degree distribution of many biological neural network sparse deep neural network which learn an economical representation from the data resemble biological neural network in many way in this paper we study if these artificial network also exhibit property of the power law experimental result on two popular deep learn model namely multilayer perceptrons and convolutional neural network be affirmative the power law be also naturally related to preferential attachment to study the dynamical property of deep network in continual learn we propose an internal preferential attachment model to explain how the network topology evolves experimental result show that with the arrival of a new task the new connection make follow this preferential attachment process
Kernel-based Translations of Convolutional Networks,"Convolutional Neural Networks, as most artificial neural networks, are commonly viewed as methods different in essence from kernel-based methods. We provide a systematic translation of Convolutional Neural Networks (ConvNets) into their kernel-based counterparts, Convolutional Kernel Networks (CKNs), and demonstrate that this perception is unfounded both formally and empirically. We show that, given a Convolutional Neural Network, we can design a corresponding Convolutional Kernel Network, easily trainable using a new stochastic gradient algorithm based on an accurate gradient computation, that performs on par with its Convolutional Neural Network counterpart. We present experimental results supporting our claims on landmark ConvNet architectures comparing each ConvNet to its CKN counterpart over several parameter settings.","Corinne Jones, Vincent Roulet, Zaid Harchaoui",http://arxiv.org/abs/1903.08131v1,arXiv,convolutional neural network a most artificial neural network be commonly view a method different in essence from kernel-based method we provide a systematic translation of convolutional neural network convnets into their kernel-based counterpart convolutional kernel network ckns and demonstrate that this perception be unfounded both formally and empirically we show that give a convolutional neural network we can design a correspond convolutional kernel network easily trainable use a new stochastic gradient algorithm base on an accurate gradient computation that performs on par with it convolutional neural network counterpart we present experimental result support our claim on landmark convnet architecture compare each convnet to it ckn counterpart over several parameter setting
A Survey on Graph Classification and Link Prediction based on GNN,"Traditional convolutional neural networks are limited to handling Euclidean space data, overlooking the vast realm of real-life scenarios represented as graph data, including transportation networks, social networks, and reference networks. The pivotal step in transferring convolutional neural networks to graph data analysis and processing lies in the construction of graph convolutional operators and graph pooling operators. This comprehensive review article delves into the world of graph convolutional neural networks. Firstly, it elaborates on the fundamentals of graph convolutional neural networks. Subsequently, it elucidates the graph neural network models based on attention mechanisms and autoencoders, summarizing their application in node classification, graph classification, and link prediction along with the associated datasets.","Xingyu Liu, Juan Chen, Quan Wen",http://arxiv.org/abs/2307.00865v1,arXiv,traditional convolutional neural network be limited to handle euclidean space data overlook the vast realm of real-life scenario represent a graph data include transportation network social network and reference network the pivotal step in transfer convolutional neural network to graph data analysis and processing lie in the construction of graph convolutional operator and graph pool operator this comprehensive review article delf into the world of graph convolutional neural network firstly it elaborates on the fundamental of graph convolutional neural network subsequently it elucidates the graph neural network model base on attention mechanism and autoencoders summarize their application in node classification graph classification and link prediction along with the associate datasets
"Evolving Self-taught Neural Networks: The Baldwin Effect and the
  Emergence of Intelligence","The so-called Baldwin Effect generally says how learning, as a form of ontogenetic adaptation, can influence the process of phylogenetic adaptation, or evolution. This idea has also been taken into computation in which evolution and learning are used as computational metaphors, including evolving neural networks. This paper presents a technique called evolving self-taught neural networks - neural networks that can teach themselves without external supervision or reward. The self-taught neural network is intrinsically motivated. Moreover, the self-taught neural network is the product of the interplay between evolution and learning. We simulate a multi-agent system in which neural networks are used to control autonomous agents. These agents have to forage for resources and compete for their own survival. Experimental results show that the interaction between evolution and the ability to teach oneself in self-taught neural networks outperform evolution and self-teaching alone. More specifically, the emergence of an intelligent foraging strategy is also demonstrated through that interaction. Indications for future work on evolving neural networks are also presented.",Nam Le,http://arxiv.org/abs/1906.08854v1,arXiv,the so-called baldwin effect generally say how learn a a form of ontogenetic adaptation can influence the process of phylogenetic adaptation or evolution this idea have also be take into computation in which evolution and learn be use a computational metaphor include evolve neural network this paper present a technique call evolve self-taught neural network - neural network that can teach themselves without external supervision or reward the self-taught neural network be intrinsically motivate moreover the self-taught neural network be the product of the interplay between evolution and learn we simulate a multi-agent system in which neural network be use to control autonomous agent these agent have to forage for resource and compete for their own survival experimental result show that the interaction between evolution and the ability to teach oneself in self-taught neural network outperform evolution and self-teaching alone more specifically the emergence of an intelligent forage strategy be also demonstrate through that interaction indication for future work on evolve neural network be also present
Context-adaptive neural network based prediction for image compression,"This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained. These gains are on average 0.99% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.","Thierry Dumas, Aline Roumy, Christine Guillemot",http://arxiv.org/abs/1807.06244v2,arXiv,this paper describes a set of neural network architecture call prediction neural network set pnns base on both fully-connected and convolutional neural network for intra image prediction the choice of neural network for predict a give image block depends on the block size hence do not need to be signal to the decoder it be show that while fully-connected neural network give good performance for small block size convolutional neural network provide well prediction in large block with complex texture thanks to the use of mask of random size during training the neural network of pnns well adapt to the available context that may vary depend on the position of the image block to be predict when integrate pnns into a h 265 codec psnr-rate performance gain go from 1 46 to 5 20 be obtain these gain be on average 0 99 large than those of prior neural network base method unlike the h 265 intra prediction mode which be each specialized in predict a specific texture the propose pnns can model a large set of complex texture
Simultaneous Weight and Architecture Optimization for Neural Networks,"Neural networks are trained by choosing an architecture and training the parameters. The choice of architecture is often by trial and error or with Neural Architecture Search (NAS) methods. While NAS provides some automation, it often relies on discrete steps that optimize the architecture and then train the parameters. We introduce a novel neural network training framework that fundamentally transforms the process by learning architecture and parameters simultaneously with gradient descent. With the appropriate setting of the loss function, it can discover sparse and compact neural networks for given datasets. Central to our approach is a multi-scale encoder-decoder, in which the encoder embeds pairs of neural networks with similar functionalities close to each other (irrespective of their architectures and weights). To train a neural network with a given dataset, we randomly sample a neural network embedding in the embedding space and then perform gradient descent using our custom loss function, which incorporates a sparsity penalty to encourage compactness. The decoder generates a neural network corresponding to the embedding. Experiments demonstrate that our framework can discover sparse and compact neural networks maintaining a high performance.","Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava",http://arxiv.org/abs/2410.08339v1,arXiv,neural network be train by choose an architecture and training the parameter the choice of architecture be often by trial and error or with neural architecture search na method while na provide some automation it often relies on discrete step that optimize the architecture and then train the parameter we introduce a novel neural network training framework that fundamentally transforms the process by learn architecture and parameter simultaneously with gradient descent with the appropriate set of the loss function it can discover sparse and compact neural network for give datasets central to our approach be a multi-scale encoder-decoder in which the encoder embeds pair of neural network with similar functionality close to each other irrespective of their architecture and weight to train a neural network with a give dataset we randomly sample a neural network embed in the embed space and then perform gradient descent use our custom loss function which incorporates a sparsity penalty to encourage compactness the decoder generates a neural network correspond to the embed experiment demonstrate that our framework can discover sparse and compact neural network maintain a high performance
Making Neural Networks FAIR,"Research on neural networks has gained significant momentum over the past few years. Because training is a resource-intensive process and training data cannot always be made available to everyone, there has been a trend to reuse pre-trained neural networks. As such, neural networks themselves have become research data. In this paper, we first present the neural network ontology FAIRnets Ontology, an ontology to make existing neural network models findable, accessible, interoperable, and reusable according to the FAIR principles. Our ontology allows us to model neural networks on a meta-level in a structured way, including the representation of all network layers and their characteristics. Secondly, we have modeled over 18,400 neural networks from GitHub based on this ontology, which we provide to the public as a knowledge graph called FAIRnets, ready to be used for recommending suitable neural networks to data scientists.","Anna Nguyen, Tobias Weller, Michael Färber, York Sure-Vetter",http://arxiv.org/abs/1907.11569v4,arXiv,research on neural network have gain significant momentum over the past few year because training be a resource-intensive process and training data can not always be make available to everyone there have be a trend to reuse pre-trained neural network a such neural network themselves have become research data in this paper we first present the neural network ontology fairnets ontology an ontology to make exist neural network model findable accessible interoperable and reusable accord to the fair principle our ontology allows u to model neural network on a meta-level in a structure way include the representation of all network layer and their characteristic secondly we have model over 18 400 neural network from github base on this ontology which we provide to the public a a knowledge graph call fairnets ready to be use for recommend suitable neural network to data scientist
An SMT-Based Approach for Verifying Binarized Neural Networks,"Deep learning has emerged as an effective approach for creating modern software systems, with neural networks often surpassing hand-crafted systems. Unfortunately, neural networks are known to suffer from various safety and security issues. Formal verification is a promising avenue for tackling this difficulty, by formally certifying that networks are correct. We propose an SMT-based technique for verifying Binarized Neural Networks - a popular kind of neural network, where some weights have been binarized in order to render the neural network more memory and energy efficient, and quicker to evaluate. One novelty of our technique is that it allows the verification of neural networks that include both binarized and non-binarized components. Neural network verification is computationally very difficult, and so we propose here various optimizations, integrated into our SMT procedure as deduction steps, as well as an approach for parallelizing verification queries. We implement our technique as an extension to the Marabou framework, and use it to evaluate the approach on popular binarized neural network architectures.","Guy Amir, Haoze Wu, Clark Barrett, Guy Katz",http://arxiv.org/abs/2011.02948v2,arXiv,deep learn have emerge a an effective approach for create modern software system with neural network often surpass hand-crafted system unfortunately neural network be know to suffer from various safety and security issue formal verification be a promising avenue for tackle this difficulty by formally certify that network be correct we propose an smt-based technique for verify binarized neural network - a popular kind of neural network where some weight have be binarized in order to render the neural network more memory and energy efficient and quicker to evaluate one novelty of our technique be that it allows the verification of neural network that include both binarized and non-binarized component neural network verification be computationally very difficult and so we propose here various optimization integrate into our smt procedure a deduction step a well a an approach for parallelize verification query we implement our technique a an extension to the marabou framework and use it to evaluate the approach on popular binarized neural network architecture
"Exploring the Imposition of Synaptic Precision Restrictions For
  Evolutionary Synthesis of Deep Neural Networks","A key contributing factor to incredible success of deep neural networks has been the significant rise on massively parallel computing devices allowing researchers to greatly increase the size and depth of deep neural networks, leading to significant improvements in modeling accuracy. Although deeper, larger, or complex deep neural networks have shown considerable promise, the computational complexity of such networks is a major barrier to utilization in resource-starved scenarios. We explore the synaptogenesis of deep neural networks in the formation of efficient deep neural network architectures within an evolutionary deep intelligence framework, where a probabilistic generative modeling strategy is introduced to stochastically synthesize increasingly efficient yet effective offspring deep neural networks over generations, mimicking evolutionary processes such as heredity, random mutation, and natural selection in a probabilistic manner. In this study, we primarily explore the imposition of synaptic precision restrictions and its impact on the evolutionary synthesis of deep neural networks to synthesize more efficient network architectures tailored for resource-starved scenarios. Experimental results show significant improvements in synaptic efficiency (~10X decrease for GoogLeNet-based DetectNet) and inference speed (>5X increase for GoogLeNet-based DetectNet) while preserving modeling accuracy.","Mohammad Javad Shafiee, Francis Li, Alexander Wong",http://arxiv.org/abs/1707.00095v1,arXiv,a key contribute factor to incredible success of deep neural network have be the significant rise on massively parallel compute device allow researcher to greatly increase the size and depth of deep neural network lead to significant improvement in model accuracy although deeper large or complex deep neural network have show considerable promise the computational complexity of such network be a major barrier to utilization in resource-starved scenario we explore the synaptogenesis of deep neural network in the formation of efficient deep neural network architecture within an evolutionary deep intelligence framework where a probabilistic generative model strategy be introduce to stochastically synthesize increasingly efficient yet effective offspring deep neural network over generation mimic evolutionary process such a heredity random mutation and natural selection in a probabilistic manner in this study we primarily explore the imposition of synaptic precision restriction and it impact on the evolutionary synthesis of deep neural network to synthesize more efficient network architecture tailor for resource-starved scenario experimental result show significant improvement in synaptic efficiency 10x decrease for googlenet-based detectnet and inference speed 5x increase for googlenet-based detectnet while preserve model accuracy
"Scalable Training of Artificial Neural Networks with Adaptive Sparse
  Connectivity inspired by Network Science","Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd\H{o}s-R\'enyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.","Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta",http://arxiv.org/abs/1707.04780v2,arXiv,through the success of deep learn in various domain artificial neural network be currently among the most use artificial intelligence method take inspiration from the network property of biological neural network e g sparsity scale-freeness we argue that contrary to general practice artificial neural network too should not have fully-connected layer here we propose sparse evolutionary training of artificial neural network an algorithm which evolves an initial sparse topology erd s-r enyi random graph of two consecutive layer of neuron into a scale-free topology during learn our method replaces artificial neural network fully-connected layer with sparse one before training reduce quadratically the number of parameter with no decrease in accuracy we demonstrate our claim on restrict boltzmann machine multi-layer perceptrons and convolutional neural network for unsupervised and supervise learn on 15 datasets our approach have the potential to enable artificial neural network to scale up beyond what be currently possible
Mean Field Analysis of Neural Networks: A Law of Large Numbers,"Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis, and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (A) large network sizes and (B) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called ""propagation of chaos"".","Justin Sirignano, Konstantinos Spiliopoulos",http://arxiv.org/abs/1805.01053v4,arXiv,machine learn and in particular neural network model have revolutionize field such a image text and speech recognition today many important real-world application in these area be driven by neural network there be also grow application in engineering robotics medicine and finance despite their immense success in practice there be limited mathematical understand of neural network this paper illustrates how neural network can be study via stochastic analysis and develops approach for address some of the technical challenge which arise we analyze one-layer neural network in the asymptotic regime of simultaneously a large network size and b large number of stochastic gradient descent training iteration we rigorously prove that the empirical distribution of the neural network parameter converges to the solution of a nonlinear partial differential equation this result can be consider a law of large number for neural network in addition a consequence of our analysis be that the train parameter of the neural network asymptotically become independent a property which be commonly call propagation of chaos
Deep Convolutional Spiking Neural Networks for Image Classification,"Spiking neural networks are biologically plausible counterparts of the artificial neural networks, artificial neural networks are usually trained with stochastic gradient descent and spiking neural networks are trained with spike timing dependant plasticity. Training deep convolutional neural networks is a memory and power intensive job. Spiking networks could potentially help in reducing the power usage. There is a large pool of tools for one to chose to train artificial neural networks of any size, on the other hand all the available tools to simulate spiking neural networks are geared towards computational neuroscience applications and they are not suitable for real life applications. In this work we focus on implementing a spiking CNN using Tensorflow to examine behaviour of the network and empirically study the effect of various parameters on learning capabilities and also study catastrophic forgetting in the spiking CNN and weight initialization problem in R-STDP using MNIST and N-MNIST data sets.","Ruthvik Vaila, John Chiasson, Vishal Saxena",http://arxiv.org/abs/1903.12272v2,arXiv,spike neural network be biologically plausible counterpart of the artificial neural network artificial neural network be usually train with stochastic gradient descent and spike neural network be train with spike timing dependant plasticity training deep convolutional neural network be a memory and power intensive job spike network could potentially help in reduce the power usage there be a large pool of tool for one to chose to train artificial neural network of any size on the other hand all the available tool to simulate spike neural network be gear towards computational neuroscience application and they be not suitable for real life application in this work we focus on implement a spike cnn use tensorflow to examine behaviour of the network and empirically study the effect of various parameter on learn capability and also study catastrophic forget in the spike cnn and weight initialization problem in r-stdp use mnist and n-mnist data set
"Univariate ReLU neural network and its application in nonlinear system
  identification","ReLU (rectified linear units) neural network has received significant attention since its emergence. In this paper, a univariate ReLU (UReLU) neural network is proposed to both modelling the nonlinear dynamic system and revealing insights about the system. Specifically, the neural network consists of neurons with linear and UReLU activation functions, and the UReLU functions are defined as the ReLU functions respect to each dimension. The UReLU neural network is a single hidden layer neural network, and the structure is relatively simple. The initialization of the neural network employs the decoupling method, which provides a good initialization and some insight into the nonlinear system. Compared with normal ReLU neural network, the number of parameters of UReLU network is less, but it still provide a good approximation of the nonlinear dynamic system. The performance of the UReLU neural network is shown through a Hysteretic benchmark system: the Bouc-Wen system. Simulation results verify the effectiveness of the proposed method.","Xinglong Liang, Jun Xu",http://arxiv.org/abs/2003.02666v1,arXiv,relu rectify linear unit neural network have receive significant attention since it emergence in this paper a univariate relu urelu neural network be propose to both model the nonlinear dynamic system and reveal insight about the system specifically the neural network consists of neuron with linear and urelu activation function and the urelu function be define a the relu function respect to each dimension the urelu neural network be a single hidden layer neural network and the structure be relatively simple the initialization of the neural network employ the decouple method which provide a good initialization and some insight into the nonlinear system compare with normal relu neural network the number of parameter of urelu network be less but it still provide a good approximation of the nonlinear dynamic system the performance of the urelu neural network be show through a hysteretic benchmark system the bouc-wen system simulation result verify the effectiveness of the propose method
Feedforward Sequential Memory Neural Networks without Recurrent Feedback,"We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.","ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai",http://arxiv.org/abs/1510.02693v1,arXiv,we introduce a new structure for memory neural network call feedforward sequential memory network fsmn which can learn long-term dependency without use recurrent feedback the propose fsmn be a standard feedforward neural network equip with learnable sequential memory block in the hidden layer in this work we have apply fsmn to several language model lm task experimental result have show that the memory block in fsmn can learn effective representation of long history experiment have show that fsmn base language model can significantly outperform not only feedforward neural network fnn base lm but also the popular recurrent neural network rnn lm
Flow of Information in Feed-Forward Deep Neural Networks,"Feed-forward deep neural networks have been used extensively in various machine learning applications. Developing a precise understanding of the underling behavior of neural networks is crucial for their efficient deployment. In this paper, we use an information theoretic approach to study the flow of information in a neural network and to determine how entropy of information changes between consecutive layers. Moreover, using the Information Bottleneck principle, we develop a constrained optimization problem that can be used in the training process of a deep neural network. Furthermore, we determine a lower bound for the level of data representation that can be achieved in a deep neural network with an acceptable level of distortion.","Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan",http://arxiv.org/abs/1603.06220v1,arXiv,feed-forward deep neural network have be use extensively in various machine learn application develop a precise understand of the underling behavior of neural network be crucial for their efficient deployment in this paper we use an information theoretic approach to study the flow of information in a neural network and to determine how entropy of information change between consecutive layer moreover use the information bottleneck principle we develop a constrain optimization problem that can be use in the training process of a deep neural network furthermore we determine a low bound for the level of data representation that can be achieve in a deep neural network with an acceptable level of distortion
"Fixed-point optimization of deep neural networks with adaptive step size
  retraining","Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).","Sungho Shin, Yoonho Boo, Wonyong Sung",http://arxiv.org/abs/1702.08171v1,arXiv,fixed-point optimization of deep neural network play an important role in hardware base design and low-power implementation many deep neural network show fairly good performance even with 2- or 3-bit precision when quantize weight be fine-tuned by retrain we propose an improve fixedpoint optimization algorithm that estimate the quantization step size dynamically during the retrain in addition a gradual quantization scheme be also test which sequentially applies fixed-point optimization from high- to low-precision the experiment be conduct for feed-forward deep neural network ffdnns convolutional neural network cnns and recurrent neural network rnns
Ablation of a Robot's Brain: Neural Networks Under a Knife,"It is still not fully understood exactly how neural networks are able to solve the complex tasks that have recently pushed AI research forward. We present a novel method for determining how information is structured inside a neural network. Using ablation (a neuroscience technique for cutting away parts of a brain to determine their function), we approach several neural network architectures from a biological perspective. Through an analysis of this method's results, we examine important similarities between biological and artificial neural networks to search for the implicit knowledge locked away in the network's weights.","Peter E. Lillian, Richard Meyes, Tobias Meisen",http://arxiv.org/abs/1812.05687v2,arXiv,it be still not fully understood exactly how neural network be able to solve the complex task that have recently push ai research forward we present a novel method for determine how information be structure inside a neural network use ablation a neuroscience technique for cut away part of a brain to determine their function we approach several neural network architecture from a biological perspective through an analysis of this method s result we examine important similarity between biological and artificial neural network to search for the implicit knowledge lock away in the network s weight
Fourier Neural Networks: A Comparative Study,"We review neural network architectures which were motivated by Fourier series and integrals and which are referred to as Fourier neural networks. These networks are empirically evaluated in synthetic and real-world tasks. Neither of them outperforms the standard neural network with sigmoid activation function in the real-world tasks. All neural networks, both Fourier and the standard one, empirically demonstrate lower approximation error than the truncated Fourier series when it comes to an approximation of a known function of multiple variables.","Abylay Zhumekenov, Malika Uteuliyeva, Olzhas Kabdolov, Rustem Takhanov, Zhenisbek Assylbekov, Alejandro J. Castro",http://arxiv.org/abs/1902.03011v1,arXiv,we review neural network architecture which be motivate by fourier series and integral and which be refer to a fourier neural network these network be empirically evaluate in synthetic and real-world task neither of them outperforms the standard neural network with sigmoid activation function in the real-world task all neural network both fourier and the standard one empirically demonstrate low approximation error than the truncate fourier series when it come to an approximation of a know function of multiple variable
"Neural Networks, Hypersurfaces, and Radon Transforms","Connections between integration along hypersufaces, Radon transforms, and neural networks are exploited to highlight an integral geometric mathematical interpretation of neural networks. By analyzing the properties of neural networks as operators on probability distributions for observed data, we show that the distribution of outputs for any node in a neural network can be interpreted as a nonlinear projection along hypersurfaces defined by level surfaces over the input data space. We utilize these descriptions to provide new interpretation for phenomena such as nonlinearity, pooling, activation functions, and adversarial examples in neural network-based learning problems.","Soheil Kolouri, Xuwang Yin, Gustavo K. Rohde",http://arxiv.org/abs/1907.02220v1,arXiv,connection between integration along hypersufaces radon transforms and neural network be exploit to highlight an integral geometric mathematical interpretation of neural network by analyze the property of neural network a operator on probability distribution for observe data we show that the distribution of output for any node in a neural network can be interpret a a nonlinear projection along hypersurfaces define by level surface over the input data space we utilize these description to provide new interpretation for phenomenon such a nonlinearity pool activation function and adversarial example in neural network-based learn problem
"Statistical Tests and Confidential Intervals as Thresholds for Quantum
  Neural Networks","Some basic quantum neural networks were analyzed and constructed in the recent work of the author \cite{dndiep3}, published in International Journal of Theoretical Physics (2020). In particular the Least Quare Problem (LSP) and the Linear Regression Problem (LRP) was discussed. In this second paper we continue to analyze and construct the least square quantum neural network (LS-QNN), the polynomial interpolation quantum neural network (PI-QNN), the polynomial regression quantum neural network (PR-QNN) and chi-squared quantum neural network ($\chi^2$-QNN). We use the corresponding solution or tests as the threshold for the corresponding training rules.",Do Ngoc Diep,http://arxiv.org/abs/2001.11844v1,arXiv,some basic quantum neural network be analyze and construct in the recent work of the author publish in international journal of theoretical physic 2020 in particular the least quare problem lsp and the linear regression problem lrp be discuss in this second paper we continue to analyze and construct the least square quantum neural network ls-qnn the polynomial interpolation quantum neural network pi-qnn the polynomial regression quantum neural network pr-qnn and chi-squared quantum neural network -qnn we use the correspond solution or test a the threshold for the correspond training rule
Power Series Expansion Neural Network,"In this paper, we develop a new neural network family based on power series expansion, which is proved to achieve a better approximation accuracy in comparison with existing neural networks. This new set of neural networks embeds the power series expansion (PSE) into the neural network structure. Then it can improve the representation ability while preserving comparable computational cost by increasing the degree of PSE instead of increasing the depth or width. Both theoretical approximation and numerical results show the advantages of this new neural network.","Qipin Chen, Wenrui Hao, Juncai He",http://arxiv.org/abs/2102.13221v2,arXiv,in this paper we develop a new neural network family base on power series expansion which be prove to achieve a well approximation accuracy in comparison with exist neural network this new set of neural network embeds the power series expansion pse into the neural network structure then it can improve the representation ability while preserve comparable computational cost by increase the degree of pse instead of increase the depth or width both theoretical approximation and numerical result show the advantage of this new neural network
Symbiotic Hybrid Neural Network Watchdog For Outlier Detection,"Neural networks are largely black boxes. A neural network trained to classify fruit may classify a picture of a giraffe as a banana. A neural network watchdog's job is to identify such inputs, allowing a classifier to disregard such data. We investigate whether the watchdog should be separate from the neural network or symbiotically attached. We present empirical evidence that the symbiotic watchdog performs better than when the neural networks are disjoint.","Justin Bui, Robert J. Marks II",http://arxiv.org/abs/2103.00582v2,arXiv,neural network be largely black box a neural network train to classify fruit may classify a picture of a giraffe a a banana a neural network watchdog s job be to identify such input allow a classifier to disregard such data we investigate whether the watchdog should be separate from the neural network or symbiotically attach we present empirical evidence that the symbiotic watchdog performs well than when the neural network be disjoint
Stock Price Prediction using Dynamic Neural Networks,"This paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices. Neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic, non-linear, and seemingly random data, thus providing a mechanism to predict stock price movements much more precisely than many current techniques. Contemporary methods for stock analysis, including fundamental, technical, and regression techniques, are conversed and paralleled with the performance of neural networks. Also, the Efficient Market Hypothesis (EMH) is presented and contrasted with Chaos theory using neural networks. This paper will refute the EMH and support Chaos theory. Finally, recommendations for using neural networks in stock price prediction will be presented.",David Noel,http://arxiv.org/abs/2306.12969v1,arXiv,this paper will analyze and implement a time series dynamic neural network to predict daily closing stock price neural network possess unsurpassed ability in identify underlie pattern in chaotic non-linear and seemingly random data thus provide a mechanism to predict stock price movement much more precisely than many current technique contemporary method for stock analysis include fundamental technical and regression technique be converse and parallel with the performance of neural network also the efficient market hypothesis emh be present and contrast with chaos theory use neural network this paper will refute the emh and support chaos theory finally recommendation for use neural network in stock price prediction will be present
Equivariant neural networks and piecewise linear representation theory,"Equivariant neural networks are neural networks with symmetry. Motivated by the theory of group representations, we decompose the layers of an equivariant neural network into simple representations. The nonlinear activation functions lead to interesting nonlinear equivariant maps between simple representations. For example, the rectified linear unit (ReLU) gives rise to piecewise linear maps. We show that these considerations lead to a filtration of equivariant neural networks, generalizing Fourier series. This observation might provide a useful tool for interpreting equivariant neural networks.","Joel Gibson, Daniel Tubbenhauer, Geordie Williamson",http://arxiv.org/abs/2408.00949v2,arXiv,equivariant neural network be neural network with symmetry motivate by the theory of group representation we decompose the layer of an equivariant neural network into simple representation the nonlinear activation function lead to interest nonlinear equivariant map between simple representation for example the rectify linear unit relu give rise to piecewise linear map we show that these consideration lead to a filtration of equivariant neural network generalize fourier series this observation might provide a useful tool for interpret equivariant neural network
"An Efficient Method for Solving Lane Emden Equation using Legendre
  Neural Network","The aim of this manuscript is to address non-linear differential equations of the Lane Emden equation of second order using the shifted Legendre neural network (SLNN) method. Here all the equations are classified as singular initial value problems. To manage the singularity challenge, we employ an artificial neural network method. The approach utilizes a neural network of a single layer, where the hidden layer is omitted by enlarge the input using shifted Legendre polynomials. We apply a feed forward neural network model along with the principle of error back propagation. The effectiveness of the Legendre Neural Network model is demonstrated through LaneEmden equations.","Vijay Kumar Patel, Vivek Sharma, Nitin Kumar, Anoop Tiwari",http://arxiv.org/abs/2410.05409v1,arXiv,the aim of this manuscript be to address non-linear differential equation of the lane emden equation of second order use the shift legendre neural network slnn method here all the equation be classify a singular initial value problem to manage the singularity challenge we employ an artificial neural network method the approach utilizes a neural network of a single layer where the hidden layer be omit by enlarge the input use shift legendre polynomial we apply a feed forward neural network model along with the principle of error back propagation the effectiveness of the legendre neural network model be demonstrate through laneemden equation
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.",Alan Oursland,http://arxiv.org/abs/2410.19352v1,arXiv,this paper introduces a theoretical framework that connects neural network linear layer with the mahalanobis distance offering a new perspective on neural network interpretability while previous study have explore activation function primarily for performance optimization our work interprets these function through statistical distance measure a less explore area in neural network research by establish this connection we provide a foundation for develop more interpretable neural network model which be crucial for application require transparency although this work be theoretical and do not include empirical data the propose distance-based interpretation have the potential to enhance model robustness improve generalization and provide more intuitive explanation of neural network decision
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient dimension reduction (SDR), demonstrating that neural networks inherently perform SDR in regression tasks under appropriate rank regularizations. Specifically, the weights in the first layer span the central mean subspace. We establish the statistical consistency of the neural network-based estimator for the central mean subspace, underscoring the suitability of neural networks in addressing SDR-related challenges. Numerical experiments further validate our theoretical findings, and highlight the underlying capability of neural networks to facilitate SDR compared to the existing methods. Additionally, we discuss an extension to unravel the central subspace, broadening the scope of our investigation.","Shuntuo Xu, Zhou Yu",http://arxiv.org/abs/2412.19033v1,arXiv,this paper investigates the connection between neural network and sufficient dimension reduction sdr demonstrate that neural network inherently perform sdr in regression task under appropriate rank regularization specifically the weight in the first layer span the central mean subspace we establish the statistical consistency of the neural network-based estimator for the central mean subspace underscore the suitability of neural network in address sdr-related challenge numerical experiment far validate our theoretical finding and highlight the underlie capability of neural network to facilitate sdr compare to the exist method additionally we discus an extension to unravel the central subspace broadening the scope of our investigation
"Optimization of the Woodcock Particle Tracking Method Using Neural
  Network","The acceptance rate in Woodcock tracking algorithm is generalized to an arbitrary position-dependent variable $q(x)$. A neural network is used to optimize $q(x)$, and the FOM value is used as the loss function. This idea comes from physics informed neural network(PINN), where a neural network is used to represent the solution of differential equations. Here the neural network $q(x)$ should solve the functional equations that optimize FOM. For a 1d transmission problem with Gaussian absorption cross section, we observe a significant improvement of the FOM value compared to the constant $q$ case and the original Woodcock method. Generalizations of the neural network Woodcock(NNW) method to 3d voxel models are waiting to be explored.",Bingnan Zhang,http://arxiv.org/abs/2502.13620v1,arXiv,the acceptance rate in woodcock track algorithm be generalize to an arbitrary position-dependent variable a neural network be use to optimize and the fom value be use a the loss function this idea come from physic inform neural network pinn where a neural network be use to represent the solution of differential equation here the neural network should solve the functional equation that optimize fom for a 1d transmission problem with gaussian absorption cross section we observe a significant improvement of the fom value compare to the constant case and the original woodcock method generalization of the neural network woodcock nnw method to 3d voxel model be wait to be explore
Visual Character Recognition using Artificial Neural Networks,"The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks, which partially emulate human thinking in the domain of artificial intelligence. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. The document is expected to serve as a resource for learners and amateur investigators in pattern recognition, neural networking and related disciplines.",Shashank Araokar,http://arxiv.org/abs/cs/0505016v1,arXiv,the recognition of optical character be know to be one of the early application of artificial neural network which partially emulate human think in the domain of artificial intelligence in this paper a simplify neural approach to recognition of optical or visual character be portrayed and discuss the document be expect to serve a a resource for learner and amateur investigator in pattern recognition neural networking and related discipline
Self-organizing neural networks in classification and image recognition,Self-organizing neural networks are used for brick finding in OPERA experiment. Self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images.,"G. A. Ososkov, S. G. Dmitrievskiy, A. V. Stadnik",http://arxiv.org/abs/cs/0406047v1,arXiv,self-organizing neural network be use for brick find in opera experiment self-organizing neural network and wavelet analysis use for recognition and extraction of car number from image
"Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network",The article presents new results on the use of variable thresholds to increase the capacity of a feedback neural network. Non-binary networks are also considered in this analysis.,Praveen Kuruvada,http://arxiv.org/abs/1103.5081v2,arXiv,the article present new result on the use of variable threshold to increase the capacity of a feedback neural network non-binary network be also consider in this analysis
