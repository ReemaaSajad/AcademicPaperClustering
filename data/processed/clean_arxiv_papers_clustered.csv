title,abstract,authors,link,source,cleaned_abstract,kmeans_cluster,bertopic_topic
Lecture Notes: Neural Network Architectures,"These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",Evelyn Herberg,http://arxiv.org/abs/2304.05133v2,arXiv,these lecture notes provide an overview of neural network architectures from a mathematical point of view especially machine learning with neural networks is seen as an optimization problem covered are an introduction to neural networks and the following architectures feedforward neural network convolutional neural network resnet and recurrent neural network,0,-1
Self-Organizing Multilayered Neural Networks of Optimal Complexity,The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.,V. Schetinin,http://arxiv.org/abs/cs/0504056v1,arXiv,the principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set the method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics,3,-1
"Neural Network Processing Neural Networks: An efficient way to learn
  higher order functions","Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called ""Neural Network Processing Neural Networks"" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures.",Firat Tuna,http://arxiv.org/abs/1911.05640v2,arXiv,functions are rich in meaning and can be interpreted in a variety of ways neural networks were proven to be capable of approximating a large class of functions 1 in this paper we propose a new class of neural networks called neural network processing neural networks nnpnns which inputs neural networks and numerical values instead of just numerical values thus enabling neural networks to represent and process rich structures,3,0
"Guaranteed Quantization Error Computation for Neural Network Model
  Compression","Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.","Wesley Cooke, Zihao Mo, Weiming Xiang",http://arxiv.org/abs/2304.13812v1,arXiv,neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems the guaranteed output error computation problem for neural network compression with quantization is addressed in this paper a merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks then optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error finally a numerical example is proposed to validate the applicability and effectiveness of the proposed approach,3,0
Graph Structure of Neural Networks,"Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a ""sweet spot"" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.","Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",http://arxiv.org/abs/2007.06559v2,arXiv,neural networks are often represented as graphs of connections between neurons however despite their wide use there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance here we systematically investigate how does the graph structure of neural networks affect their predictive performance to this end we develop a novel graph-based representation of neural networks called relational graph where layers of neural network computation correspond to rounds of message exchange along the graph structure using this representation we show that 1 a sweet spot of relational graphs leads to neural networks with significantly improved predictive performance 2 neural network s performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph 3 our findings are consistent across many different tasks and datasets 4 the sweet spot can be identified efficiently 5 top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks our work opens new directions for the design of neural architectures and the understanding on neural networks in general,5,-1
"Hybrid Quantum-Classical Neural Networks for Downlink Beamforming
  Optimization","This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network. The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method. The second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network. Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters. The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices.","Juping Zhang, Gan Zheng, Toshiaki Koike-Akino, Kai-Kit Wong, Fraser Burton",http://arxiv.org/abs/2408.04747v1,arXiv,this paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system we aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance specifically we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system the first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network the classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method the second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network our results demonstrate the feasibility of the proposed hybrid neural networks and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters while the second method can achieve higher sum rate especially in presence of many users still with less training parameters the robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices,2,-1
"A Novel Neural Filter to Improve Accuracy of Neural Network Models of
  Dynamic Systems","The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state's accuracy. The neural filter's improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems. Numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance, outperforming the neural network predictions. Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network.","Parham Oveissi, Turibius Rozario, Ankit Goel",http://arxiv.org/abs/2409.13654v2,arXiv,the application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions despite their effectiveness neural networks face challenges in long-term predictions where the prediction error diverges over time thus degrading their accuracy this paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems motivated by the extended kalman filter the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state s accuracy the neural filter s improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance outperforming the neural network predictions furthermore it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model potentially decreasing the training cost and required data to train a neural network,3,-1
Cortex Neural Network: learning with Neural Network groups,"Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.",Liyao Gao,http://arxiv.org/abs/1804.03313v1,arXiv,neural network has been successfully applied to many real-world problems such as image recognition and machine translation however for the current architecture of neural networks it is hard to perform complex cognitive tasks for example to process the image and audio inputs together cortex as an important architecture in the brain is important for animals to perform the complex cognitive task we view the architecture of cortex in the brain as a missing part in the design of the current artificial neural network in this paper we purpose cortex neural network crtxnn the cortex neural network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system it is able to identify different tasks and solve them with different methods in our implementation the cortex neural network is able to process different cognitive tasks and perform reflection to get a higher accuracy we provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks our experiments proved its ability on the cortex neural network can reach accuracy by 98 32 on mnist and 62 on cifar10 at the same time which can promisingly reduce the loss by 40,3,-1
Parametrical Neural Networks and Some Other Similar Architectures,"A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",Leonid B. Litinskii,http://arxiv.org/abs/cs/0608073v1,arXiv,a review of works on associative neural networks accomplished during last four years in the institute of optical neural technologies ras is given the presentation is based on description of parametrical neural networks pnn for today pnn have record recognizing characteristics storage capacity noise immunity and speed of operation presentation of basic ideas and principles is accentuated,9,0
Assessing Intelligence in Artificial Neural Networks,"The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.","Nicholas J. Schaub, Nathan Hotaling",http://arxiv.org/abs/2006.02909v1,arXiv,the purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance to this end the concept of neural efficiency is introduced to measure neural layer utilization and a second metric called artificial intelligence quotient aiq was created to balance neural network performance and neural network efficiency to study aiq and neural efficiency two simple neural networks were trained on mnist a fully connected network lenet-300-100 and a convolutional neural network lenet-5 the lenet-5 network with the highest aiq was 2 32 less accurate but contained 30 912 times fewer parameters than the highest accuracy network both batch normalization and dropout layers were found to increase neural efficiency finally high aiq networks are shown to be memorization and overtraining resistant capable of learning proper digit classification with an accuracy of 92 51 even when 75 of the class labels are randomized these results demonstrate the utility of aiq and neural efficiency as metrics for balancing network performance and size,1,-1
Rational Neural Network Controllers,"Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is important to consider alternative architectures. This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops. Rational activation functions are replaced by a general rational neural network structure, which is convex in the neural network's parameters. A method is proposed to recover a stabilising controller from a Sum of Squares feasibility test. This approach is then applied to a refined rational neural network which is more compatible with Sum of Squares programming. Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty.","Matthew Newton, Antonis Papachristodoulou",http://arxiv.org/abs/2307.06287v1,arXiv,neural networks have shown great success in many machine learning related tasks due to their ability to act as general function approximators recent work has demonstrated the effectiveness of neural networks in control systems known as neural feedback loops most notably by using a neural network as a controller however one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks this means that unless they are designed properly they are not an ideal candidate for controllers due to issues with robustness and uncertainty which are pivotal aspects of control systems there has been initial work on robustness to both analyse and design dynamical systems with neural network controllers however one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks these structures may not be appropriate for neural network controllers and it is important to consider alternative architectures this paper considers rational neural networks and presents novel rational activation functions which can be used effectively in robustness problems for neural feedback loops rational activation functions are replaced by a general rational neural network structure which is convex in the neural network s parameters a method is proposed to recover a stabilising controller from a sum of squares feasibility test this approach is then applied to a refined rational neural network which is more compatible with sum of squares programming numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty,3,0
Asymptotic Theory of Expectile Neural Networks,"Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework.","Jinghang Lin, Xiaoxi Shen, Qing Lu",http://arxiv.org/abs/2011.01218v1,arXiv,neural networks are becoming an increasingly important tool in applications however neural networks are not widely used in statistical genetics in this paper we propose a new neural networks method called expectile neural networks when the size of parameter is too large the standard maximum likelihood procedures may not work we use sieve method to constrain parameter space and we prove its consistency and normality under nonparametric regression framework,3,0
"Combining Recurrent and Convolutional Neural Networks for Relation
  Classification","This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.","Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze",http://arxiv.org/abs/1605.07333v1,arXiv,this paper investigates two different neural architectures for the task of relation classification convolutional neural networks and recurrent neural networks for both models we demonstrate the effect of different architectural choices we present a new context representation for convolutional neural networks for relation classification extended middle context furthermore we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization finally we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results our neural models achieve state-of-the-art results on the semeval 2010 relation classification task,1,1
"A Comprehensive Review of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices","Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.","Kai Malcolm, Josue Casco-Rodriguez",http://arxiv.org/abs/2303.10780v2,arXiv,biological neural networks continue to inspire breakthroughs in neural network performance and yet one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible energy-efficient spiking neural networks whose potential is especially attractive for low-power mobile or otherwise hardware-constrained settings we present a literature review of recent developments in the interpretation optimization efficiency and accuracy of spiking neural networks key contributions include identification discussion and comparison of cutting-edge methods in spiking neural network optimization energy-efficiency and evaluation starting from first principles so as to be accessible to new practitioners,6,-1
"Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics","Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.",Sanjana Shetty,http://arxiv.org/abs/2401.10289v1,arXiv,neural networks have been employed for a wide range of processing applications like image processing motor control object detection and many others living neural networks offer advantages of lower power consumption faster processing and biological realism optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks this work proposes a simulated living neural network trained indirectly by backpropagating stdp based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms,3,-1
Convex Formulation of Overparameterized Deep Neural Networks,"Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice.","Cong Fang, Yihong Gu, Weizhong Zhang, Tong Zhang",http://arxiv.org/abs/1911.07626v1,arXiv,analysis of over-parameterized neural networks has drawn significant attention in recentyears it was shown that such systems behave like convex systems under various restrictedsettings such as for two-level neural networks and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations however there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice this paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained we generalize a new technique called neural feature repopulation originally introduced in fang et al 2019a for two-level neural networks to analyze deep neural networks it is shown that under suitable representations overparameterized deep neural networks are inherently convex and when optimized the system can learn effective features suitable for the underlying learning task under mild conditions this new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations therefore the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks empirical studies confirm that predictions of our theory are consistent with results observed in practice,1,1
"Approximate Bisimulation Relations for Neural Networks and Application
  to Assured Neural Network Compression","In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.","Weiming Xiang, Zhongzhu Shao",http://arxiv.org/abs/2202.01214v1,arXiv,in this paper we propose a concept of approximate bisimulation relation for feedforward neural networks in the framework of approximate bisimulation relation a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks the developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs then we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision i e assured neural networks compression at last using the assured neural network compression we accelerate the verification processes of acas xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach,3,0
"Optimal rates of approximation by shallow ReLU$^k$ neural networks and
  applications to nonparametric regression","We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\""older functions, which complements recent results for deep neural networks. It is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.","Yunfei Yang, Ding-Xuan Zhou",http://arxiv.org/abs/2304.01561v3,arXiv,we study the approximation capacity of some variation spaces corresponding to shallow relu neural networks it is shown that sufficiently smooth functions are contained in these spaces with finite variation norms for functions with less smoothness the approximation rates in terms of the variation norm are established using these results we are able to prove the optimal approximation rates in terms of the number of neurons for shallow relu neural networks it is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks cnns as applications we study convergence rates for nonparametric regression using three relu neural network models shallow neural network over-parameterized neural network and cnn in particular we show that shallow neural networks can achieve the minimax optimal rates for learning h older functions which complements recent results for deep neural networks it is also proven that over-parameterized deep or shallow neural networks can achieve nearly optimal rates for nonparametric regression,1,-1
"Understanding Vector-Valued Neural Networks and Their Relationship with
  Real and Hypercomplex-Valued Neural Networks","Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels. Finally, we show how V-nets, including hypercomplex-valued neural networks, can be implemented in current deep-learning libraries as real-valued networks.",Marcos Eduardo Valle,http://arxiv.org/abs/2309.07716v2,arXiv,despite the many successful applications of deep learning models for multidimensional signal and image processing most traditional neural networks process data represented by multidimensional arrays of real numbers the intercorrelation between feature channels is usually expected to be learned from the training data requiring numerous parameters and careful training in contrast vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels consequently they usually have fewer parameters and often undergo more robust training than traditional neural networks this paper aims to present a broad framework for vector-valued neural networks referred to as v-nets in this context hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties furthermore this paper explains the relationship between vector-valued and traditional neural networks precisely a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels finally we show how v-nets including hypercomplex-valued neural networks can be implemented in current deep-learning libraries as real-valued networks,1,0
One weird trick for parallelizing convolutional neural networks,I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.,Alex Krizhevsky,http://arxiv.org/abs/1404.5997v2,arXiv,i present a new way to parallelize the training of convolutional neural networks across multiple gpus the method scales significantly better than all alternatives when applied to modern convolutional neural networks,4,1
Geometric Decomposition of Feed Forward Neural Networks,"There have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. The field has exploded in the last decade, yet neural networks are still treated much like a black box. In this work we describe a structure that is inherent to a feed forward neural network. This will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. Our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective.",Sven Cattell,http://arxiv.org/abs/1612.02522v1,arXiv,there have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives the field has exploded in the last decade yet neural networks are still treated much like a black box in this work we describe a structure that is inherent to a feed forward neural network this will provide a framework for future work on neural networks to improve training algorithms compute the homology of the network and other applications our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective,3,0
Neural Networks Architecture Evaluation in a Quantum Computer,"In this work, we propose a quantum algorithm to evaluate neural networks architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks. Unlike conventional algorithms for evaluating neural network architectures, QNNAE does not depend on initialization of weights. The proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network. And its computational cost is equal to the computational cost to train a neural network.","Adenilton José da Silva, Rodolfo Luan F. de Oliveira",http://arxiv.org/abs/1711.04759v1,arXiv,in this work we propose a quantum algorithm to evaluate neural networks architectures named quantum neural network architecture evaluation qnnae the proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks unlike conventional algorithms for evaluating neural network architectures qnnae does not depend on initialization of weights the proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network and its computational cost is equal to the computational cost to train a neural network,2,-1
Building Compact and Robust Deep Neural Networks with Toeplitz Matrices,"Deep neural networks are state-of-the-art in a wide variety of tasks, however, they exhibit important limitations which hinder their use and deployment in real-world applications. When developing and training neural networks, the accuracy should not be the only concern, neural networks must also be cost-effective and reliable. Although accurate, large neural networks often lack these properties. This thesis focuses on the problem of training neural networks which are not only accurate but also compact, easy to train, reliable and robust to adversarial examples. To tackle these problems, we leverage the properties of structured matrices from the Toeplitz family to build compact and secure neural networks.",Alexandre Araujo,http://arxiv.org/abs/2109.00959v1,arXiv,deep neural networks are state-of-the-art in a wide variety of tasks however they exhibit important limitations which hinder their use and deployment in real-world applications when developing and training neural networks the accuracy should not be the only concern neural networks must also be cost-effective and reliable although accurate large neural networks often lack these properties this thesis focuses on the problem of training neural networks which are not only accurate but also compact easy to train reliable and robust to adversarial examples to tackle these problems we leverage the properties of structured matrices from the toeplitz family to build compact and secure neural networks,0,-1
Application of Neural Network in Optimization of Chemical Process,"Artificial neural network (ANN) has been widely used due to its strong nonlinear mapping ability, fault tolerance and self-learning ability. This article summarizes the development history of artificial neural networks, introduces three common neural network types, BP neural network, RBF neural network and convolutional neural network, and focuses on the practical application in chemical process optimization, especially the results achieved in multi-objective control optimization and process parameter improvement.","Fei Liang, Taowen Zhang",http://arxiv.org/abs/2110.04942v1,arXiv,artificial neural network ann has been widely used due to its strong nonlinear mapping ability fault tolerance and self-learning ability this article summarizes the development history of artificial neural networks introduces three common neural network types bp neural network rbf neural network and convolutional neural network and focuses on the practical application in chemical process optimization especially the results achieved in multi-objective control optimization and process parameter improvement,3,0
Nonlinear Systems Identification Using Deep Dynamic Neural Networks,"Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data","Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans",http://arxiv.org/abs/1610.01439v1,arXiv,neural networks are known to be effective function approximators recently deep neural networks have proven to be very effective in pattern recognition classification tasks and human-level control to model highly nonlinear realworld systems this paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior three deep neural network structures are trained on sequential data and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems we carry out similar evaluations on select publicly available system identification datasets we demonstrate that deep neural networks are effective model estimators from input-output data,3,-1
"Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks","The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.","Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira",http://arxiv.org/abs/2401.02277v2,arXiv,the universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision this theorem supports using neural networks for various applications including regression and classification tasks furthermore it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex- quaternion- tessarine- and clifford-valued neural networks however hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties this paper extends the universal approximation theorem for a wide range of vector-valued neural networks including hypercomplex-valued models as particular instances precisely we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras,3,0
"Stable Learning Using Spiking Neural Networks Equipped With Affine
  Encoders and Decoders","We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder; we refer to these as affine spiking neural networks. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights enables a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these affine spiking neural networks to standard machine learning benchmarks and reach competitive results. Finally, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.","A. Martina Neuman, Dominik Dold, Philipp Christian Petersen",http://arxiv.org/abs/2404.04549v3,arXiv,we study the learning problem associated with spiking neural networks specifically we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights equipped with an affine encoder and decoder we refer to these as affine spiking neural networks these neural networks are shown to depend continuously on their parameters which facilitates classical covering number-based generalization statements and supports stable gradient-based training we demonstrate that the positivity of the weights enables a wide range of expressivity results including rate-optimal approximation of smooth functions and dimension-independent approximation of barron regular functions in particular we show in theory and simulations that affine spiking neural networks are capable of approximating shallow relu neural networks furthermore we apply these affine spiking neural networks to standard machine learning benchmarks and reach competitive results finally we observe that from a generalization perspective contrary to feedforward neural networks or previous results for general spiking neural networks the depth has little to no adverse effect on the generalization capabilities,6,-1
Detecting Neural Trojans Through Merkle Trees,"Deep neural networks are utilized in a growing number of industries. Much of the current literature focuses on the applications of deep neural networks without discussing the security of the network itself. One security issue facing deep neural networks is neural trojans. Through a neural trojan, a malicious actor may force the deep neural network to act in unintended ways. Several potential defenses have been proposed, but they are computationally expensive, complex, or unusable in commercial applications. We propose Merkle trees as a novel way to detect and isolate neural trojans.",Joshua Strubel,http://arxiv.org/abs/2306.05368v1,arXiv,deep neural networks are utilized in a growing number of industries much of the current literature focuses on the applications of deep neural networks without discussing the security of the network itself one security issue facing deep neural networks is neural trojans through a neural trojan a malicious actor may force the deep neural network to act in unintended ways several potential defenses have been proposed but they are computationally expensive complex or unusable in commercial applications we propose merkle trees as a novel way to detect and isolate neural trojans,1,-1
"Performance Analysis Of Neural Network Models For Oxazolines And
  Oxazoles Derivatives Descriptor Dataset","Neural networks have been used successfully to a broad range of areas such as business, data mining, drug discovery and biology. In medicine, neural networks have been applied widely in medical diagnosis, detection and evaluation of new drugs and treatment cost estimation. In addition, neural networks have begin practice in data mining strategies for the aim of prediction, knowledge discovery. This paper will present the application of neural networks for the prediction and analysis of antitubercular activity of Oxazolines and Oxazoles derivatives. This study presents techniques based on the development of Single hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural network (GDBPNN), Gradient Descent Back propagation with momentum neural network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN) and Quantile regression neural network (QRNN) of artificial neural network (ANN) models Here, we comparatively evaluate the performance of five neural network techniques. The evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application. Cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties. Predictive accuracy was evaluated using the root mean squared error (RMSE), Coefficient determination(???), mean absolute error(MAE), mean percentage error(MPE) and relative square error(RSE). We found that all five neural network models were able to produce feasible models. QRNN model is outperforms with all statistical tests amongst other four models."," Doreswamy, Chanabasayya . M. Vastrad",http://arxiv.org/abs/1312.2853v1,arXiv,neural networks have been used successfully to a broad range of areas such as business data mining drug discovery and biology in medicine neural networks have been applied widely in medical diagnosis detection and evaluation of new drugs and treatment cost estimation in addition neural networks have begin practice in data mining strategies for the aim of prediction knowledge discovery this paper will present the application of neural networks for the prediction and analysis of antitubercular activity of oxazolines and oxazoles derivatives this study presents techniques based on the development of single hidden layer neural network shlffnn gradient descent back propagation neural network gdbpnn gradient descent back propagation with momentum neural network gdbpmnn back propagation with weight decay neural network bpwdnn and quantile regression neural network qrnn of artificial neural network ann models here we comparatively evaluate the performance of five neural network techniques the evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties predictive accuracy was evaluated using the root mean squared error rmse coefficient determination mean absolute error mae mean percentage error mpe and relative square error rse we found that all five neural network models were able to produce feasible models qrnn model is outperforms with all statistical tests amongst other four models,3,0
"Why Quantization Improves Generalization: NTK of Binary Weight Neural
  Networks","Quantized neural networks have drawn a lot of attention as they reduce the space and computational complexity during the inference. Moreover, there has been folklore that quantization acts as an implicit regularizer and thus can improve the generalizability of neural networks, yet no existing work formalizes this interesting folklore. In this paper, we take the binary weights in a neural network as random variables under stochastic rounding, and study the distribution propagation over different layers in the neural network. We propose a quasi neural network to approximate the distribution propagation, which is a neural network with continuous parameters and smooth activation function. We derive the neural tangent kernel (NTK) for this quasi neural network, and show that the eigenvalue of NTK decays at approximately exponential rate, which is comparable to that of Gaussian kernel with randomized scale. This in turn indicates that the Reproducing Kernel Hilbert Space (RKHS) of a binary weight neural network covers a strict subset of functions compared with the one with real value weights. We use experiments to verify that the quasi neural network we proposed can well approximate binary weight neural network. Furthermore, binary weight neural network gives a lower generalization gap compared with real value weight neural network, which is similar to the difference between Gaussian kernel and Laplace kernel.","Kaiqi Zhang, Ming Yin, Yu-Xiang Wang",http://arxiv.org/abs/2206.05916v1,arXiv,quantized neural networks have drawn a lot of attention as they reduce the space and computational complexity during the inference moreover there has been folklore that quantization acts as an implicit regularizer and thus can improve the generalizability of neural networks yet no existing work formalizes this interesting folklore in this paper we take the binary weights in a neural network as random variables under stochastic rounding and study the distribution propagation over different layers in the neural network we propose a quasi neural network to approximate the distribution propagation which is a neural network with continuous parameters and smooth activation function we derive the neural tangent kernel ntk for this quasi neural network and show that the eigenvalue of ntk decays at approximately exponential rate which is comparable to that of gaussian kernel with randomized scale this in turn indicates that the reproducing kernel hilbert space rkhs of a binary weight neural network covers a strict subset of functions compared with the one with real value weights we use experiments to verify that the quasi neural network we proposed can well approximate binary weight neural network furthermore binary weight neural network gives a lower generalization gap compared with real value weight neural network which is similar to the difference between gaussian kernel and laplace kernel,7,-1
Fourier Neural Networks for Function Approximation,"The success of Neural networks in providing miraculous results when applied to a wide variety of tasks is astonishing. Insight in the working can be obtained by studying the universal approximation property of neural networks. It is proved extensively that neural networks are universal approximators. Further it is proved that deep Neural networks are better approximators. It is specifically proved that for a narrow neural network to approximate a function which is otherwise implemented by a deep Neural network, the network take exponentially large number of neurons. In this work, we have implemented existing methodologies for a variety of synthetic functions and identified their deficiencies. Further, we examined that Fourier neural network is able to perform fairly good with only two layers in the neural network. A modified Fourier Neural network which has sinusoidal activation and two hidden layer is proposed and the results are tabulated.","R Subhash Chandra Bose, Kakarla Yaswanth",http://arxiv.org/abs/2111.08438v1,arXiv,the success of neural networks in providing miraculous results when applied to a wide variety of tasks is astonishing insight in the working can be obtained by studying the universal approximation property of neural networks it is proved extensively that neural networks are universal approximators further it is proved that deep neural networks are better approximators it is specifically proved that for a narrow neural network to approximate a function which is otherwise implemented by a deep neural network the network take exponentially large number of neurons in this work we have implemented existing methodologies for a variety of synthetic functions and identified their deficiencies further we examined that fourier neural network is able to perform fairly good with only two layers in the neural network a modified fourier neural network which has sinusoidal activation and two hidden layer is proposed and the results are tabulated,3,0
Bayesian Neural Networks: Essentials,"Bayesian neural networks utilize probabilistic layers that capture uncertainty over weights and activations, and are trained using Bayesian inference. Since these probabilistic layers are designed to be drop-in replacement of their deterministic counter parts, Bayesian neural networks provide a direct and natural way to extend conventional deep neural networks to support probabilistic deep learning. However, it is nontrivial to understand, design and train Bayesian neural networks due to their complexities. We discuss the essentials of Bayesian neural networks including duality (deep neural networks, probabilistic models), approximate Bayesian inference, Bayesian priors, Bayesian posteriors, and deep variational learning. We use TensorFlow Probability APIs and code examples for illustration. The main problem with Bayesian neural networks is that the architecture of deep neural networks makes it quite redundant, and costly, to account for uncertainty for a large number of successive layers. Hybrid Bayesian neural networks, which use few probabilistic layers judicially positioned in the networks, provide a practical solution.",Daniel T. Chang,http://arxiv.org/abs/2106.13594v1,arXiv,bayesian neural networks utilize probabilistic layers that capture uncertainty over weights and activations and are trained using bayesian inference since these probabilistic layers are designed to be drop-in replacement of their deterministic counter parts bayesian neural networks provide a direct and natural way to extend conventional deep neural networks to support probabilistic deep learning however it is nontrivial to understand design and train bayesian neural networks due to their complexities we discuss the essentials of bayesian neural networks including duality deep neural networks probabilistic models approximate bayesian inference bayesian priors bayesian posteriors and deep variational learning we use tensorflow probability apis and code examples for illustration the main problem with bayesian neural networks is that the architecture of deep neural networks makes it quite redundant and costly to account for uncertainty for a large number of successive layers hybrid bayesian neural networks which use few probabilistic layers judicially positioned in the networks provide a practical solution,8,-1
"Genetic cellular neural networks for generating three-dimensional
  geometry","There are a number of ways to procedurally generate interesting three-dimensional shapes, and a method where a cellular neural network is combined with a mesh growth algorithm is presented here. The aim is to create a shape from a genetic code in such a way that a crude search can find interesting shapes. Identical neural networks are placed at each vertex of a mesh which can communicate with neural networks on neighboring vertices. The output of the neural networks determine how the mesh grows, allowing interesting shapes to be produced emergently, mimicking some of the complexity of biological organism development. Since the neural networks' parameters can be freely mutated, the approach is amenable for use in a genetic algorithm.",Hugo Martay,http://arxiv.org/abs/1603.08551v1,arXiv,there are a number of ways to procedurally generate interesting three-dimensional shapes and a method where a cellular neural network is combined with a mesh growth algorithm is presented here the aim is to create a shape from a genetic code in such a way that a crude search can find interesting shapes identical neural networks are placed at each vertex of a mesh which can communicate with neural networks on neighboring vertices the output of the neural networks determine how the mesh grows allowing interesting shapes to be produced emergently mimicking some of the complexity of biological organism development since the neural networks parameters can be freely mutated the approach is amenable for use in a genetic algorithm,3,-1
Survey of Dropout Methods for Deep Neural Networks,"Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.","Alex Labach, Hojjat Salehinejad, Shahrokh Valaee",http://arxiv.org/abs/1904.13310v2,arXiv,dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice they have been successfully applied in neural network regularization model compression and in measuring the uncertainty of neural network outputs while original formulated for dense neural network layers recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers this paper summarizes the history of dropout methods their various applications and current areas of research interest important proposed methods are described in additional detail,7,0
"General Regression Neural Networks, Radial Basis Function Neural
  Networks, Support Vector Machines, and Feedforward Neural Networks","The aim of this project is to develop a code to discover the optimal sigma value that maximum the F1 score and the optimal sigma value that maximizes the accuracy and to find out if they are the same. Four algorithms which can be used to solve this problem are: Genetic Regression Neural Networks (GRNNs), Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines (SVMs) and Feedforward Neural Network (FFNNs).","Alison Jenkins, Vinika Gupta, Mary Lenoir",http://arxiv.org/abs/1911.07115v1,arXiv,the aim of this project is to develop a code to discover the optimal sigma value that maximum the f1 score and the optimal sigma value that maximizes the accuracy and to find out if they are the same four algorithms which can be used to solve this problem are genetic regression neural networks grnns radial based function rbf neural networks rbfnns support vector machines svms and feedforward neural network ffnns,3,0
On neural network kernels and the storage capacity problem,"In this short note, we reify the connection between work on the storage capacity problem in wide two-layer treelike neural networks and the rapidly-growing body of literature on kernel limits of wide neural networks. Concretely, we observe that the ""effective order parameter"" studied in the statistical mechanics literature is exactly equivalent to the infinite-width Neural Network Gaussian Process Kernel. This correspondence connects the expressivity and trainability of wide two-layer neural networks.","Jacob A. Zavatone-Veth, Cengiz Pehlevan",http://arxiv.org/abs/2201.04669v1,arXiv,in this short note we reify the connection between work on the storage capacity problem in wide two-layer treelike neural networks and the rapidly-growing body of literature on kernel limits of wide neural networks concretely we observe that the effective order parameter studied in the statistical mechanics literature is exactly equivalent to the infinite-width neural network gaussian process kernel this correspondence connects the expressivity and trainability of wide two-layer neural networks,1,0
Unary Coding for Neural Network Learning,This paper presents some properties of unary coding of significance for biological learning and instantaneously trained neural networks.,Subhash Kak,http://arxiv.org/abs/1009.4495v1,arXiv,this paper presents some properties of unary coding of significance for biological learning and instantaneously trained neural networks,9,-1
Deep Neural Networks - A Brief History,Introduction to deep neural networks and their history.,Krzysztof J. Cios,http://arxiv.org/abs/1701.05549v1,arXiv,introduction to deep neural networks and their history,0,-1
GPU Acceleration of Sparse Neural Networks,"In this paper, we use graphics processing units(GPU) to accelerate sparse and arbitrary structured neural networks. Sparse networks have nodes in the network that are not fully connected with nodes in preceding and following layers, and arbitrary structure neural networks have different number of nodes in each layers. Sparse Neural networks with arbitrary structures are generally created in the processes like neural network pruning and evolutionary machine learning strategies. We show that we can gain significant speedup for full activation of such neural networks using graphical processing units. We do a prepossessing step to determine dependency groups for all the nodes in a network, and use that information to guide the progression of activation in the neural network. Then we compute activation for each nodes in its own separate thread in the GPU, which allows for massive parallelization. We use CUDA framework to implement our approach and compare the results of sequential and GPU implementations. Our results show that the activation of sparse neural networks lends very well to GPU acceleration and can help speed up machine learning strategies which generate such networks or other processes that have similar structure.","Aavaas Gajurel, Sushil J. Louis, Frederick C Harris",http://arxiv.org/abs/2005.04347v1,arXiv,in this paper we use graphics processing units gpu to accelerate sparse and arbitrary structured neural networks sparse networks have nodes in the network that are not fully connected with nodes in preceding and following layers and arbitrary structure neural networks have different number of nodes in each layers sparse neural networks with arbitrary structures are generally created in the processes like neural network pruning and evolutionary machine learning strategies we show that we can gain significant speedup for full activation of such neural networks using graphical processing units we do a prepossessing step to determine dependency groups for all the nodes in a network and use that information to guide the progression of activation in the neural network then we compute activation for each nodes in its own separate thread in the gpu which allows for massive parallelization we use cuda framework to implement our approach and compare the results of sequential and gpu implementations our results show that the activation of sparse neural networks lends very well to gpu acceleration and can help speed up machine learning strategies which generate such networks or other processes that have similar structure,4,1
Spiking Neural Networks with Random Network Architecture,"The spiking neural network, known as the third generation neural network, is an important network paradigm. Due to its mode of information propagation that follows biological rationality, the spiking neural network has strong energy efficiency and has advantages in complex high-energy application scenarios. However, unlike the artificial neural network (ANN) which has a mature and unified framework, the SNN models and training methods have not yet been widely unified due to the discontinuous and non-differentiable property of the firing mechanism. Although several algorithms for training spiking neural networks have been proposed in the subsequent development process, some fundamental issues remain unsolved. Inspired by random network design, this work proposes a new architecture for spiking neural networks, RanSNN, where only part of the network weights need training and all the classic training methods can be adopted. Compared with traditional training methods for spiking neural networks, it greatly improves the training efficiency while ensuring the training performance, and also has good versatility and stability as validated by benchmark tests.","Zihan Dai, Huanfei Ma",http://arxiv.org/abs/2505.13622v1,arXiv,the spiking neural network known as the third generation neural network is an important network paradigm due to its mode of information propagation that follows biological rationality the spiking neural network has strong energy efficiency and has advantages in complex high-energy application scenarios however unlike the artificial neural network ann which has a mature and unified framework the snn models and training methods have not yet been widely unified due to the discontinuous and non-differentiable property of the firing mechanism although several algorithms for training spiking neural networks have been proposed in the subsequent development process some fundamental issues remain unsolved inspired by random network design this work proposes a new architecture for spiking neural networks ransnn where only part of the network weights need training and all the classic training methods can be adopted compared with traditional training methods for spiking neural networks it greatly improves the training efficiency while ensuring the training performance and also has good versatility and stability as validated by benchmark tests,6,-1
Neural Network Pruning as Spectrum Preserving Process,"Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.","Shibo Yao, Dantong Yu, Ioannis Koutis",http://arxiv.org/abs/2307.08982v1,arXiv,neural networks have achieved remarkable performance in various application domains nevertheless a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems it is highly desirable to obtain lightweight versions of neural networks for inference in edge devices many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space however a unified theoretical foundation for the problem mostly is missing in this paper we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum based on the analysis we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result we carefully design and conduct experiments to support our arguments hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights,1,1
On Hiding Neural Networks Inside Neural Networks,Modern neural networks often contain significantly more parameters than the size of their training data. We show that this excess capacity provides an opportunity for embedding secret machine learning models within a trained neural network. Our novel framework hides the existence of a secret neural network with arbitrary desired functionality within a carrier network. We prove theoretically that the secret network's detection is computationally infeasible and demonstrate empirically that the carrier network does not compromise the secret network's disguise. Our paper introduces a previously unknown steganographic technique that can be exploited by adversaries if left unchecked.,"Chuan Guo, Ruihan Wu, Kilian Q. Weinberger",http://arxiv.org/abs/2002.10078v3,arXiv,modern neural networks often contain significantly more parameters than the size of their training data we show that this excess capacity provides an opportunity for embedding secret machine learning models within a trained neural network our novel framework hides the existence of a secret neural network with arbitrary desired functionality within a carrier network we prove theoretically that the secret network s detection is computationally infeasible and demonstrate empirically that the carrier network does not compromise the secret network s disguise our paper introduces a previously unknown steganographic technique that can be exploited by adversaries if left unchecked,1,-1
Consistency of Neural Networks with Regularization,"Neural networks have attracted a lot of attention due to its success in applications such as natural language processing and computer vision. For large scale data, due to the tremendous number of parameters in neural networks, overfitting is an issue in training neural networks. To avoid overfitting, one common approach is to penalize the parameters especially the weights in neural networks. Although neural networks has demonstrated its advantages in many applications, the theoretical foundation of penalized neural networks has not been well-established. Our goal of this paper is to propose the general framework of neural networks with regularization and prove its consistency. Under certain conditions, the estimated neural network will converge to true underlying function as the sample size increases. The method of sieves and the theory on minimal neural networks are used to overcome the issue of unidentifiability for the parameters. Two types of activation functions: hyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been taken into consideration. Simulations have been conducted to verify the validation of theorem of consistency.","Xiaoxi Shen, Jinghang Lin",http://arxiv.org/abs/2207.01538v1,arXiv,neural networks have attracted a lot of attention due to its success in applications such as natural language processing and computer vision for large scale data due to the tremendous number of parameters in neural networks overfitting is an issue in training neural networks to avoid overfitting one common approach is to penalize the parameters especially the weights in neural networks although neural networks has demonstrated its advantages in many applications the theoretical foundation of penalized neural networks has not been well-established our goal of this paper is to propose the general framework of neural networks with regularization and prove its consistency under certain conditions the estimated neural network will converge to true underlying function as the sample size increases the method of sieves and the theory on minimal neural networks are used to overcome the issue of unidentifiability for the parameters two types of activation functions hyperbolic tangent function tanh and rectified linear unit relu have been taken into consideration simulations have been conducted to verify the validation of theorem of consistency,3,-1
"Understanding Weight Similarity of Neural Networks via Chain
  Normalization Rule and Hypothesis-Training-Testing","We present a weight similarity measure method that can quantify the weight similarity of non-convex neural networks. To understand the weight similarity of different trained models, we propose to extract the feature representation from the weights of neural networks. We first normalize the weights of neural networks by introducing a chain normalization rule, which is used for weight representation learning and weight similarity measure. We extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural networks. With the chain normalization rule and the new statistical inference, we study the weight similarity measure on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), and find that the weights of an identical neural network optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a similar local solution in a metric space. The weight similarity measure provides more insight into the local solutions of neural networks. Experiments on several datasets consistently validate the hypothesis of weight similarity measure.","Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai",http://arxiv.org/abs/2208.04369v1,arXiv,we present a weight similarity measure method that can quantify the weight similarity of non-convex neural networks to understand the weight similarity of different trained models we propose to extract the feature representation from the weights of neural networks we first normalize the weights of neural networks by introducing a chain normalization rule which is used for weight representation learning and weight similarity measure we extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural networks with the chain normalization rule and the new statistical inference we study the weight similarity measure on multi-layer perceptron mlp convolutional neural network cnn and recurrent neural network rnn and find that the weights of an identical neural network optimized with the stochastic gradient descent sgd algorithm converge to a similar local solution in a metric space the weight similarity measure provides more insight into the local solutions of neural networks experiments on several datasets consistently validate the hypothesis of weight similarity measure,1,1
"A New Constructive Method to Optimize Neural Network Architecture and
  Generalization","In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular value of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with every error in, and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks in by each error, and a function with jumping discontinuity point can be constructively approximated by a decay RBF neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data.","Hou Muzhou, Moon Ho Lee",http://arxiv.org/abs/1302.0324v1,arXiv,in this paper after analyzing the reasons of poor generalization and overfitting in neural networks we consider some noise data as a singular value of a continuous function - jump discontinuity point the continuous part can be approximated with the simplest neural networks which have good generalization performance and optimal network architecture by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training bp algorithm elm algorithm various constructive algorithm rbf approximation and svm at the same time we will construct rbf neural networks to fit the singular value with every error in and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay rbf neural networks in by each error and a function with jumping discontinuity point can be constructively approximated by a decay rbf neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance reduce the overfitting phenomenon by avoid fitting the noisy data,3,0
"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering. However, modern deep neural networks' growing energy requirements now increasingly limit their scaling and broader use. We propose a radical alternative for implementing deep neural network models: Physical Neural Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware Training to efficiently train sequences of controllable physical systems to act as deep neural networks. This method automatically trains the functionality of any sequence of real physical systems, directly, using backpropagation, the same technique used for modern deep neural networks. To illustrate their generality, we demonstrate physical neural networks with three diverse physical systems-optical, mechanical, and electrical. Physical neural networks may facilitate unconventional machine learning hardware that is orders of magnitude faster and more energy efficient than conventional electronic processors.","Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu, Peter L. McMahon",http://arxiv.org/abs/2104.13386v1,arXiv,deep neural networks have become a pervasive tool in science and engineering however modern deep neural networks growing energy requirements now increasingly limit their scaling and broader use we propose a radical alternative for implementing deep neural network models physical neural networks we introduce a hybrid physical-digital algorithm called physics-aware training to efficiently train sequences of controllable physical systems to act as deep neural networks this method automatically trains the functionality of any sequence of real physical systems directly using backpropagation the same technique used for modern deep neural networks to illustrate their generality we demonstrate physical neural networks with three diverse physical systems-optical mechanical and electrical physical neural networks may facilitate unconventional machine learning hardware that is orders of magnitude faster and more energy efficient than conventional electronic processors,7,-1
Graph Metanetworks for Processing Diverse Neural Architectures,"Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.","Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",http://arxiv.org/abs/2312.04501v2,arXiv,neural networks efficiently encode learned information within their parameters consequently many tasks can be unified by treating neural networks themselves as input data when doing so recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces however those works developed architectures tailored to specific networks such as mlps and cnns without normalization layers and generalizing such architectures to other types of networks can be challenging in this work we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input put simply we carefully build graphs representing the input neural networks and process the graphs using graph neural networks our approach graph metanetworks gmns generalizes to neural architectures where competing methods struggle such as multi-head attention layers normalization layers convolutional layers resnet blocks and group-equivariant linear layers we prove that gmns are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged we validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures,1,1
"Bayesian Learning of Neural Networks for Signal/Background
  Discrimination in Particle Physics","Neural networks are used extensively in classification problems in particle physics research. Since the training of neural networks can be viewed as a problem of inference, Bayesian learning of neural networks can provide more optimal and robust results than conventional learning methods. We have investigated the use of Bayesian neural networks for signal/background discrimination in the search for second generation leptoquarks at the Tevatron, as an example. We present a comparison of the results obtained from the conventional training of feedforward neural networks and networks trained with Bayesian methods.","Michael Pogwizd, Laura Jane Elgass, Pushpalatha C. Bhat",http://arxiv.org/abs/0707.0930v1,arXiv,neural networks are used extensively in classification problems in particle physics research since the training of neural networks can be viewed as a problem of inference bayesian learning of neural networks can provide more optimal and robust results than conventional learning methods we have investigated the use of bayesian neural networks for signal background discrimination in the search for second generation leptoquarks at the tevatron as an example we present a comparison of the results obtained from the conventional training of feedforward neural networks and networks trained with bayesian methods,8,-1
Deep Neural Networks for Pattern Recognition,"In the field of pattern recognition research, the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods. Deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification, object detection, and segmentation. This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Then we identify the operational processes and applications of conditional generative adversarial networks, which are being actively researched based on the bottom-up and top-down mechanisms, the most important functions of the human visual perception process. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.","Kyongsik Yun, Alexander Huyen, Thomas Lu",http://arxiv.org/abs/1809.09645v1,arXiv,in the field of pattern recognition research the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification object detection and segmentation this chapter introduces the basic structure of deep neural networks that simulate human neural networks then we identify the operational processes and applications of conditional generative adversarial networks which are being actively researched based on the bottom-up and top-down mechanisms the most important functions of the human visual perception process finally recent developments in training strategies for effective learning of complex deep neural networks are addressed,1,-1
"Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization","Understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning. Despite the close association of notable successes in this domain with the convergence of artificial neural networks, this concept remains predominantly theoretical. In reality, due to the non-convex nature of the optimization problems that artificial neural networks tackle, very few trained networks actually achieve convergence. To expand recent research efforts on artificial-neural-network convergence, this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network.",Thien An L. Nguyen,http://arxiv.org/abs/2403.05610v1,arXiv,understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning despite the close association of notable successes in this domain with the convergence of artificial neural networks this concept remains predominantly theoretical in reality due to the non-convex nature of the optimization problems that artificial neural networks tackle very few trained networks actually achieve convergence to expand recent research efforts on artificial-neural-network convergence this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network,3,0
"Hybrid deep neural network based prediction method for unsteady flows
  with moving boundaries","A novel hybrid deep neural network architecture is designed to capture the spatial-temporal features of unsteady flows around moving boundaries directly from high-dimensional unsteady flow fields data. The hybrid deep neural network is constituted by the convolutional neural network (CNN), improved convolutional Long-Short Term Memory neural network (ConvLSTM) and deconvolutional neural network (DeCNN). Flow fields at future time step can be predicted through flow fields by previous time steps and boundary positions at those steps by the novel hybrid deep neural network. Unsteady wake flows around a forced oscillation cylinder with various amplitudes are calculated to establish the datasets as training samples for training the hybrid deep neural networks. The trained hybrid deep neural networks are then tested by predicting the unsteady flow fields around a forced oscillation cylinder with new amplitude. The effect of neural network structure parameters on prediction accuracy was analyzed. The hybrid deep neural network, constituted by the best parameter combination, is used to predict the flow fields in the future time. The predicted flow fields are in good agreement with those calculated directly by computational fluid dynamic solver, which means that this kind of deep neural network can capture accurate spatial-temporal information from the spatial-temporal series of unsteady flows around moving boundaries. The result shows the potential capability of this kind novel hybrid deep neural network in flow control for vibrating cylinder, where the fast calculation of high-dimensional nonlinear unsteady flow around moving boundaries is needed.","Renkun Han, Zhong Zhang, Yixing Wang, Ziyang Liu, Yang Zhang, Gang Chen",http://arxiv.org/abs/2006.00690v1,arXiv,a novel hybrid deep neural network architecture is designed to capture the spatial-temporal features of unsteady flows around moving boundaries directly from high-dimensional unsteady flow fields data the hybrid deep neural network is constituted by the convolutional neural network cnn improved convolutional long-short term memory neural network convlstm and deconvolutional neural network decnn flow fields at future time step can be predicted through flow fields by previous time steps and boundary positions at those steps by the novel hybrid deep neural network unsteady wake flows around a forced oscillation cylinder with various amplitudes are calculated to establish the datasets as training samples for training the hybrid deep neural networks the trained hybrid deep neural networks are then tested by predicting the unsteady flow fields around a forced oscillation cylinder with new amplitude the effect of neural network structure parameters on prediction accuracy was analyzed the hybrid deep neural network constituted by the best parameter combination is used to predict the flow fields in the future time the predicted flow fields are in good agreement with those calculated directly by computational fluid dynamic solver which means that this kind of deep neural network can capture accurate spatial-temporal information from the spatial-temporal series of unsteady flows around moving boundaries the result shows the potential capability of this kind novel hybrid deep neural network in flow control for vibrating cylinder where the fast calculation of high-dimensional nonlinear unsteady flow around moving boundaries is needed,3,-1
"Perception-Informed Neural Networks: Beyond Physics-Informed Neural
  Networks","This article introduces Perception-Informed Neural Networks (PrINNs), a framework designed to incorporate perception-based information into neural networks, addressing both systems with known and unknown physics laws or differential equations. Moreover, PrINNs extend the concept of Physics-Informed Neural Networks (PINNs) and their variants, offering a platform for the integration of diverse forms of perception precisiation, including singular, probability distribution, possibility distribution, interval, and fuzzy graph. In fact, PrINNs allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions, enabling the creation of modern data-driven models. Some of the key contributions include Mixture of Experts Informed Neural Networks (MOEINNs), which combine heterogeneous expert knowledge into the network, and Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the incorporation of meta-information for enhanced model performance. Additionally, Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture, allowing online training without pre-training and eliminating the need for defuzzification. PrINNs represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches, enabling neural networks to learn from both structured physics laws and flexible perception-based rules. This approach empowers neural networks to operate in uncertain environments, model complex systems, and discover new forms of differential equations, making PrINNs a powerful tool for advancing computational science and engineering.","Mehran Mazandarani, Marzieh Najariyan",http://arxiv.org/abs/2505.03806v2,arXiv,this article introduces perception-informed neural networks prinns a framework designed to incorporate perception-based information into neural networks addressing both systems with known and unknown physics laws or differential equations moreover prinns extend the concept of physics-informed neural networks pinns and their variants offering a platform for the integration of diverse forms of perception precisiation including singular probability distribution possibility distribution interval and fuzzy graph in fact prinns allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions enabling the creation of modern data-driven models some of the key contributions include mixture of experts informed neural networks moeinns which combine heterogeneous expert knowledge into the network and transformed-knowledge informed neural networks tkinns which facilitate the incorporation of meta-information for enhanced model performance additionally fuzzy-informed neural networks finns as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture allowing online training without pre-training and eliminating the need for defuzzification prinns represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches enabling neural networks to learn from both structured physics laws and flexible perception-based rules this approach empowers neural networks to operate in uncertain environments model complex systems and discover new forms of differential equations making prinns a powerful tool for advancing computational science and engineering,8,-1
Neural network learning dynamics in a path integral framework,A path-integral formalism is proposed for studying the dynamical evolution in time of patterns in an artificial neural network in the presence of noise. An effective cost function is constructed which determines the unique global minimum of the neural network system. The perturbative method discussed also provides a way for determining the storage capacity of the network.,J. Balakrishnan,http://arxiv.org/abs/cond-mat/0308503v1,arXiv,a path-integral formalism is proposed for studying the dynamical evolution in time of patterns in an artificial neural network in the presence of noise an effective cost function is constructed which determines the unique global minimum of the neural network system the perturbative method discussed also provides a way for determining the storage capacity of the network,5,-1
Feedforward Neural Networks for Caching: Enough or Too Much?,"We propose a caching policy that uses a feedforward neural network (FNN) to predict content popularity. Our scheme outperforms popular eviction policies like LRU or ARC, but also a new policy relying on the more complex recurrent neural networks. At the same time, replacing the FNN predictor with a naive linear estimator does not degrade caching performance significantly, questioning then the role of neural networks for these applications.","Vladyslav Fedchenko, Giovanni Neglia, Bruno Ribeiro",http://arxiv.org/abs/1810.06930v1,arXiv,we propose a caching policy that uses a feedforward neural network fnn to predict content popularity our scheme outperforms popular eviction policies like lru or arc but also a new policy relying on the more complex recurrent neural networks at the same time replacing the fnn predictor with a naive linear estimator does not degrade caching performance significantly questioning then the role of neural networks for these applications,7,-1
"Deep Kronecker neural networks: A general framework for neural networks
  with adaptive activation functions","We propose a new type of neural networks, Kronecker neural networks (KNNs), that form a general framework for neural networks with adaptive activation functions. KNNs employ the Kronecker product, which provides an efficient way of constructing a very wide network while keeping the number of parameters low. Our theoretical analysis reveals that under suitable conditions, KNNs induce a faster decay of the loss than that by the feed-forward networks. This is also empirically verified through a set of computational examples. Furthermore, under certain technical assumptions, we establish global convergence of gradient descent for KNNs. As a specific case, we propose the Rowdy activation function that is designed to get rid of any saturation region by injecting sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy activation function can be employed in any neural network architecture like feed-forward neural networks, Recurrent neural networks, Convolutional neural networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated through various computational experiments including function approximation using feed-forward neural networks, solution inference of partial differential equations using the physics-informed neural networks, and standard deep learning benchmark problems using convolutional and fully-connected neural networks.","Ameya D. Jagtap, Yeonjong Shin, Kenji Kawaguchi, George Em Karniadakis",http://arxiv.org/abs/2105.09513v2,arXiv,we propose a new type of neural networks kronecker neural networks knns that form a general framework for neural networks with adaptive activation functions knns employ the kronecker product which provides an efficient way of constructing a very wide network while keeping the number of parameters low our theoretical analysis reveals that under suitable conditions knns induce a faster decay of the loss than that by the feed-forward networks this is also empirically verified through a set of computational examples furthermore under certain technical assumptions we establish global convergence of gradient descent for knns as a specific case we propose the rowdy activation function that is designed to get rid of any saturation region by injecting sinusoidal fluctuations which include trainable parameters the proposed rowdy activation function can be employed in any neural network architecture like feed-forward neural networks recurrent neural networks convolutional neural networks etc the effectiveness of knns with rowdy activation is demonstrated through various computational experiments including function approximation using feed-forward neural networks solution inference of partial differential equations using the physics-informed neural networks and standard deep learning benchmark problems using convolutional and fully-connected neural networks,7,-1
The Representation Theory of Neural Networks,"In this work, we show that neural networks can be represented via the mathematical theory of quiver representations. More specifically, we prove that a neural network is a quiver representation with activation functions, a mathematical object that we represent using a network quiver. Also, we show that network quivers gently adapt to common neural network concepts such as fully-connected layers, convolution operations, residual connections, batch normalization, pooling operations and even randomly wired neural networks. We show that this mathematical representation is by no means an approximation of what neural networks are as it exactly matches reality. This interpretation is algebraic and can be studied with algebraic methods. We also provide a quiver representation model to understand how a neural network creates representations from the data. We show that a neural network saves the data as quiver representations, and maps it to a geometrical space called the moduli space, which is given in terms of the underlying oriented graph of the network, i.e., its quiver. This results as a consequence of our defined objects and of understanding how the neural network computes a prediction in a combinatorial and algebraic way. Overall, representing neural networks through the quiver representation theory leads to 9 consequences and 4 inquiries for future research that we believe are of great interest to better understand what neural networks are and how they work.","Marco Antonio Armenta, Pierre-Marc Jodoin",http://arxiv.org/abs/2007.12213v2,arXiv,in this work we show that neural networks can be represented via the mathematical theory of quiver representations more specifically we prove that a neural network is a quiver representation with activation functions a mathematical object that we represent using a network quiver also we show that network quivers gently adapt to common neural network concepts such as fully-connected layers convolution operations residual connections batch normalization pooling operations and even randomly wired neural networks we show that this mathematical representation is by no means an approximation of what neural networks are as it exactly matches reality this interpretation is algebraic and can be studied with algebraic methods we also provide a quiver representation model to understand how a neural network creates representations from the data we show that a neural network saves the data as quiver representations and maps it to a geometrical space called the moduli space which is given in terms of the underlying oriented graph of the network i e its quiver this results as a consequence of our defined objects and of understanding how the neural network computes a prediction in a combinatorial and algebraic way overall representing neural networks through the quiver representation theory leads to 9 consequences and 4 inquiries for future research that we believe are of great interest to better understand what neural networks are and how they work,1,-1
Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise,"Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.","Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh",http://arxiv.org/abs/1906.02355v1,arXiv,neural ordinary differential equation neural ode has been proposed as a continuous approximation to the resnet architecture some commonly used regularization mechanisms in discrete neural networks e g dropout gaussian noise are missing in current neural ode networks in this paper we propose a new continuous neural network framework called neural stochastic differential equation neural sde network which naturally incorporates various commonly used regularization mechanisms based on random noise injection our framework can model various types of noise injection frequently used in discrete networks for regularization purpose such as dropout and additive multiplicative noise in each block we provide theoretical analysis explaining the improved robustness of neural sde models against input perturbations adversarial attacks furthermore we demonstrate that the neural sde network can achieve better generalization than the neural ode and is more resistant to adversarial and non-adversarial input perturbations,7,-1
Can a powerful neural network be a teacher for a weaker neural network?,"The transfer learning technique is widely used to learning in one context and applying it to another, i.e. the capacity to apply acquired knowledge and skills to new situations. But is it possible to transfer the learning from a deep neural network to a weaker neural network? Is it possible to improve the performance of a weak neural network using the knowledge acquired by a more powerful neural network? In this work, during the training process of a weak network, we add a loss function that minimizes the distance between the features previously learned from a strong neural network with the features that the weak network must try to learn. To demonstrate the effectiveness and robustness of our approach, we conducted a large number of experiments using three known datasets and demonstrated that a weak neural network can increase its performance if its learning process is driven by a more powerful neural network.","Nicola Landro, Ignazio Gallo, Riccardo La Grassa",http://arxiv.org/abs/2005.00393v2,arXiv,the transfer learning technique is widely used to learning in one context and applying it to another i e the capacity to apply acquired knowledge and skills to new situations but is it possible to transfer the learning from a deep neural network to a weaker neural network is it possible to improve the performance of a weak neural network using the knowledge acquired by a more powerful neural network in this work during the training process of a weak network we add a loss function that minimizes the distance between the features previously learned from a strong neural network with the features that the weak network must try to learn to demonstrate the effectiveness and robustness of our approach we conducted a large number of experiments using three known datasets and demonstrated that a weak neural network can increase its performance if its learning process is driven by a more powerful neural network,1,-1
"Message Passing Variational Autoregressive Network for Solving
  Intractable Ising Models","Many deep neural networks have been used to solve Ising models, including autoregressive neural networks, convolutional neural networks, recurrent neural networks, and graph neural networks. Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems. Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking. Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables. The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures. The advantages also come from the great mitigation of mode collapse during the training process of deep neural networks. Considering these extremely difficult problems to be solved, our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems.","Qunlong Ma, Zhi Ma, Jinlong Xu, Hairui Zhang, Ming Gao",http://arxiv.org/abs/2404.06225v1,arXiv,many deep neural networks have been used to solve ising models including autoregressive neural networks convolutional neural networks recurrent neural networks and graph neural networks learning a probability distribution of energy configuration or finding the ground states of a disordered fully connected ising model is essential for statistical mechanics and np-hard problems despite tremendous efforts a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking here we propose a variational autoregressive architecture with a message passing mechanism which can effectively utilize the interactions between spin variables the new network trained under an annealing framework outperforms existing methods in solving several prototypical ising spin hamiltonians especially for larger spin systems at low temperatures the advantages also come from the great mitigation of mode collapse during the training process of deep neural networks considering these extremely difficult problems to be solved our method extends the current computational limits of unsupervised neural networks to solve combinatorial optimization problems,7,1
Dynamics of Deep Neural Networks and Neural Tangent Hierarchy,"The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel (NTK) as introduced in [20], where it was proven that in the infinite width limit the NTK converges to an explicit limiting kernel and it stays constant during training. The NTK was also implicit in some other recent papers [6,13,14]. In the overparametrization regime, a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting NTK. And the gradient descent achieves zero training loss for a deep overparameterized neural network. However, it was observed in [5] that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. This performance gap is likely to originate from the change of the NTK along training due to the finite width effect. The change of the NTK along the training is central to describe the generalization features of deep neural networks.   In the current paper, we study the dynamic of the NTK for finite width deep fully-connected neural networks. We derive an infinite hierarchy of ordinary differential equations, the neural tangent hierarchy (NTH) which captures the gradient descent dynamic of the deep neural network. Moreover, under certain conditions on the neural network width and the data set dimension, we prove that the truncated hierarchy of NTH approximates the dynamic of the NTK up to arbitrary precision. This description makes it possible to directly study the change of the NTK for deep neural networks, and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting NTK.","Jiaoyang Huang, Horng-Tzer Yau",http://arxiv.org/abs/1909.08156v1,arXiv,the evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel ntk as introduced in 20 where it was proven that in the infinite width limit the ntk converges to an explicit limiting kernel and it stays constant during training the ntk was also implicit in some other recent papers 6 13 14 in the overparametrization regime a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting ntk and the gradient descent achieves zero training loss for a deep overparameterized neural network however it was observed in 5 that there is a performance gap between the kernel regression using the limiting ntk and the deep neural networks this performance gap is likely to originate from the change of the ntk along training due to the finite width effect the change of the ntk along the training is central to describe the generalization features of deep neural networks in the current paper we study the dynamic of the ntk for finite width deep fully-connected neural networks we derive an infinite hierarchy of ordinary differential equations the neural tangent hierarchy nth which captures the gradient descent dynamic of the deep neural network moreover under certain conditions on the neural network width and the data set dimension we prove that the truncated hierarchy of nth approximates the dynamic of the ntk up to arbitrary precision this description makes it possible to directly study the change of the ntk for deep neural networks and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting ntk,1,1
"Novel Kernel Models and Exact Representor Theory for Neural Networks
  Beyond the Over-Parameterized Regime","This paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width, depth and topology, assuming only finite-energy neural activations; and a novel representor theory for neural networks in terms of a matrix-valued kernel. The first model is exact (un-approximated) and global, casting the neural network as an elements in a reproducing kernel Banach space (RKBS); we use this model to provide tight bounds on Rademacher complexity. The second model is exact and local, casting the change in neural network function resulting from a bounded change in weights and biases (ie. a training step) in reproducing kernel Hilbert space (RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model provides insight into model adaptation through tight bounds on Rademacher complexity of network adaptation. We also prove that the neural tangent kernel (NTK) is a first-order approximation of the LiNK kernel. Finally, and noting that the LiNK does not provide a representor theory for technical reasons, we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel (LeNK). This representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models. Throughout the paper (a) feedforward ReLU networks and (b) residual networks (ResNet) are used as illustrative examples.","Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh",http://arxiv.org/abs/2405.15254v1,arXiv,this paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width depth and topology assuming only finite-energy neural activations and a novel representor theory for neural networks in terms of a matrix-valued kernel the first model is exact un-approximated and global casting the neural network as an elements in a reproducing kernel banach space rkbs we use this model to provide tight bounds on rademacher complexity the second model is exact and local casting the change in neural network function resulting from a bounded change in weights and biases ie a training step in reproducing kernel hilbert space rkhs in terms of a local-intrinsic neural kernel link this local model provides insight into model adaptation through tight bounds on rademacher complexity of network adaptation we also prove that the neural tangent kernel ntk is a first-order approximation of the link kernel finally and noting that the link does not provide a representor theory for technical reasons we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel lenk this representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models throughout the paper a feedforward relu networks and b residual networks resnet are used as illustrative examples,1,-1
Feature Weight Tuning for Recursive Neural Networks,"This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform ""weight tuning"" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.",Jiwei Li,http://arxiv.org/abs/1412.3714v2,arXiv,this paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence in other words to perform weight tuning for higher-level representation acquisition we propose two models weighted neural network wnn and binary-expectation neural network benn which automatically control how much one specific unit contributes to the higher-level representation the proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks experimental results demonstrate the significant improvement over standard neural models,1,-1
"Neural Nets via Forward State Transformation and Backward Loss
  Transformation","This article studies (multilayer perceptron) neural networks with an emphasis on the transformations involved --- both forward and backward --- in order to develop a semantical/logical perspective that is in line with standard program semantics. The common two-pass neural network training algorithms make this viewpoint particularly fitting. In the forward direction, neural networks act as state transformers. In the reverse direction, however, neural networks change losses of outputs to losses of inputs, thereby acting like a (real-valued) predicate transformer. In this way, backpropagation is functorial by construction, as shown earlier in recent other work. We illustrate this perspective by training a simple instance of a neural network.","Bart Jacobs, David Sprunger",http://arxiv.org/abs/1803.09356v1,arXiv,this article studies multilayer perceptron neural networks with an emphasis on the transformations involved --- both forward and backward --- in order to develop a semantical logical perspective that is in line with standard program semantics the common two-pass neural network training algorithms make this viewpoint particularly fitting in the forward direction neural networks act as state transformers in the reverse direction however neural networks change losses of outputs to losses of inputs thereby acting like a real-valued predicate transformer in this way backpropagation is functorial by construction as shown earlier in recent other work we illustrate this perspective by training a simple instance of a neural network,3,-1
Neural Networks Processing Mean Values of Random Variables,"We introduce a class of neural networks derived from probabilistic models in the form of Bayesian belief networks. By imposing additional assumptions about the nature of the probabilistic models represented in the belief networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian belief networks, providing distributed versions of probabilistic models.","M. J. Barber, J. W. Clark, C. H. Anderson",http://arxiv.org/abs/cond-mat/0407436v1,arXiv,we introduce a class of neural networks derived from probabilistic models in the form of bayesian belief networks by imposing additional assumptions about the nature of the probabilistic models represented in the belief networks we derive neural networks with standard dynamics that require no training to determine the synaptic weights that can pool multiple sources of evidence and that deal cleanly and consistently with inconsistent or contradictory evidence the presented neural networks capture many properties of bayesian belief networks providing distributed versions of probabilistic models,8,-1
Stability of a neural network model with small-world connections,"Small-world networks are highly clustered networks with small distances among the nodes. There are many biological neural networks that present this kind of connections. There are no special weightings in the connections of most existing small-world network models. However, this kind of simply-connected models cannot characterize biological neural networks, in which there are different weights in synaptic connections. In this paper, we present a neural network model with weighted small-world connections, and further investigate the stability of this model.","Chunguang Li, Guanrong Chen",http://arxiv.org/abs/cond-mat/0410492v1,arXiv,small-world networks are highly clustered networks with small distances among the nodes there are many biological neural networks that present this kind of connections there are no special weightings in the connections of most existing small-world network models however this kind of simply-connected models cannot characterize biological neural networks in which there are different weights in synaptic connections in this paper we present a neural network model with weighted small-world connections and further investigate the stability of this model,5,-1
Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks,"Mobility prediction allows estimating the stability of paths in a mobile wireless Ad Hoc networks. Identifying stable paths helps to improve routing by reducing the overhead and the number of connection interruptions. In this paper, we introduce a neural network based method for mobility prediction in Ad Hoc networks. This method consists of a multi-layer and recurrent neural network using back propagation through time algorithm for training.","Heni Kaaniche, Farouk Kamoun",http://arxiv.org/abs/1004.4610v1,arXiv,mobility prediction allows estimating the stability of paths in a mobile wireless ad hoc networks identifying stable paths helps to improve routing by reducing the overhead and the number of connection interruptions in this paper we introduce a neural network based method for mobility prediction in ad hoc networks this method consists of a multi-layer and recurrent neural network using back propagation through time algorithm for training,5,-1
Designing neural networks that process mean values of random variables,"We introduce a class of neural networks derived from probabilistic models in the form of Bayesian networks. By imposing additional assumptions about the nature of the probabilistic models represented in the networks, we derive neural networks with standard dynamics that require no training to determine the synaptic weights, that perform accurate calculation of the mean values of the random variables, that can pool multiple sources of evidence, and that deal cleanly and consistently with inconsistent or contradictory evidence. The presented neural networks capture many properties of Bayesian networks, providing distributed versions of probabilistic models.","Michael J. Barber, John W. Clark",http://arxiv.org/abs/1004.5326v1,arXiv,we introduce a class of neural networks derived from probabilistic models in the form of bayesian networks by imposing additional assumptions about the nature of the probabilistic models represented in the networks we derive neural networks with standard dynamics that require no training to determine the synaptic weights that perform accurate calculation of the mean values of the random variables that can pool multiple sources of evidence and that deal cleanly and consistently with inconsistent or contradictory evidence the presented neural networks capture many properties of bayesian networks providing distributed versions of probabilistic models,8,-1
A Primer on Neural Network Models for Natural Language Processing,"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",Yoav Goldberg,http://arxiv.org/abs/1510.00726v1,arXiv,over the past few years neural networks have re-emerged as powerful machine-learning models yielding state-of-the-art results in fields such as image recognition and speech processing more recently neural network models started to be applied also to textual natural language signals again with very promising results this tutorial surveys neural network models from the perspective of natural language processing research in an attempt to bring natural-language researchers up to speed with the neural techniques the tutorial covers input encoding for natural language tasks feed-forward networks convolutional networks recurrent networks and recursive networks as well as the computation graph abstraction for automatic gradient computation,7,-1
On the Relative Expressiveness of Bayesian and Neural Networks,"A neural network computes a function. A central property of neural networks is that they are ""universal approximators:"" for a given continuous function, there exists a neural network that can approximate it arbitrarily well, given enough neurons (and some additional assumptions). In contrast, a Bayesian network is a model, but each of its queries can be viewed as computing a function. In this paper, we identify some key distinctions between the functions computed by neural networks and those by marginal Bayesian network queries, showing that the former are more expressive than the latter. Moreover, we propose a simple augmentation to Bayesian networks (a testing operator), which enables their marginal queries to become ""universal approximators.""","Arthur Choi, Ruocheng Wang, Adnan Darwiche",http://arxiv.org/abs/1812.08957v1,arXiv,a neural network computes a function a central property of neural networks is that they are universal approximators for a given continuous function there exists a neural network that can approximate it arbitrarily well given enough neurons and some additional assumptions in contrast a bayesian network is a model but each of its queries can be viewed as computing a function in this paper we identify some key distinctions between the functions computed by neural networks and those by marginal bayesian network queries showing that the former are more expressive than the latter moreover we propose a simple augmentation to bayesian networks a testing operator which enables their marginal queries to become universal approximators,8,-1
Power Law in Sparsified Deep Neural Networks,"The power law has been observed in the degree distributions of many biological neural networks. Sparse deep neural networks, which learn an economical representation from the data, resemble biological neural networks in many ways. In this paper, we study if these artificial networks also exhibit properties of the power law. Experimental results on two popular deep learning models, namely, multilayer perceptrons and convolutional neural networks, are affirmative. The power law is also naturally related to preferential attachment. To study the dynamical properties of deep networks in continual learning, we propose an internal preferential attachment model to explain how the network topology evolves. Experimental results show that with the arrival of a new task, the new connections made follow this preferential attachment process.","Lu Hou, James T. Kwok",http://arxiv.org/abs/1805.01891v1,arXiv,the power law has been observed in the degree distributions of many biological neural networks sparse deep neural networks which learn an economical representation from the data resemble biological neural networks in many ways in this paper we study if these artificial networks also exhibit properties of the power law experimental results on two popular deep learning models namely multilayer perceptrons and convolutional neural networks are affirmative the power law is also naturally related to preferential attachment to study the dynamical properties of deep networks in continual learning we propose an internal preferential attachment model to explain how the network topology evolves experimental results show that with the arrival of a new task the new connections made follow this preferential attachment process,5,-1
Kernel-based Translations of Convolutional Networks,"Convolutional Neural Networks, as most artificial neural networks, are commonly viewed as methods different in essence from kernel-based methods. We provide a systematic translation of Convolutional Neural Networks (ConvNets) into their kernel-based counterparts, Convolutional Kernel Networks (CKNs), and demonstrate that this perception is unfounded both formally and empirically. We show that, given a Convolutional Neural Network, we can design a corresponding Convolutional Kernel Network, easily trainable using a new stochastic gradient algorithm based on an accurate gradient computation, that performs on par with its Convolutional Neural Network counterpart. We present experimental results supporting our claims on landmark ConvNet architectures comparing each ConvNet to its CKN counterpart over several parameter settings.","Corinne Jones, Vincent Roulet, Zaid Harchaoui",http://arxiv.org/abs/1903.08131v1,arXiv,convolutional neural networks as most artificial neural networks are commonly viewed as methods different in essence from kernel-based methods we provide a systematic translation of convolutional neural networks convnets into their kernel-based counterparts convolutional kernel networks ckns and demonstrate that this perception is unfounded both formally and empirically we show that given a convolutional neural network we can design a corresponding convolutional kernel network easily trainable using a new stochastic gradient algorithm based on an accurate gradient computation that performs on par with its convolutional neural network counterpart we present experimental results supporting our claims on landmark convnet architectures comparing each convnet to its ckn counterpart over several parameter settings,1,1
A Survey on Graph Classification and Link Prediction based on GNN,"Traditional convolutional neural networks are limited to handling Euclidean space data, overlooking the vast realm of real-life scenarios represented as graph data, including transportation networks, social networks, and reference networks. The pivotal step in transferring convolutional neural networks to graph data analysis and processing lies in the construction of graph convolutional operators and graph pooling operators. This comprehensive review article delves into the world of graph convolutional neural networks. Firstly, it elaborates on the fundamentals of graph convolutional neural networks. Subsequently, it elucidates the graph neural network models based on attention mechanisms and autoencoders, summarizing their application in node classification, graph classification, and link prediction along with the associated datasets.","Xingyu Liu, Juan Chen, Quan Wen",http://arxiv.org/abs/2307.00865v1,arXiv,traditional convolutional neural networks are limited to handling euclidean space data overlooking the vast realm of real-life scenarios represented as graph data including transportation networks social networks and reference networks the pivotal step in transferring convolutional neural networks to graph data analysis and processing lies in the construction of graph convolutional operators and graph pooling operators this comprehensive review article delves into the world of graph convolutional neural networks firstly it elaborates on the fundamentals of graph convolutional neural networks subsequently it elucidates the graph neural network models based on attention mechanisms and autoencoders summarizing their application in node classification graph classification and link prediction along with the associated datasets,1,1
"Evolving Self-taught Neural Networks: The Baldwin Effect and the
  Emergence of Intelligence","The so-called Baldwin Effect generally says how learning, as a form of ontogenetic adaptation, can influence the process of phylogenetic adaptation, or evolution. This idea has also been taken into computation in which evolution and learning are used as computational metaphors, including evolving neural networks. This paper presents a technique called evolving self-taught neural networks - neural networks that can teach themselves without external supervision or reward. The self-taught neural network is intrinsically motivated. Moreover, the self-taught neural network is the product of the interplay between evolution and learning. We simulate a multi-agent system in which neural networks are used to control autonomous agents. These agents have to forage for resources and compete for their own survival. Experimental results show that the interaction between evolution and the ability to teach oneself in self-taught neural networks outperform evolution and self-teaching alone. More specifically, the emergence of an intelligent foraging strategy is also demonstrated through that interaction. Indications for future work on evolving neural networks are also presented.",Nam Le,http://arxiv.org/abs/1906.08854v1,arXiv,the so-called baldwin effect generally says how learning as a form of ontogenetic adaptation can influence the process of phylogenetic adaptation or evolution this idea has also been taken into computation in which evolution and learning are used as computational metaphors including evolving neural networks this paper presents a technique called evolving self-taught neural networks - neural networks that can teach themselves without external supervision or reward the self-taught neural network is intrinsically motivated moreover the self-taught neural network is the product of the interplay between evolution and learning we simulate a multi-agent system in which neural networks are used to control autonomous agents these agents have to forage for resources and compete for their own survival experimental results show that the interaction between evolution and the ability to teach oneself in self-taught neural networks outperform evolution and self-teaching alone more specifically the emergence of an intelligent foraging strategy is also demonstrated through that interaction indications for future work on evolving neural networks are also presented,9,-1
Context-adaptive neural network based prediction for image compression,"This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained. These gains are on average 0.99% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.","Thierry Dumas, Aline Roumy, Christine Guillemot",http://arxiv.org/abs/1807.06244v2,arXiv,this paper describes a set of neural network architectures called prediction neural networks set pnns based on both fully-connected and convolutional neural networks for intra image prediction the choice of neural network for predicting a given image block depends on the block size hence does not need to be signalled to the decoder it is shown that while fully-connected neural networks give good performance for small block sizes convolutional neural networks provide better predictions in large blocks with complex textures thanks to the use of masks of random sizes during training the neural networks of pnns well adapt to the available context that may vary depending on the position of the image block to be predicted when integrating pnns into a h 265 codec psnr-rate performance gains going from 1 46 to 5 20 are obtained these gains are on average 0 99 larger than those of prior neural network based methods unlike the h 265 intra prediction modes which are each specialized in predicting a specific texture the proposed pnns can model a large set of complex textures,1,1
Simultaneous Weight and Architecture Optimization for Neural Networks,"Neural networks are trained by choosing an architecture and training the parameters. The choice of architecture is often by trial and error or with Neural Architecture Search (NAS) methods. While NAS provides some automation, it often relies on discrete steps that optimize the architecture and then train the parameters. We introduce a novel neural network training framework that fundamentally transforms the process by learning architecture and parameters simultaneously with gradient descent. With the appropriate setting of the loss function, it can discover sparse and compact neural networks for given datasets. Central to our approach is a multi-scale encoder-decoder, in which the encoder embeds pairs of neural networks with similar functionalities close to each other (irrespective of their architectures and weights). To train a neural network with a given dataset, we randomly sample a neural network embedding in the embedding space and then perform gradient descent using our custom loss function, which incorporates a sparsity penalty to encourage compactness. The decoder generates a neural network corresponding to the embedding. Experiments demonstrate that our framework can discover sparse and compact neural networks maintaining a high performance.","Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava",http://arxiv.org/abs/2410.08339v1,arXiv,neural networks are trained by choosing an architecture and training the parameters the choice of architecture is often by trial and error or with neural architecture search nas methods while nas provides some automation it often relies on discrete steps that optimize the architecture and then train the parameters we introduce a novel neural network training framework that fundamentally transforms the process by learning architecture and parameters simultaneously with gradient descent with the appropriate setting of the loss function it can discover sparse and compact neural networks for given datasets central to our approach is a multi-scale encoder-decoder in which the encoder embeds pairs of neural networks with similar functionalities close to each other irrespective of their architectures and weights to train a neural network with a given dataset we randomly sample a neural network embedding in the embedding space and then perform gradient descent using our custom loss function which incorporates a sparsity penalty to encourage compactness the decoder generates a neural network corresponding to the embedding experiments demonstrate that our framework can discover sparse and compact neural networks maintaining a high performance,1,1
Making Neural Networks FAIR,"Research on neural networks has gained significant momentum over the past few years. Because training is a resource-intensive process and training data cannot always be made available to everyone, there has been a trend to reuse pre-trained neural networks. As such, neural networks themselves have become research data. In this paper, we first present the neural network ontology FAIRnets Ontology, an ontology to make existing neural network models findable, accessible, interoperable, and reusable according to the FAIR principles. Our ontology allows us to model neural networks on a meta-level in a structured way, including the representation of all network layers and their characteristics. Secondly, we have modeled over 18,400 neural networks from GitHub based on this ontology, which we provide to the public as a knowledge graph called FAIRnets, ready to be used for recommending suitable neural networks to data scientists.","Anna Nguyen, Tobias Weller, Michael Färber, York Sure-Vetter",http://arxiv.org/abs/1907.11569v4,arXiv,research on neural networks has gained significant momentum over the past few years because training is a resource-intensive process and training data cannot always be made available to everyone there has been a trend to reuse pre-trained neural networks as such neural networks themselves have become research data in this paper we first present the neural network ontology fairnets ontology an ontology to make existing neural network models findable accessible interoperable and reusable according to the fair principles our ontology allows us to model neural networks on a meta-level in a structured way including the representation of all network layers and their characteristics secondly we have modeled over 18 400 neural networks from github based on this ontology which we provide to the public as a knowledge graph called fairnets ready to be used for recommending suitable neural networks to data scientists,1,-1
An SMT-Based Approach for Verifying Binarized Neural Networks,"Deep learning has emerged as an effective approach for creating modern software systems, with neural networks often surpassing hand-crafted systems. Unfortunately, neural networks are known to suffer from various safety and security issues. Formal verification is a promising avenue for tackling this difficulty, by formally certifying that networks are correct. We propose an SMT-based technique for verifying Binarized Neural Networks - a popular kind of neural network, where some weights have been binarized in order to render the neural network more memory and energy efficient, and quicker to evaluate. One novelty of our technique is that it allows the verification of neural networks that include both binarized and non-binarized components. Neural network verification is computationally very difficult, and so we propose here various optimizations, integrated into our SMT procedure as deduction steps, as well as an approach for parallelizing verification queries. We implement our technique as an extension to the Marabou framework, and use it to evaluate the approach on popular binarized neural network architectures.","Guy Amir, Haoze Wu, Clark Barrett, Guy Katz",http://arxiv.org/abs/2011.02948v2,arXiv,deep learning has emerged as an effective approach for creating modern software systems with neural networks often surpassing hand-crafted systems unfortunately neural networks are known to suffer from various safety and security issues formal verification is a promising avenue for tackling this difficulty by formally certifying that networks are correct we propose an smt-based technique for verifying binarized neural networks - a popular kind of neural network where some weights have been binarized in order to render the neural network more memory and energy efficient and quicker to evaluate one novelty of our technique is that it allows the verification of neural networks that include both binarized and non-binarized components neural network verification is computationally very difficult and so we propose here various optimizations integrated into our smt procedure as deduction steps as well as an approach for parallelizing verification queries we implement our technique as an extension to the marabou framework and use it to evaluate the approach on popular binarized neural network architectures,1,1
"Exploring the Imposition of Synaptic Precision Restrictions For
  Evolutionary Synthesis of Deep Neural Networks","A key contributing factor to incredible success of deep neural networks has been the significant rise on massively parallel computing devices allowing researchers to greatly increase the size and depth of deep neural networks, leading to significant improvements in modeling accuracy. Although deeper, larger, or complex deep neural networks have shown considerable promise, the computational complexity of such networks is a major barrier to utilization in resource-starved scenarios. We explore the synaptogenesis of deep neural networks in the formation of efficient deep neural network architectures within an evolutionary deep intelligence framework, where a probabilistic generative modeling strategy is introduced to stochastically synthesize increasingly efficient yet effective offspring deep neural networks over generations, mimicking evolutionary processes such as heredity, random mutation, and natural selection in a probabilistic manner. In this study, we primarily explore the imposition of synaptic precision restrictions and its impact on the evolutionary synthesis of deep neural networks to synthesize more efficient network architectures tailored for resource-starved scenarios. Experimental results show significant improvements in synaptic efficiency (~10X decrease for GoogLeNet-based DetectNet) and inference speed (>5X increase for GoogLeNet-based DetectNet) while preserving modeling accuracy.","Mohammad Javad Shafiee, Francis Li, Alexander Wong",http://arxiv.org/abs/1707.00095v1,arXiv,a key contributing factor to incredible success of deep neural networks has been the significant rise on massively parallel computing devices allowing researchers to greatly increase the size and depth of deep neural networks leading to significant improvements in modeling accuracy although deeper larger or complex deep neural networks have shown considerable promise the computational complexity of such networks is a major barrier to utilization in resource-starved scenarios we explore the synaptogenesis of deep neural networks in the formation of efficient deep neural network architectures within an evolutionary deep intelligence framework where a probabilistic generative modeling strategy is introduced to stochastically synthesize increasingly efficient yet effective offspring deep neural networks over generations mimicking evolutionary processes such as heredity random mutation and natural selection in a probabilistic manner in this study we primarily explore the imposition of synaptic precision restrictions and its impact on the evolutionary synthesis of deep neural networks to synthesize more efficient network architectures tailored for resource-starved scenarios experimental results show significant improvements in synaptic efficiency 10x decrease for googlenet-based detectnet and inference speed 5x increase for googlenet-based detectnet while preserving modeling accuracy,1,1
"Scalable Training of Artificial Neural Networks with Adaptive Sparse
  Connectivity inspired by Network Science","Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd\H{o}s-R\'enyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.","Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta",http://arxiv.org/abs/1707.04780v2,arXiv,through the success of deep learning in various domains artificial neural networks are currently among the most used artificial intelligence methods taking inspiration from the network properties of biological neural networks e g sparsity scale-freeness we argue that contrary to general practice artificial neural networks too should not have fully-connected layers here we propose sparse evolutionary training of artificial neural networks an algorithm which evolves an initial sparse topology erd s-r enyi random graph of two consecutive layers of neurons into a scale-free topology during learning our method replaces artificial neural networks fully-connected layers with sparse ones before training reducing quadratically the number of parameters with no decrease in accuracy we demonstrate our claims on restricted boltzmann machines multi-layer perceptrons and convolutional neural networks for unsupervised and supervised learning on 15 datasets our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible,1,1
Mean Field Analysis of Neural Networks: A Law of Large Numbers,"Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis, and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (A) large network sizes and (B) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called ""propagation of chaos"".","Justin Sirignano, Konstantinos Spiliopoulos",http://arxiv.org/abs/1805.01053v4,arXiv,machine learning and in particular neural network models have revolutionized fields such as image text and speech recognition today many important real-world applications in these areas are driven by neural networks there are also growing applications in engineering robotics medicine and finance despite their immense success in practice there is limited mathematical understanding of neural networks this paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise we analyze one-layer neural networks in the asymptotic regime of simultaneously a large network sizes and b large numbers of stochastic gradient descent training iterations we rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation this result can be considered a law of large numbers for neural networks in addition a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent a property which is commonly called propagation of chaos,7,-1
Deep Convolutional Spiking Neural Networks for Image Classification,"Spiking neural networks are biologically plausible counterparts of the artificial neural networks, artificial neural networks are usually trained with stochastic gradient descent and spiking neural networks are trained with spike timing dependant plasticity. Training deep convolutional neural networks is a memory and power intensive job. Spiking networks could potentially help in reducing the power usage. There is a large pool of tools for one to chose to train artificial neural networks of any size, on the other hand all the available tools to simulate spiking neural networks are geared towards computational neuroscience applications and they are not suitable for real life applications. In this work we focus on implementing a spiking CNN using Tensorflow to examine behaviour of the network and empirically study the effect of various parameters on learning capabilities and also study catastrophic forgetting in the spiking CNN and weight initialization problem in R-STDP using MNIST and N-MNIST data sets.","Ruthvik Vaila, John Chiasson, Vishal Saxena",http://arxiv.org/abs/1903.12272v2,arXiv,spiking neural networks are biologically plausible counterparts of the artificial neural networks artificial neural networks are usually trained with stochastic gradient descent and spiking neural networks are trained with spike timing dependant plasticity training deep convolutional neural networks is a memory and power intensive job spiking networks could potentially help in reducing the power usage there is a large pool of tools for one to chose to train artificial neural networks of any size on the other hand all the available tools to simulate spiking neural networks are geared towards computational neuroscience applications and they are not suitable for real life applications in this work we focus on implementing a spiking cnn using tensorflow to examine behaviour of the network and empirically study the effect of various parameters on learning capabilities and also study catastrophic forgetting in the spiking cnn and weight initialization problem in r-stdp using mnist and n-mnist data sets,6,-1
"Univariate ReLU neural network and its application in nonlinear system
  identification","ReLU (rectified linear units) neural network has received significant attention since its emergence. In this paper, a univariate ReLU (UReLU) neural network is proposed to both modelling the nonlinear dynamic system and revealing insights about the system. Specifically, the neural network consists of neurons with linear and UReLU activation functions, and the UReLU functions are defined as the ReLU functions respect to each dimension. The UReLU neural network is a single hidden layer neural network, and the structure is relatively simple. The initialization of the neural network employs the decoupling method, which provides a good initialization and some insight into the nonlinear system. Compared with normal ReLU neural network, the number of parameters of UReLU network is less, but it still provide a good approximation of the nonlinear dynamic system. The performance of the UReLU neural network is shown through a Hysteretic benchmark system: the Bouc-Wen system. Simulation results verify the effectiveness of the proposed method.","Xinglong Liang, Jun Xu",http://arxiv.org/abs/2003.02666v1,arXiv,relu rectified linear units neural network has received significant attention since its emergence in this paper a univariate relu urelu neural network is proposed to both modelling the nonlinear dynamic system and revealing insights about the system specifically the neural network consists of neurons with linear and urelu activation functions and the urelu functions are defined as the relu functions respect to each dimension the urelu neural network is a single hidden layer neural network and the structure is relatively simple the initialization of the neural network employs the decoupling method which provides a good initialization and some insight into the nonlinear system compared with normal relu neural network the number of parameters of urelu network is less but it still provide a good approximation of the nonlinear dynamic system the performance of the urelu neural network is shown through a hysteretic benchmark system the bouc-wen system simulation results verify the effectiveness of the proposed method,3,0
Feedforward Sequential Memory Neural Networks without Recurrent Feedback,"We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.","ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai",http://arxiv.org/abs/1510.02693v1,arXiv,we introduce a new structure for memory neural networks called feedforward sequential memory networks fsmn which can learn long-term dependency without using recurrent feedback the proposed fsmn is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers in this work we have applied fsmn to several language modeling lm tasks experimental results have shown that the memory blocks in fsmn can learn effective representations of long history experiments have shown that fsmn based language models can significantly outperform not only feedforward neural network fnn based lms but also the popular recurrent neural network rnn lms,7,-1
Flow of Information in Feed-Forward Deep Neural Networks,"Feed-forward deep neural networks have been used extensively in various machine learning applications. Developing a precise understanding of the underling behavior of neural networks is crucial for their efficient deployment. In this paper, we use an information theoretic approach to study the flow of information in a neural network and to determine how entropy of information changes between consecutive layers. Moreover, using the Information Bottleneck principle, we develop a constrained optimization problem that can be used in the training process of a deep neural network. Furthermore, we determine a lower bound for the level of data representation that can be achieved in a deep neural network with an acceptable level of distortion.","Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan",http://arxiv.org/abs/1603.06220v1,arXiv,feed-forward deep neural networks have been used extensively in various machine learning applications developing a precise understanding of the underling behavior of neural networks is crucial for their efficient deployment in this paper we use an information theoretic approach to study the flow of information in a neural network and to determine how entropy of information changes between consecutive layers moreover using the information bottleneck principle we develop a constrained optimization problem that can be used in the training process of a deep neural network furthermore we determine a lower bound for the level of data representation that can be achieved in a deep neural network with an acceptable level of distortion,1,-1
"Fixed-point optimization of deep neural networks with adaptive step size
  retraining","Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).","Sungho Shin, Yoonho Boo, Wonyong Sung",http://arxiv.org/abs/1702.08171v1,arXiv,fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining we propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining in addition a gradual quantization scheme is also tested which sequentially applies fixed-point optimizations from high- to low-precision the experiments are conducted for feed-forward deep neural networks ffdnns convolutional neural networks cnns and recurrent neural networks rnns,7,-1
Ablation of a Robot's Brain: Neural Networks Under a Knife,"It is still not fully understood exactly how neural networks are able to solve the complex tasks that have recently pushed AI research forward. We present a novel method for determining how information is structured inside a neural network. Using ablation (a neuroscience technique for cutting away parts of a brain to determine their function), we approach several neural network architectures from a biological perspective. Through an analysis of this method's results, we examine important similarities between biological and artificial neural networks to search for the implicit knowledge locked away in the network's weights.","Peter E. Lillian, Richard Meyes, Tobias Meisen",http://arxiv.org/abs/1812.05687v2,arXiv,it is still not fully understood exactly how neural networks are able to solve the complex tasks that have recently pushed ai research forward we present a novel method for determining how information is structured inside a neural network using ablation a neuroscience technique for cutting away parts of a brain to determine their function we approach several neural network architectures from a biological perspective through an analysis of this method s results we examine important similarities between biological and artificial neural networks to search for the implicit knowledge locked away in the network s weights,3,0
Fourier Neural Networks: A Comparative Study,"We review neural network architectures which were motivated by Fourier series and integrals and which are referred to as Fourier neural networks. These networks are empirically evaluated in synthetic and real-world tasks. Neither of them outperforms the standard neural network with sigmoid activation function in the real-world tasks. All neural networks, both Fourier and the standard one, empirically demonstrate lower approximation error than the truncated Fourier series when it comes to an approximation of a known function of multiple variables.","Abylay Zhumekenov, Malika Uteuliyeva, Olzhas Kabdolov, Rustem Takhanov, Zhenisbek Assylbekov, Alejandro J. Castro",http://arxiv.org/abs/1902.03011v1,arXiv,we review neural network architectures which were motivated by fourier series and integrals and which are referred to as fourier neural networks these networks are empirically evaluated in synthetic and real-world tasks neither of them outperforms the standard neural network with sigmoid activation function in the real-world tasks all neural networks both fourier and the standard one empirically demonstrate lower approximation error than the truncated fourier series when it comes to an approximation of a known function of multiple variables,3,0
"Neural Networks, Hypersurfaces, and Radon Transforms","Connections between integration along hypersufaces, Radon transforms, and neural networks are exploited to highlight an integral geometric mathematical interpretation of neural networks. By analyzing the properties of neural networks as operators on probability distributions for observed data, we show that the distribution of outputs for any node in a neural network can be interpreted as a nonlinear projection along hypersurfaces defined by level surfaces over the input data space. We utilize these descriptions to provide new interpretation for phenomena such as nonlinearity, pooling, activation functions, and adversarial examples in neural network-based learning problems.","Soheil Kolouri, Xuwang Yin, Gustavo K. Rohde",http://arxiv.org/abs/1907.02220v1,arXiv,connections between integration along hypersufaces radon transforms and neural networks are exploited to highlight an integral geometric mathematical interpretation of neural networks by analyzing the properties of neural networks as operators on probability distributions for observed data we show that the distribution of outputs for any node in a neural network can be interpreted as a nonlinear projection along hypersurfaces defined by level surfaces over the input data space we utilize these descriptions to provide new interpretation for phenomena such as nonlinearity pooling activation functions and adversarial examples in neural network-based learning problems,1,-1
"Statistical Tests and Confidential Intervals as Thresholds for Quantum
  Neural Networks","Some basic quantum neural networks were analyzed and constructed in the recent work of the author \cite{dndiep3}, published in International Journal of Theoretical Physics (2020). In particular the Least Quare Problem (LSP) and the Linear Regression Problem (LRP) was discussed. In this second paper we continue to analyze and construct the least square quantum neural network (LS-QNN), the polynomial interpolation quantum neural network (PI-QNN), the polynomial regression quantum neural network (PR-QNN) and chi-squared quantum neural network ($\chi^2$-QNN). We use the corresponding solution or tests as the threshold for the corresponding training rules.",Do Ngoc Diep,http://arxiv.org/abs/2001.11844v1,arXiv,some basic quantum neural networks were analyzed and constructed in the recent work of the author published in international journal of theoretical physics 2020 in particular the least quare problem lsp and the linear regression problem lrp was discussed in this second paper we continue to analyze and construct the least square quantum neural network ls-qnn the polynomial interpolation quantum neural network pi-qnn the polynomial regression quantum neural network pr-qnn and chi-squared quantum neural network -qnn we use the corresponding solution or tests as the threshold for the corresponding training rules,2,-1
Power Series Expansion Neural Network,"In this paper, we develop a new neural network family based on power series expansion, which is proved to achieve a better approximation accuracy in comparison with existing neural networks. This new set of neural networks embeds the power series expansion (PSE) into the neural network structure. Then it can improve the representation ability while preserving comparable computational cost by increasing the degree of PSE instead of increasing the depth or width. Both theoretical approximation and numerical results show the advantages of this new neural network.","Qipin Chen, Wenrui Hao, Juncai He",http://arxiv.org/abs/2102.13221v2,arXiv,in this paper we develop a new neural network family based on power series expansion which is proved to achieve a better approximation accuracy in comparison with existing neural networks this new set of neural networks embeds the power series expansion pse into the neural network structure then it can improve the representation ability while preserving comparable computational cost by increasing the degree of pse instead of increasing the depth or width both theoretical approximation and numerical results show the advantages of this new neural network,3,0
Symbiotic Hybrid Neural Network Watchdog For Outlier Detection,"Neural networks are largely black boxes. A neural network trained to classify fruit may classify a picture of a giraffe as a banana. A neural network watchdog's job is to identify such inputs, allowing a classifier to disregard such data. We investigate whether the watchdog should be separate from the neural network or symbiotically attached. We present empirical evidence that the symbiotic watchdog performs better than when the neural networks are disjoint.","Justin Bui, Robert J. Marks II",http://arxiv.org/abs/2103.00582v2,arXiv,neural networks are largely black boxes a neural network trained to classify fruit may classify a picture of a giraffe as a banana a neural network watchdog s job is to identify such inputs allowing a classifier to disregard such data we investigate whether the watchdog should be separate from the neural network or symbiotically attached we present empirical evidence that the symbiotic watchdog performs better than when the neural networks are disjoint,5,-1
Stock Price Prediction using Dynamic Neural Networks,"This paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices. Neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic, non-linear, and seemingly random data, thus providing a mechanism to predict stock price movements much more precisely than many current techniques. Contemporary methods for stock analysis, including fundamental, technical, and regression techniques, are conversed and paralleled with the performance of neural networks. Also, the Efficient Market Hypothesis (EMH) is presented and contrasted with Chaos theory using neural networks. This paper will refute the EMH and support Chaos theory. Finally, recommendations for using neural networks in stock price prediction will be presented.",David Noel,http://arxiv.org/abs/2306.12969v1,arXiv,this paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic non-linear and seemingly random data thus providing a mechanism to predict stock price movements much more precisely than many current techniques contemporary methods for stock analysis including fundamental technical and regression techniques are conversed and paralleled with the performance of neural networks also the efficient market hypothesis emh is presented and contrasted with chaos theory using neural networks this paper will refute the emh and support chaos theory finally recommendations for using neural networks in stock price prediction will be presented,3,-1
Equivariant neural networks and piecewise linear representation theory,"Equivariant neural networks are neural networks with symmetry. Motivated by the theory of group representations, we decompose the layers of an equivariant neural network into simple representations. The nonlinear activation functions lead to interesting nonlinear equivariant maps between simple representations. For example, the rectified linear unit (ReLU) gives rise to piecewise linear maps. We show that these considerations lead to a filtration of equivariant neural networks, generalizing Fourier series. This observation might provide a useful tool for interpreting equivariant neural networks.","Joel Gibson, Daniel Tubbenhauer, Geordie Williamson",http://arxiv.org/abs/2408.00949v2,arXiv,equivariant neural networks are neural networks with symmetry motivated by the theory of group representations we decompose the layers of an equivariant neural network into simple representations the nonlinear activation functions lead to interesting nonlinear equivariant maps between simple representations for example the rectified linear unit relu gives rise to piecewise linear maps we show that these considerations lead to a filtration of equivariant neural networks generalizing fourier series this observation might provide a useful tool for interpreting equivariant neural networks,3,0
"An Efficient Method for Solving Lane Emden Equation using Legendre
  Neural Network","The aim of this manuscript is to address non-linear differential equations of the Lane Emden equation of second order using the shifted Legendre neural network (SLNN) method. Here all the equations are classified as singular initial value problems. To manage the singularity challenge, we employ an artificial neural network method. The approach utilizes a neural network of a single layer, where the hidden layer is omitted by enlarge the input using shifted Legendre polynomials. We apply a feed forward neural network model along with the principle of error back propagation. The effectiveness of the Legendre Neural Network model is demonstrated through LaneEmden equations.","Vijay Kumar Patel, Vivek Sharma, Nitin Kumar, Anoop Tiwari",http://arxiv.org/abs/2410.05409v1,arXiv,the aim of this manuscript is to address non-linear differential equations of the lane emden equation of second order using the shifted legendre neural network slnn method here all the equations are classified as singular initial value problems to manage the singularity challenge we employ an artificial neural network method the approach utilizes a neural network of a single layer where the hidden layer is omitted by enlarge the input using shifted legendre polynomials we apply a feed forward neural network model along with the principle of error back propagation the effectiveness of the legendre neural network model is demonstrated through laneemden equations,7,0
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.",Alan Oursland,http://arxiv.org/abs/2410.19352v1,arXiv,this paper introduces a theoretical framework that connects neural network linear layers with the mahalanobis distance offering a new perspective on neural network interpretability while previous studies have explored activation functions primarily for performance optimization our work interprets these functions through statistical distance measures a less explored area in neural network research by establishing this connection we provide a foundation for developing more interpretable neural network models which is crucial for applications requiring transparency although this work is theoretical and does not include empirical data the proposed distance-based interpretation has the potential to enhance model robustness improve generalization and provide more intuitive explanations of neural network decisions,1,-1
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient dimension reduction (SDR), demonstrating that neural networks inherently perform SDR in regression tasks under appropriate rank regularizations. Specifically, the weights in the first layer span the central mean subspace. We establish the statistical consistency of the neural network-based estimator for the central mean subspace, underscoring the suitability of neural networks in addressing SDR-related challenges. Numerical experiments further validate our theoretical findings, and highlight the underlying capability of neural networks to facilitate SDR compared to the existing methods. Additionally, we discuss an extension to unravel the central subspace, broadening the scope of our investigation.","Shuntuo Xu, Zhou Yu",http://arxiv.org/abs/2412.19033v1,arXiv,this paper investigates the connection between neural networks and sufficient dimension reduction sdr demonstrating that neural networks inherently perform sdr in regression tasks under appropriate rank regularizations specifically the weights in the first layer span the central mean subspace we establish the statistical consistency of the neural network-based estimator for the central mean subspace underscoring the suitability of neural networks in addressing sdr-related challenges numerical experiments further validate our theoretical findings and highlight the underlying capability of neural networks to facilitate sdr compared to the existing methods additionally we discuss an extension to unravel the central subspace broadening the scope of our investigation,1,0
"Optimization of the Woodcock Particle Tracking Method Using Neural
  Network","The acceptance rate in Woodcock tracking algorithm is generalized to an arbitrary position-dependent variable $q(x)$. A neural network is used to optimize $q(x)$, and the FOM value is used as the loss function. This idea comes from physics informed neural network(PINN), where a neural network is used to represent the solution of differential equations. Here the neural network $q(x)$ should solve the functional equations that optimize FOM. For a 1d transmission problem with Gaussian absorption cross section, we observe a significant improvement of the FOM value compared to the constant $q$ case and the original Woodcock method. Generalizations of the neural network Woodcock(NNW) method to 3d voxel models are waiting to be explored.",Bingnan Zhang,http://arxiv.org/abs/2502.13620v1,arXiv,the acceptance rate in woodcock tracking algorithm is generalized to an arbitrary position-dependent variable a neural network is used to optimize and the fom value is used as the loss function this idea comes from physics informed neural network pinn where a neural network is used to represent the solution of differential equations here the neural network should solve the functional equations that optimize fom for a 1d transmission problem with gaussian absorption cross section we observe a significant improvement of the fom value compared to the constant case and the original woodcock method generalizations of the neural network woodcock nnw method to 3d voxel models are waiting to be explored,7,-1
Visual Character Recognition using Artificial Neural Networks,"The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks, which partially emulate human thinking in the domain of artificial intelligence. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. The document is expected to serve as a resource for learners and amateur investigators in pattern recognition, neural networking and related disciplines.",Shashank Araokar,http://arxiv.org/abs/cs/0505016v1,arXiv,the recognition of optical characters is known to be one of the earliest applications of artificial neural networks which partially emulate human thinking in the domain of artificial intelligence in this paper a simplified neural approach to recognition of optical or visual characters is portrayed and discussed the document is expected to serve as a resource for learners and amateur investigators in pattern recognition neural networking and related disciplines,9,-1
Self-organizing neural networks in classification and image recognition,Self-organizing neural networks are used for brick finding in OPERA experiment. Self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images.,"G. A. Ososkov, S. G. Dmitrievskiy, A. V. Stadnik",http://arxiv.org/abs/cs/0406047v1,arXiv,self-organizing neural networks are used for brick finding in opera experiment self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images,9,-1
"Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network",The article presents new results on the use of variable thresholds to increase the capacity of a feedback neural network. Non-binary networks are also considered in this analysis.,Praveen Kuruvada,http://arxiv.org/abs/1103.5081v2,arXiv,the article presents new results on the use of variable thresholds to increase the capacity of a feedback neural network non-binary networks are also considered in this analysis,5,0
