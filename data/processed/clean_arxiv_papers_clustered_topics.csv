Topic,Count,Name,Representation,Representative_Docs
-1,59,-1_the_neural_of_networks,"['the', 'neural', 'of', 'networks', 'and', 'network', 'to', 'in', 'we', 'is']","['machine learning and in particular neural network models have revolutionized fields such as image text and speech recognition today many important real-world applications in these areas are driven by neural networks there are also growing applications in engineering robotics medicine and finance despite their immense success in practice there is limited mathematical understanding of neural networks this paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise we analyze one-layer neural networks in the asymptotic regime of simultaneously a large network sizes and b large numbers of stochastic gradient descent training iterations we rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation this result can be considered a law of large numbers for neural networks in addition a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent a property which is commonly called propagation of chaos', 'quantized neural networks have drawn a lot of attention as they reduce the space and computational complexity during the inference moreover there has been folklore that quantization acts as an implicit regularizer and thus can improve the generalizability of neural networks yet no existing work formalizes this interesting folklore in this paper we take the binary weights in a neural network as random variables under stochastic rounding and study the distribution propagation over different layers in the neural network we propose a quasi neural network to approximate the distribution propagation which is a neural network with continuous parameters and smooth activation function we derive the neural tangent kernel ntk for this quasi neural network and show that the eigenvalue of ntk decays at approximately exponential rate which is comparable to that of gaussian kernel with randomized scale this in turn indicates that the reproducing kernel hilbert space rkhs of a binary weight neural network covers a strict subset of functions compared with the one with real value weights we use experiments to verify that the quasi neural network we proposed can well approximate binary weight neural network furthermore binary weight neural network gives a lower generalization gap compared with real value weight neural network which is similar to the difference between gaussian kernel and laplace kernel', 'neural networks have attracted a lot of attention due to its success in applications such as natural language processing and computer vision for large scale data due to the tremendous number of parameters in neural networks overfitting is an issue in training neural networks to avoid overfitting one common approach is to penalize the parameters especially the weights in neural networks although neural networks has demonstrated its advantages in many applications the theoretical foundation of penalized neural networks has not been well-established our goal of this paper is to propose the general framework of neural networks with regularization and prove its consistency under certain conditions the estimated neural network will converge to true underlying function as the sample size increases the method of sieves and the theory on minimal neural networks are used to overcome the issue of unidentifiability for the parameters two types of activation functions hyperbolic tangent function tanh and rectified linear unit relu have been taken into consideration simulations have been conducted to verify the validation of theorem of consistency']"
0,25,0_neural_the_of_network,"['neural', 'the', 'of', 'network', 'networks', 'and', 'to', 'in', 'is', 'this']","['the success of neural networks in providing miraculous results when applied to a wide variety of tasks is astonishing insight in the working can be obtained by studying the universal approximation property of neural networks it is proved extensively that neural networks are universal approximators further it is proved that deep neural networks are better approximators it is specifically proved that for a narrow neural network to approximate a function which is otherwise implemented by a deep neural network the network take exponentially large number of neurons in this work we have implemented existing methodologies for a variety of synthetic functions and identified their deficiencies further we examined that fourier neural network is able to perform fairly good with only two layers in the neural network a modified fourier neural network which has sinusoidal activation and two hidden layer is proposed and the results are tabulated', 'neural networks have been used successfully to a broad range of areas such as business data mining drug discovery and biology in medicine neural networks have been applied widely in medical diagnosis detection and evaluation of new drugs and treatment cost estimation in addition neural networks have begin practice in data mining strategies for the aim of prediction knowledge discovery this paper will present the application of neural networks for the prediction and analysis of antitubercular activity of oxazolines and oxazoles derivatives this study presents techniques based on the development of single hidden layer neural network shlffnn gradient descent back propagation neural network gdbpnn gradient descent back propagation with momentum neural network gdbpmnn back propagation with weight decay neural network bpwdnn and quantile regression neural network qrnn of artificial neural network ann models here we comparatively evaluate the performance of five neural network techniques the evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties predictive accuracy was evaluated using the root mean squared error rmse coefficient determination mean absolute error mae mean percentage error mpe and relative square error rse we found that all five neural network models were able to produce feasible models qrnn model is outperforms with all statistical tests amongst other four models', 'neural networks have shown great success in many machine learning related tasks due to their ability to act as general function approximators recent work has demonstrated the effectiveness of neural networks in control systems known as neural feedback loops most notably by using a neural network as a controller however one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks this means that unless they are designed properly they are not an ideal candidate for controllers due to issues with robustness and uncertainty which are pivotal aspects of control systems there has been initial work on robustness to both analyse and design dynamical systems with neural network controllers however one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks these structures may not be appropriate for neural network controllers and it is important to consider alternative architectures this paper considers rational neural networks and presents novel rational activation functions which can be used effectively in robustness problems for neural feedback loops rational activation functions are replaced by a general rational neural network structure which is convex in the neural network s parameters a method is proposed to recover a stabilising controller from a sum of squares feasibility test this approach is then applied to a refined rational neural network which is more compatible with sum of squares programming numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty']"
1,16,1_the_neural_networks_of,"['the', 'neural', 'networks', 'of', 'and', 'to', 'we', 'in', 'for', 'that']","['neural networks are trained by choosing an architecture and training the parameters the choice of architecture is often by trial and error or with neural architecture search nas methods while nas provides some automation it often relies on discrete steps that optimize the architecture and then train the parameters we introduce a novel neural network training framework that fundamentally transforms the process by learning architecture and parameters simultaneously with gradient descent with the appropriate setting of the loss function it can discover sparse and compact neural networks for given datasets central to our approach is a multi-scale encoder-decoder in which the encoder embeds pairs of neural networks with similar functionalities close to each other irrespective of their architectures and weights to train a neural network with a given dataset we randomly sample a neural network embedding in the embedding space and then perform gradient descent using our custom loss function which incorporates a sparsity penalty to encourage compactness the decoder generates a neural network corresponding to the embedding experiments demonstrate that our framework can discover sparse and compact neural networks maintaining a high performance', 'neural networks have achieved remarkable performance in various application domains nevertheless a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems it is highly desirable to obtain lightweight versions of neural networks for inference in edge devices many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space however a unified theoretical foundation for the problem mostly is missing in this paper we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum based on the analysis we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result we carefully design and conduct experiments to support our arguments hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights', 'the evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel ntk as introduced in 20 where it was proven that in the infinite width limit the ntk converges to an explicit limiting kernel and it stays constant during training the ntk was also implicit in some other recent papers 6 13 14 in the overparametrization regime a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting ntk and the gradient descent achieves zero training loss for a deep overparameterized neural network however it was observed in 5 that there is a performance gap between the kernel regression using the limiting ntk and the deep neural networks this performance gap is likely to originate from the change of the ntk along training due to the finite width effect the change of the ntk along the training is central to describe the generalization features of deep neural networks in the current paper we study the dynamic of the ntk for finite width deep fully-connected neural networks we derive an infinite hierarchy of ordinary differential equations the neural tangent hierarchy nth which captures the gradient descent dynamic of the deep neural network moreover under certain conditions on the neural network width and the data set dimension we prove that the truncated hierarchy of nth approximates the dynamic of the ntk up to arbitrary precision this description makes it possible to directly study the change of the ntk for deep neural networks and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting ntk']"
